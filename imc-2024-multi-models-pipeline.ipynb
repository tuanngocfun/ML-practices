{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "157f598a",
   "metadata": {
    "papermill": {
     "duration": 0.012862,
     "end_time": "2024-05-16T05:43:53.811785",
     "exception": false,
     "start_time": "2024-05-16T05:43:53.798923",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Updated information:\n",
    "- version4:\n",
    "  - Image matching methods\n",
    "     - added: MatchFormer\n",
    "     - added: SIFT + LightGlue\n",
    "     - added: DISK + LightGlue\n",
    "     - modified: Aliked + LightGlue ... speeding up by cuda cache of keypoints/descriptors\n",
    "     - modified: Superpoint + LightGlue ... speeding up by cuda cache of keypoints/descriptors\n",
    "     - modified: DogHardNet + LightGlue ... speeding up by cuda cache of keypoints/descriptors\n",
    "     - modified: Superpoint + SuperGlue ... added `torch.no_grad()` and speeding up by cuda cache of keypoints/descriptors\n",
    "  - Configuration\n",
    "     - added: CAMERA_MODEL = \"simple-radial\" or \"simple-pinhole\"\n",
    "     - added: ROTATION_CORRECTION ... `check_orientation` (LightGlue series only are supported. Others image matching methods are under construction.)\n",
    "         - https://github.com/ternaus/check_orientation\n",
    "     - added: DRY_RUN ... to run pipeline with only 10 images\n",
    "  - Pipeline\n",
    "     - Parallel execution of image matching and COLMAP processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb8b262",
   "metadata": {
    "papermill": {
     "duration": 0.01185,
     "end_time": "2024-05-16T05:43:53.835993",
     "exception": false,
     "start_time": "2024-05-16T05:43:53.824143",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17a38229",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-05-16T05:43:53.861925Z",
     "iopub.status.busy": "2024-05-16T05:43:53.861571Z",
     "iopub.status.idle": "2024-05-16T05:46:06.439385Z",
     "shell.execute_reply": "2024-05-16T05:46:06.438142Z"
    },
    "papermill": {
     "duration": 132.593665,
     "end_time": "2024-05-16T05:46:06.441963",
     "exception": false,
     "start_time": "2024-05-16T05:43:53.848298",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/dependencies-imc/pycolmap/pycolmap-0.4.0-cp310-cp310-manylinux2014_x86_64.whl\r\n",
      "Installing collected packages: pycolmap\r\n",
      "Successfully installed pycolmap-0.4.0\r\n",
      "Processing /kaggle/input/dependencies-imc/safetensors/safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Installing collected packages: safetensors\r\n",
      "  Attempting uninstall: safetensors\r\n",
      "    Found existing installation: safetensors 0.4.3\r\n",
      "    Uninstalling safetensors-0.4.3:\r\n",
      "      Successfully uninstalled safetensors-0.4.3\r\n",
      "Successfully installed safetensors-0.4.1\r\n",
      "Processing /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\r\n",
      "Installing collected packages: lightglue\r\n",
      "Successfully installed lightglue-0.0\r\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install --no-deps /kaggle/input/dependencies-imc/pycolmap/pycolmap-0.4.0-cp310-cp310-manylinux2014_x86_64.whl\n",
    "!python -m pip install --no-deps /kaggle/input/dependencies-imc/safetensors/safetensors-0.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!python -m pip install --no-index --find-links=/kaggle/input/dependencies-imc/transformers/ transformers > /dev/null\n",
    "!python -m pip install  --no-deps /kaggle/input/imc2024-packages-lightglue-rerun-kornia/lightglue-0.0-py3-none-any.whl\n",
    "\n",
    "# dkm\n",
    "!python -m pip install --no-index --find-links=/kaggle/input/dkm-dependencies/packages einops > /dev/null\n",
    "\n",
    "# match former\n",
    "!python -m pip install --no-index --find-links=/kaggle/input/matchformer-dependencies yacs > /dev/null\n",
    "\n",
    "# lightglue models\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/aliked/pytorch/aliked-n16/1/* /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/* /root/.cache/torch/hub/checkpoints/\n",
    "!cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth\n",
    "!cp /kaggle/input/pytorch-lightglue-models/* /root/.cache/torch/hub/checkpoints/\n",
    "\n",
    "# dkm model\n",
    "!mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "!cp /kaggle/input/dkm-dependencies/DKMv3_outdoor.pth /root/.cache/torch/hub/checkpoints/\n",
    "\n",
    "# check rotation\n",
    "!python -m pip install --no-index --find-links=/kaggle/input/pkg-check-orientation/ check_orientation==0.0.5 > /dev/null\n",
    "!cp /kaggle/input/pkg-check-orientation/2020-11-16_resnext50_32x4d.zip /root/.cache/torch/hub/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95885c29",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-05-16T05:46:06.471004Z",
     "iopub.status.busy": "2024-05-16T05:46:06.470665Z",
     "iopub.status.idle": "2024-05-16T05:46:06.476821Z",
     "shell.execute_reply": "2024-05-16T05:46:06.475990Z"
    },
    "papermill": {
     "duration": 0.022356,
     "end_time": "2024-05-16T05:46:06.478700",
     "exception": false,
     "start_time": "2024-05-16T05:46:06.456344",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "965b320f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-05-16T05:46:06.506065Z",
     "iopub.status.busy": "2024-05-16T05:46:06.505812Z",
     "iopub.status.idle": "2024-05-16T05:46:15.945276Z",
     "shell.execute_reply": "2024-05-16T05:46:15.944498Z"
    },
    "papermill": {
     "duration": 9.455566,
     "end_time": "2024-05-16T05:46:15.947539",
     "exception": false,
     "start_time": "2024-05-16T05:46:06.491973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# General utilities\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "from fastprogress import progress_bar\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from IPython.display import clear_output\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "import concurrent.futures\n",
    "from collections import Counter\n",
    "\n",
    "# CV/ML\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "from PIL import Image\n",
    "import timm\n",
    "from timm.data import resolve_data_config\n",
    "from timm.data.transforms_factory import create_transform\n",
    "\n",
    "import torchvision\n",
    "\n",
    "# 3D reconstruction\n",
    "import pycolmap\n",
    "\n",
    "import glob\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# dkm\n",
    "import sys\n",
    "sys.path.append('/kaggle/input/dkm-dependencies/DKM/')\n",
    "from dkm.utils.utils import tensor_to_pil, get_tuple_transform_ops\n",
    "from dkm import DKMv3_outdoor\n",
    "\n",
    "# LoFTR\n",
    "from kornia.feature import LoFTR\n",
    "\n",
    "# LightGlue\n",
    "from lightglue import match_pair\n",
    "from lightglue import ALIKED, SuperPoint, DoGHardNet, LightGlue, DISK, SIFT\n",
    "from lightglue.utils import load_image, rbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2be37065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T05:46:15.975612Z",
     "iopub.status.busy": "2024-05-16T05:46:15.974948Z",
     "iopub.status.idle": "2024-05-16T05:46:15.979323Z",
     "shell.execute_reply": "2024-05-16T05:46:15.978490Z"
    },
    "papermill": {
     "duration": 0.020865,
     "end_time": "2024-05-16T05:46:15.981879",
     "exception": false,
     "start_time": "2024-05-16T05:46:15.961014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kornia version 0.7.2\n",
      "Pycolmap version 0.4.0\n"
     ]
    }
   ],
   "source": [
    "print('Kornia version', K.__version__)\n",
    "print('Pycolmap version', pycolmap.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a10101",
   "metadata": {
    "papermill": {
     "duration": 0.012882,
     "end_time": "2024-05-16T05:46:16.007685",
     "exception": false,
     "start_time": "2024-05-16T05:46:15.994803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c27798d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T05:46:16.035029Z",
     "iopub.status.busy": "2024-05-16T05:46:16.034784Z",
     "iopub.status.idle": "2024-05-16T05:46:16.048506Z",
     "shell.execute_reply": "2024-05-16T05:46:16.047781Z"
    },
    "papermill": {
     "duration": 0.02971,
     "end_time": "2024-05-16T05:46:16.050385",
     "exception": false,
     "start_time": "2024-05-16T05:46:16.020675",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CONFIG:\n",
    "    # DEBUG Settings\n",
    "    DRY_RUN = False\n",
    "    DRY_RUN_MAX_IMAGES = 10\n",
    "\n",
    "    # Pipeline settings\n",
    "    NUM_CORES = 2\n",
    "    \n",
    "    # COLMAP Reconstruction\n",
    "    CAMERA_MODEL = \"simple-radial\"\n",
    "    \n",
    "    # Rotation correction\n",
    "    ROTATION_CORRECTION = False\n",
    "    \n",
    "    # Keypoints handling\n",
    "    MERGE_PARAMS = {\n",
    "        \"min_matches\" : 15,\n",
    "        \n",
    "        # When merging keypoints, it is enable to filtering matches with cv2.findFundamentalMatrix.\n",
    "        \"filter_FundamentalMatrix\" : False,\n",
    "        \"filter_iterations\" : 10,\n",
    "        \"filter_threshold\" : 8,\n",
    "    }\n",
    "    \n",
    "    # Keypoints Extraction\n",
    "    use_aliked_lightglue = True\n",
    "    use_doghardnet_lightglue = False\n",
    "    use_superpoint_lightglue = False\n",
    "    use_disk_lightglue = False\n",
    "    use_sift_lightglue = False\n",
    "    use_loftr = False\n",
    "    use_dkm = False\n",
    "    use_superglue = False\n",
    "    use_matchformer = False\n",
    "        \n",
    "    # Keypoints Extraction Parameters\n",
    "    params_aliked_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_doghardnet_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_superpoint_lightglue = {\n",
    "        \"num_features\" : 4096,\n",
    "        \"detection_threshold\" : 0.005,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "    \n",
    "    params_disk_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "\n",
    "    params_sift_lightglue = {\n",
    "        \"num_features\" : 8192,\n",
    "        \"detection_threshold\" : 0.001,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : 1024,\n",
    "    }\n",
    "\n",
    "    params_loftr = {\n",
    "        \"resize_small_edge_to\" : 750,\n",
    "        \"min_matches\" : 15,\n",
    "    }\n",
    "    \n",
    "    params_dkm = {\n",
    "        \"num_features\" : 2048,\n",
    "        \"detection_threshold\" : 0.4,\n",
    "        \"min_matches\" : 15,\n",
    "        \"resize_to\" : (540, 720),    \n",
    "    }\n",
    "    \n",
    "    # superpoint + superglue  ...  https://www.kaggle.com/competitions/image-matching-challenge-2023/discussion/416873\n",
    "    params_sg1 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1088,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sg2 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1280,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sg3 = {\n",
    "        \"sg_config\" : \n",
    "        {\n",
    "            \"superpoint\": {\n",
    "                \"nms_radius\": 4, \n",
    "                \"keypoint_threshold\": 0.005,\n",
    "                \"max_keypoints\": -1,\n",
    "            },\n",
    "            \"superglue\": {\n",
    "                \"weights\": \"outdoor\",\n",
    "                \"sinkhorn_iterations\": 20,\n",
    "                \"match_threshold\": 0.2,\n",
    "            },\n",
    "        },\n",
    "        \"resize_to\": 1376,\n",
    "        \"min_matches\": 15,\n",
    "    }\n",
    "    params_sgs = [params_sg1, params_sg2, params_sg3]\n",
    "    \n",
    "    params_matchformer = {\n",
    "        \"detection_threshold\" : 0.15,\n",
    "        \"resize_to\" : (560, 750),\n",
    "        \"num_features\" : 2000,\n",
    "        \"min_matches\" : 15, \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "489d3c60",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T05:46:16.077524Z",
     "iopub.status.busy": "2024-05-16T05:46:16.077245Z",
     "iopub.status.idle": "2024-05-16T05:46:16.081004Z",
     "shell.execute_reply": "2024-05-16T05:46:16.080247Z"
    },
    "papermill": {
     "duration": 0.019239,
     "end_time": "2024-05-16T05:46:16.082790",
     "exception": false,
     "start_time": "2024-05-16T05:46:16.063551",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "device=torch.device('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21efdd2c",
   "metadata": {
    "papermill": {
     "duration": 0.012786,
     "end_time": "2024-05-16T05:46:16.108811",
     "exception": false,
     "start_time": "2024-05-16T05:46:16.096025",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# COLMAP utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d58feba",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-16T05:46:16.136096Z",
     "iopub.status.busy": "2024-05-16T05:46:16.135828Z",
     "iopub.status.idle": "2024-05-16T05:46:16.163752Z",
     "shell.execute_reply": "2024-05-16T05:46:16.163053Z"
    },
    "papermill": {
     "duration": 0.04389,
     "end_time": "2024-05-16T05:46:16.165688",
     "exception": false,
     "start_time": "2024-05-16T05:46:16.121798",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code to manipulate a colmap database.\n",
    "# Forked from https://github.com/colmap/colmap/blob/dev/scripts/python/database.py\n",
    "\n",
    "# Copyright (c) 2018, ETH Zurich and UNC Chapel Hill.\n",
    "# All rights reserved.\n",
    "#\n",
    "# Redistribution and use in source and binary forms, with or without\n",
    "# modification, are permitted provided that the following conditions are met:\n",
    "#\n",
    "#     * Redistributions of source code must retain the above copyright\n",
    "#       notice, this list of conditions and the following disclaimer.\n",
    "#\n",
    "#     * Redistributions in binary form must reproduce the above copyright\n",
    "#       notice, this list of conditions and the following disclaimer in the\n",
    "#       documentation and/or other materials provided with the distribution.\n",
    "#\n",
    "#     * Neither the name of ETH Zurich and UNC Chapel Hill nor the names of\n",
    "#       its contributors may be used to endorse or promote products derived\n",
    "#       from this software without specific prior written permission.\n",
    "#\n",
    "# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n",
    "# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n",
    "# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n",
    "# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE\n",
    "# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n",
    "# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n",
    "# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n",
    "# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n",
    "# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n",
    "# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE\n",
    "# POSSIBILITY OF SUCH DAMAGE.\n",
    "#\n",
    "# Author: Johannes L. Schoenberger (jsch-at-demuc-dot-de)\n",
    "\n",
    "# This script is based on an original implementation by True Price.\n",
    "\n",
    "import sys\n",
    "import sqlite3\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "IS_PYTHON3 = sys.version_info[0] >= 3\n",
    "\n",
    "MAX_IMAGE_ID = 2**31 - 1\n",
    "\n",
    "CREATE_CAMERAS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS cameras (\n",
    "    camera_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "    model INTEGER NOT NULL,\n",
    "    width INTEGER NOT NULL,\n",
    "    height INTEGER NOT NULL,\n",
    "    params BLOB,\n",
    "    prior_focal_length INTEGER NOT NULL)\"\"\"\n",
    "\n",
    "CREATE_DESCRIPTORS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS descriptors (\n",
    "    image_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\"\"\"\n",
    "\n",
    "CREATE_IMAGES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS images (\n",
    "    image_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "    name TEXT NOT NULL UNIQUE,\n",
    "    camera_id INTEGER NOT NULL,\n",
    "    prior_qw REAL,\n",
    "    prior_qx REAL,\n",
    "    prior_qy REAL,\n",
    "    prior_qz REAL,\n",
    "    prior_tx REAL,\n",
    "    prior_ty REAL,\n",
    "    prior_tz REAL,\n",
    "    CONSTRAINT image_id_check CHECK(image_id >= 0 and image_id < {}),\n",
    "    FOREIGN KEY(camera_id) REFERENCES cameras(camera_id))\n",
    "\"\"\".format(MAX_IMAGE_ID)\n",
    "\n",
    "CREATE_TWO_VIEW_GEOMETRIES_TABLE = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS two_view_geometries (\n",
    "    pair_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    config INTEGER NOT NULL,\n",
    "    F BLOB,\n",
    "    E BLOB,\n",
    "    H BLOB)\n",
    "\"\"\"\n",
    "\n",
    "CREATE_KEYPOINTS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS keypoints (\n",
    "    image_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\n",
    "\"\"\"\n",
    "\n",
    "CREATE_MATCHES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS matches (\n",
    "    pair_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB)\"\"\"\n",
    "\n",
    "CREATE_NAME_INDEX = \\\n",
    "    \"CREATE UNIQUE INDEX IF NOT EXISTS index_name ON images(name)\"\n",
    "\n",
    "CREATE_ALL = \"; \".join([\n",
    "    CREATE_CAMERAS_TABLE,\n",
    "    CREATE_IMAGES_TABLE,\n",
    "    CREATE_KEYPOINTS_TABLE,\n",
    "    CREATE_DESCRIPTORS_TABLE,\n",
    "    CREATE_MATCHES_TABLE,\n",
    "    CREATE_TWO_VIEW_GEOMETRIES_TABLE,\n",
    "    CREATE_NAME_INDEX\n",
    "])\n",
    "\n",
    "\n",
    "def image_ids_to_pair_id(image_id1, image_id2):\n",
    "    if image_id1 > image_id2:\n",
    "        image_id1, image_id2 = image_id2, image_id1\n",
    "    return image_id1 * MAX_IMAGE_ID + image_id2\n",
    "\n",
    "\n",
    "def pair_id_to_image_ids(pair_id):\n",
    "    image_id2 = pair_id % MAX_IMAGE_ID\n",
    "    image_id1 = (pair_id - image_id2) / MAX_IMAGE_ID\n",
    "    return image_id1, image_id2\n",
    "\n",
    "\n",
    "def array_to_blob(array):\n",
    "    if IS_PYTHON3:\n",
    "        return array.tostring()\n",
    "    else:\n",
    "        return np.getbuffer(array)\n",
    "\n",
    "\n",
    "def blob_to_array(blob, dtype, shape=(-1,)):\n",
    "    if IS_PYTHON3:\n",
    "        return np.fromstring(blob, dtype=dtype).reshape(*shape)\n",
    "    else:\n",
    "        return np.frombuffer(blob, dtype=dtype).reshape(*shape)\n",
    "\n",
    "\n",
    "class COLMAPDatabase(sqlite3.Connection):\n",
    "\n",
    "    @staticmethod\n",
    "    def connect(database_path):\n",
    "        return sqlite3.connect(database_path, factory=COLMAPDatabase)\n",
    "\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(COLMAPDatabase, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.create_tables = lambda: self.executescript(CREATE_ALL)\n",
    "        self.create_cameras_table = \\\n",
    "            lambda: self.executescript(CREATE_CAMERAS_TABLE)\n",
    "        self.create_descriptors_table = \\\n",
    "            lambda: self.executescript(CREATE_DESCRIPTORS_TABLE)\n",
    "        self.create_images_table = \\\n",
    "            lambda: self.executescript(CREATE_IMAGES_TABLE)\n",
    "        self.create_two_view_geometries_table = \\\n",
    "            lambda: self.executescript(CREATE_TWO_VIEW_GEOMETRIES_TABLE)\n",
    "        self.create_keypoints_table = \\\n",
    "            lambda: self.executescript(CREATE_KEYPOINTS_TABLE)\n",
    "        self.create_matches_table = \\\n",
    "            lambda: self.executescript(CREATE_MATCHES_TABLE)\n",
    "        self.create_name_index = lambda: self.executescript(CREATE_NAME_INDEX)\n",
    "\n",
    "    def add_camera(self, model, width, height, params,\n",
    "                   prior_focal_length=False, camera_id=None):\n",
    "        params = np.asarray(params, np.float64)\n",
    "        cursor = self.execute(\n",
    "            \"INSERT INTO cameras VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "            (camera_id, model, width, height, array_to_blob(params),\n",
    "             prior_focal_length))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "    def add_image(self, name, camera_id,\n",
    "                  prior_q=np.zeros(4), prior_t=np.zeros(3), image_id=None):\n",
    "        cursor = self.execute(\n",
    "            \"INSERT INTO images VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "            (image_id, name, camera_id, prior_q[0], prior_q[1], prior_q[2],\n",
    "             prior_q[3], prior_t[0], prior_t[1], prior_t[2]))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "    def add_keypoints(self, image_id, keypoints):\n",
    "        assert(len(keypoints.shape) == 2)\n",
    "        assert(keypoints.shape[1] in [2, 4, 6])\n",
    "\n",
    "        keypoints = np.asarray(keypoints, np.float32)\n",
    "        self.execute(\n",
    "            \"INSERT INTO keypoints VALUES (?, ?, ?, ?)\",\n",
    "            (image_id,) + keypoints.shape + (array_to_blob(keypoints),))\n",
    "\n",
    "    def add_descriptors(self, image_id, descriptors):\n",
    "        descriptors = np.ascontiguousarray(descriptors, np.uint8)\n",
    "        self.execute(\n",
    "            \"INSERT INTO descriptors VALUES (?, ?, ?, ?)\",\n",
    "            (image_id,) + descriptors.shape + (array_to_blob(descriptors),))\n",
    "\n",
    "    def add_matches(self, image_id1, image_id2, matches):\n",
    "        assert(len(matches.shape) == 2)\n",
    "        assert(matches.shape[1] == 2)\n",
    "\n",
    "        if image_id1 > image_id2:\n",
    "            matches = matches[:,::-1]\n",
    "\n",
    "        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n",
    "        matches = np.asarray(matches, np.uint32)\n",
    "        self.execute(\n",
    "            \"INSERT INTO matches VALUES (?, ?, ?, ?)\",\n",
    "            (pair_id,) + matches.shape + (array_to_blob(matches),))\n",
    "\n",
    "    def add_two_view_geometry(self, image_id1, image_id2, matches,\n",
    "                              F=np.eye(3), E=np.eye(3), H=np.eye(3), config=2):\n",
    "        assert(len(matches.shape) == 2)\n",
    "        assert(matches.shape[1] == 2)\n",
    "\n",
    "        if image_id1 > image_id2:\n",
    "            matches = matches[:,::-1]\n",
    "\n",
    "        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n",
    "        matches = np.asarray(matches, np.uint32)\n",
    "        F = np.asarray(F, dtype=np.float64)\n",
    "        E = np.asarray(E, dtype=np.float64)\n",
    "        H = np.asarray(H, dtype=np.float64)\n",
    "        self.execute(\n",
    "            \"INSERT INTO two_view_geometries VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "            (pair_id,) + matches.shape + (array_to_blob(matches), config,\n",
    "             array_to_blob(F), array_to_blob(E), array_to_blob(H)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3dd330",
   "metadata": {
    "papermill": {
     "duration": 0.012671,
     "end_time": "2024-05-16T05:46:16.191512",
     "exception": false,
     "start_time": "2024-05-16T05:46:16.178841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# h5 to colmap db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fda915e6",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-16T05:46:16.218971Z",
     "iopub.status.busy": "2024-05-16T05:46:16.218681Z",
     "iopub.status.idle": "2024-05-16T05:46:16.238868Z",
     "shell.execute_reply": "2024-05-16T05:46:16.238025Z"
    },
    "papermill": {
     "duration": 0.036072,
     "end_time": "2024-05-16T05:46:16.240624",
     "exception": false,
     "start_time": "2024-05-16T05:46:16.204552",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code to interface DISK with Colmap.\n",
    "# Forked from https://github.com/cvlab-epfl/disk/blob/37f1f7e971cea3055bb5ccfc4cf28bfd643fa339/colmap/h5_to_db.py\n",
    "\n",
    "#  Copyright [2020] [Michał Tyszkiewicz, Pascal Fua, Eduard Trulls]\n",
    "#\n",
    "#   Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#   you may not use this file except in compliance with the License.\n",
    "#   You may obtain a copy of the License at\n",
    "#\n",
    "#       http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "#   Unless required by applicable law or agreed to in writing, software\n",
    "#   distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "#   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "#   See the License for the specific language governing permissions and\n",
    "#   limitations under the License.\n",
    "\n",
    "import os, argparse, h5py, warnings\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ExifTags\n",
    "\n",
    "\n",
    "def get_focal(image_path, err_on_default=False):\n",
    "    image         = Image.open(image_path)\n",
    "    max_size      = max(image.size)\n",
    "\n",
    "    exif = image.getexif()\n",
    "    focal = None\n",
    "    if exif is not None:\n",
    "        focal_35mm = None\n",
    "        # https://github.com/colmap/colmap/blob/d3a29e203ab69e91eda938d6e56e1c7339d62a99/src/util/bitmap.cc#L299\n",
    "        for tag, value in exif.items():\n",
    "            focal_35mm = None\n",
    "            if ExifTags.TAGS.get(tag, None) == 'FocalLengthIn35mmFilm':\n",
    "                focal_35mm = float(value)\n",
    "                break\n",
    "\n",
    "        if focal_35mm is not None:\n",
    "            focal = focal_35mm / 35. * max_size\n",
    "    \n",
    "    if focal is None:\n",
    "        if err_on_default:\n",
    "            raise RuntimeError(\"Failed to find focal length\")\n",
    "\n",
    "        # failed to find it in exif, use prior\n",
    "        FOCAL_PRIOR = 1.2\n",
    "        focal = FOCAL_PRIOR * max_size\n",
    "\n",
    "    return focal\n",
    "\n",
    "def create_camera(db, image_path, camera_model):\n",
    "    image         = Image.open(image_path)\n",
    "    width, height = image.size\n",
    "\n",
    "    focal = get_focal(image_path)\n",
    "\n",
    "    if camera_model == 'simple-pinhole':\n",
    "        model = 0 # simple pinhole\n",
    "        param_arr = np.array([focal, width / 2, height / 2])\n",
    "    if camera_model == 'pinhole':\n",
    "        model = 1 # pinhole\n",
    "        param_arr = np.array([focal, focal, width / 2, height / 2])\n",
    "    elif camera_model == 'simple-radial':\n",
    "        model = 2 # simple radial\n",
    "        param_arr = np.array([focal, width / 2, height / 2, 0.1])\n",
    "    elif camera_model == 'opencv':\n",
    "        model = 4 # opencv\n",
    "        param_arr = np.array([focal, focal, width / 2, height / 2, 0., 0., 0., 0.])\n",
    "         \n",
    "    return db.add_camera(model, width, height, param_arr)\n",
    "\n",
    "\n",
    "def add_keypoints(db, h5_path, image_path, img_ext, camera_model, single_camera = True):\n",
    "    keypoint_f = h5py.File(os.path.join(h5_path, 'keypoints.h5'), 'r')\n",
    "\n",
    "    camera_id = None\n",
    "    fname_to_id = {}\n",
    "    for filename in tqdm(list(keypoint_f.keys())):\n",
    "        keypoints = keypoint_f[filename][()]\n",
    "\n",
    "        fname_with_ext = filename# + img_ext\n",
    "        path = os.path.join(image_path, fname_with_ext)\n",
    "        if not os.path.isfile(path):\n",
    "            raise IOError(f'Invalid image path {path}')\n",
    "\n",
    "        if camera_id is None or not single_camera:\n",
    "            camera_id = create_camera(db, path, camera_model)\n",
    "        image_id = db.add_image(fname_with_ext, camera_id)\n",
    "        fname_to_id[filename] = image_id\n",
    "\n",
    "        db.add_keypoints(image_id, keypoints)\n",
    "\n",
    "    return fname_to_id\n",
    "\n",
    "def add_matches(db, h5_path, fname_to_id):\n",
    "    match_file = h5py.File(os.path.join(h5_path, 'matches.h5'), 'r')\n",
    "    \n",
    "    added = set()\n",
    "    n_keys = len(match_file.keys())\n",
    "    n_total = (n_keys * (n_keys - 1)) // 2\n",
    "\n",
    "    with tqdm(total=n_total) as pbar:\n",
    "        for key_1 in match_file.keys():\n",
    "            group = match_file[key_1]\n",
    "            for key_2 in group.keys():\n",
    "                id_1 = fname_to_id[key_1]\n",
    "                id_2 = fname_to_id[key_2]\n",
    "\n",
    "                pair_id = image_ids_to_pair_id(id_1, id_2)\n",
    "                if pair_id in added:\n",
    "                    warnings.warn(f'Pair {pair_id} ({id_1}, {id_2}) already added!')\n",
    "                    continue\n",
    "            \n",
    "                matches = group[key_2][()]\n",
    "                db.add_matches(id_1, id_2, matches)\n",
    "\n",
    "                added.add(pair_id)\n",
    "\n",
    "                pbar.update(1)\n",
    "                \n",
    "def import_into_colmap(img_dir,\n",
    "                       feature_dir ='.featureout',\n",
    "                       database_path = 'colmap.db',\n",
    "                       img_ext='.jpg'):\n",
    "    db = COLMAPDatabase.connect(database_path)\n",
    "    db.create_tables()\n",
    "    single_camera = False\n",
    "    fname_to_id = add_keypoints(db, feature_dir, img_dir, img_ext, CONFIG.CAMERA_MODEL, single_camera)\n",
    "    add_matches(\n",
    "        db,\n",
    "        feature_dir,\n",
    "        fname_to_id,\n",
    "    )\n",
    "\n",
    "    db.commit()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865ac1f0",
   "metadata": {
    "papermill": {
     "duration": 0.01272,
     "end_time": "2024-05-16T05:46:16.266250",
     "exception": false,
     "start_time": "2024-05-16T05:46:16.253530",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Rotation detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b016165",
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-05-16T05:46:16.293274Z",
     "iopub.status.busy": "2024-05-16T05:46:16.293016Z",
     "iopub.status.idle": "2024-05-16T05:46:16.932572Z",
     "shell.execute_reply": "2024-05-16T05:46:16.931735Z"
    },
    "papermill": {
     "duration": 0.655665,
     "end_time": "2024-05-16T05:46:16.934821",
     "exception": false,
     "start_time": "2024-05-16T05:46:16.279156",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name swsl_resnext50_32x4d to current resnext50_32x4d.fb_swsl_ig1b_ft_in1k.\n",
      "  model = create_fn(\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io import read_image as T_read_image\n",
    "from torchvision.io import ImageReadMode\n",
    "from torchvision import transforms as T\n",
    "from check_orientation.pre_trained_models import create_model\n",
    "\n",
    "def convert_rot_k(index):\n",
    "    if index == 0:\n",
    "        return 0\n",
    "    elif index == 1:\n",
    "        return 3\n",
    "    elif index == 2:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "class CheckRotationDataset(Dataset):\n",
    "    def __init__(self, files, transform=None):\n",
    "        self.transform = transform\n",
    "        self.files = files\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        imgPath = self.files[idx]\n",
    "        image = T_read_image(imgPath, mode=ImageReadMode.RGB)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "def get_CheckRotation_dataloader(images, batch_size=1):\n",
    "    transform = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ConvertImageDtype(torch.float),\n",
    "        T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "\n",
    "    dataset = CheckRotationDataset(images, transform=transform)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def exec_rotation_detection(img_files, device):\n",
    "    model = create_model(\"swsl_resnext50_32x4d\")\n",
    "    model.eval().to(device);\n",
    "    \n",
    "    dataloader = get_CheckRotation_dataloader(img_files)\n",
    "    \n",
    "    rots = []\n",
    "    for idx, image in enumerate(dataloader):\n",
    "        image = image.to(torch.float32).to(device)\n",
    "        with torch.no_grad():\n",
    "            prediction = model(image).detach().cpu().numpy()\n",
    "            detected_rot = prediction[0].argmax()\n",
    "            rot_k = convert_rot_k(detected_rot)\n",
    "            rots.append(rot_k)\n",
    "            print(f\"{os.path.basename(img_files[idx])} > rot_k={rot_k}\")\n",
    "    return rots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a01e450",
   "metadata": {
    "papermill": {
     "duration": 0.012915,
     "end_time": "2024-05-16T05:46:16.962036",
     "exception": false,
     "start_time": "2024-05-16T05:46:16.949121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Image Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e838c40",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-16T05:46:16.989836Z",
     "iopub.status.busy": "2024-05-16T05:46:16.989229Z",
     "iopub.status.idle": "2024-05-16T05:46:17.007012Z",
     "shell.execute_reply": "2024-05-16T05:46:17.006152Z"
    },
    "papermill": {
     "duration": 0.033733,
     "end_time": "2024-05-16T05:46:17.008843",
     "exception": false,
     "start_time": "2024-05-16T05:46:16.975110",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We will use ViT global descriptor to get matching shortlists.\n",
    "def get_global_desc(fnames, model,\n",
    "                    device =  torch.device('cpu')):\n",
    "    model = model.eval()\n",
    "    model= model.to(device)\n",
    "    config = resolve_data_config({}, model=model)\n",
    "    transform = create_transform(**config)\n",
    "    global_descs_convnext=[]\n",
    "    for i, img_fname_full in tqdm(enumerate(fnames),total= len(fnames)):\n",
    "        key = os.path.splitext(os.path.basename(img_fname_full))[0]\n",
    "        img = Image.open(img_fname_full).convert('RGB')\n",
    "        timg = transform(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            desc = model.forward_features(timg.to(device)).mean(dim=(-1,2))#\n",
    "            #print (desc.shape)\n",
    "            desc = desc.view(1, -1)\n",
    "            desc_norm = F.normalize(desc, dim=1, p=2)\n",
    "        #print (desc_norm)\n",
    "        global_descs_convnext.append(desc_norm.detach().cpu())\n",
    "    global_descs_all = torch.cat(global_descs_convnext, dim=0)\n",
    "    return global_descs_all.to(torch.float32)\n",
    "\n",
    "def convert_1d_to_2d(idx, num_images):\n",
    "    idx1 = idx // num_images\n",
    "    idx2 = idx % num_images\n",
    "    return (idx1, idx2)\n",
    "\n",
    "def get_pairs_from_distancematrix(mat):\n",
    "    pairs = [ convert_1d_to_2d(idx, mat.shape[0]) for idx in np.argsort(mat.flatten())]\n",
    "    pairs = [ pair for pair in pairs if pair[0] < pair[1] ]\n",
    "    return pairs\n",
    "\n",
    "def get_img_pairs_exhaustive(img_fnames, model, device):\n",
    "    #index_pairs = []\n",
    "    #for i in range(len(img_fnames)):\n",
    "    #    for j in range(i+1, len(img_fnames)):\n",
    "    #        index_pairs.append((i,j))\n",
    "    #return index_pairs\n",
    "    descs = get_global_desc(img_fnames, model, device=device)\n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "    matching_list = get_pairs_from_distancematrix(dm)\n",
    "    return matching_list\n",
    "\n",
    "\n",
    "def get_image_pairs_shortlist(fnames,\n",
    "                              sim_th = 0.6, # should be strict\n",
    "                              min_pairs = 20,\n",
    "                              exhaustive_if_less = 20,\n",
    "                              device=torch.device('cpu')):\n",
    "    num_imgs = len(fnames)\n",
    "\n",
    "    model = timm.create_model('tf_efficientnet_b7',\n",
    "                              checkpoint_path='/kaggle/input/tf-efficientnet/pytorch/tf-efficientnet-b7/1/tf_efficientnet_b7_ra-6c08e654.pth')\n",
    "    model.eval()\n",
    "    descs = get_global_desc(fnames, model, device=device)\n",
    "\n",
    "    if num_imgs <= exhaustive_if_less:\n",
    "        return get_img_pairs_exhaustive(fnames, model, device)\n",
    "    \n",
    "    dm = torch.cdist(descs, descs, p=2).detach().cpu().numpy()\n",
    "    # removing half\n",
    "    mask = dm <= sim_th\n",
    "    total = 0\n",
    "    matching_list = []\n",
    "    ar = np.arange(num_imgs)\n",
    "    already_there_set = []\n",
    "    for st_idx in range(num_imgs-1):\n",
    "        mask_idx = mask[st_idx]\n",
    "        to_match = ar[mask_idx]\n",
    "        if len(to_match) < min_pairs:\n",
    "            to_match = np.argsort(dm[st_idx])[:min_pairs]  \n",
    "        for idx in to_match:\n",
    "            if st_idx == idx:\n",
    "                continue\n",
    "            if dm[st_idx, idx] < 1000:\n",
    "                matching_list.append(tuple(sorted((st_idx, idx.item()))))\n",
    "                total+=1\n",
    "    matching_list = sorted(list(set(matching_list)))\n",
    "    return matching_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ef9e03",
   "metadata": {
    "papermill": {
     "duration": 0.013225,
     "end_time": "2024-05-16T05:46:17.035099",
     "exception": false,
     "start_time": "2024-05-16T05:46:17.021874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints: LightGlue series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "502e8c6f",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-16T05:46:17.062650Z",
     "iopub.status.busy": "2024-05-16T05:46:17.062063Z",
     "iopub.status.idle": "2024-05-16T05:46:17.086321Z",
     "shell.execute_reply": "2024-05-16T05:46:17.085498Z"
    },
    "papermill": {
     "duration": 0.040053,
     "end_time": "2024-05-16T05:46:17.088189",
     "exception": false,
     "start_time": "2024-05-16T05:46:17.048136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_torch_image(fname, device=torch.device('cpu')):\n",
    "    img = K.io.load_image(fname, K.io.ImageLoadType.RGB32, device=device)[None, ...]\n",
    "    return img\n",
    "\n",
    "def convert_coord(r, w, h, rotk):\n",
    "    if rotk == 0:\n",
    "        return r\n",
    "    elif rotk == 1:\n",
    "        rx = w-1-r[:, 1]\n",
    "        ry = r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "    elif rotk == 2:\n",
    "        rx = w-1-r[:, 0]\n",
    "        ry = h-1-r[:, 1]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "    elif rotk == 3:\n",
    "        rx = r[:, 1]\n",
    "        ry = h-1-r[:, 0]\n",
    "        return torch.concat([rx[None], ry[None]], dim=0).T\n",
    "\n",
    "def detect_common(img_fnames,\n",
    "                  model_name,\n",
    "                  rots,\n",
    "                  file_keypoints,\n",
    "                  feature_dir = '.featureout',\n",
    "                  num_features = 4096,\n",
    "                  resize_to = 1024,\n",
    "                  detection_threshold = 0.01,\n",
    "                  device=torch.device('cpu'),\n",
    "                  min_matches=15,verbose=True\n",
    "                 ):\n",
    "    if not os.path.isdir(feature_dir):\n",
    "        os.makedirs(feature_dir)\n",
    "\n",
    "    #####################################################\n",
    "    # Extract keypoints and descriptions\n",
    "    #####################################################\n",
    "    dict_model = {\n",
    "        \"aliked\" : ALIKED,\n",
    "        \"superpoint\" : SuperPoint,\n",
    "        \"doghardnet\" : DoGHardNet,\n",
    "        \"disk\" : DISK,\n",
    "        \"sift\" : SIFT,\n",
    "    }\n",
    "    extractor_class = dict_model[model_name]\n",
    "    \n",
    "    dtype = torch.float32 # ALIKED has issues with float16\n",
    "    extractor = extractor_class(\n",
    "        max_num_keypoints=num_features, detection_threshold=detection_threshold, resize=resize_to\n",
    "    ).eval().to(device, dtype)\n",
    "        \n",
    "    dict_kpts_cuda = {}\n",
    "    dict_descs_cuda = {}\n",
    "    for (img_path, rot_k) in zip(img_fnames, rots):\n",
    "        img_fname = img_path.split('/')[-1]\n",
    "        key = img_fname\n",
    "        with torch.inference_mode():\n",
    "            image0 = load_torch_image(img_path, device=device).to(dtype)\n",
    "            h, w = image0.shape[2], image0.shape[3]\n",
    "            image1 = torch.rot90(image0, rot_k, [2, 3])\n",
    "            feats0 = extractor.extract(image1)  # auto-resize the image, disable with resize=None\n",
    "            kpts = feats0['keypoints'].reshape(-1, 2).detach()\n",
    "            descs = feats0['descriptors'].reshape(len(kpts), -1).detach()\n",
    "            kpts = convert_coord(kpts, w, h, rot_k)\n",
    "            dict_kpts_cuda[f\"{key}\"] = kpts\n",
    "            dict_descs_cuda[f\"{key}\"] = descs\n",
    "            print(f\"{model_name} > rot_k={rot_k}, kpts.shape={kpts.shape}, descs.shape={descs.shape}\")\n",
    "    del extractor\n",
    "    gc.collect()\n",
    "\n",
    "    #####################################################\n",
    "    # Matching keypoints\n",
    "    #####################################################\n",
    "    lg_matcher = KF.LightGlueMatcher(model_name, {\"width_confidence\": -1,\n",
    "                                            \"depth_confidence\": -1,\n",
    "                                             \"mp\": True if 'cuda' in str(device) else False}).eval().to(device)\n",
    "    \n",
    "    cnt_pairs = 0\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:\n",
    "        for pair_idx in tqdm(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            \n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "            \n",
    "            kp1 = dict_kpts_cuda[key1]\n",
    "            kp2 = dict_kpts_cuda[key2]\n",
    "            desc1 = dict_descs_cuda[key1]\n",
    "            desc2 = dict_descs_cuda[key2]\n",
    "            with torch.inference_mode():\n",
    "                dists, idxs = lg_matcher(desc1,\n",
    "                                     desc2,\n",
    "                                     KF.laf_from_center_scale_ori(kp1[None]),\n",
    "                                     KF.laf_from_center_scale_ori(kp2[None]))\n",
    "            if len(idxs)  == 0:\n",
    "                continue\n",
    "            n_matches = len(idxs)\n",
    "            kp1 = kp1[idxs[:,0], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            kp2 = kp2[idxs[:,1], :].cpu().numpy().reshape(-1, 2).astype(np.float32)\n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=np.concatenate([kp1, kp2], axis=1))\n",
    "                cnt_pairs+=1\n",
    "                print (f'{model_name}> {key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair({model_name}+lightglue)')            \n",
    "            else:\n",
    "                print (f'{model_name}> {key1}-{key2}: {n_matches} matches --> skipped')\n",
    "    del lg_matcher\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    return\n",
    "\n",
    "def detect_lightglue_common(\n",
    "    img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "    resize_to=1024,\n",
    "    detection_threshold=0.01, \n",
    "    num_features=4096, \n",
    "    min_matches=15,\n",
    "):\n",
    "    t=time()\n",
    "    detect_common(\n",
    "        img_fnames, model_name, rots, file_keypoints, feature_dir, \n",
    "        resize_to=resize_to,\n",
    "        num_features=num_features, \n",
    "        detection_threshold=detection_threshold, \n",
    "        device=device,\n",
    "        min_matches=min_matches,\n",
    "    )\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec ({model_name}+LightGlue)')\n",
    "    return t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ac9dfd",
   "metadata": {
    "papermill": {
     "duration": 0.013832,
     "end_time": "2024-05-16T05:46:17.116178",
     "exception": false,
     "start_time": "2024-05-16T05:46:17.102346",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints: SuperGlue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad3406c6",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-16T05:46:17.144313Z",
     "iopub.status.busy": "2024-05-16T05:46:17.144064Z",
     "iopub.status.idle": "2024-05-16T05:46:17.209861Z",
     "shell.execute_reply": "2024-05-16T05:46:17.209119Z"
    },
    "papermill": {
     "duration": 0.082752,
     "end_time": "2024-05-16T05:46:17.212197",
     "exception": false,
     "start_time": "2024-05-16T05:46:17.129445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../input/super-glue-pretrained-network\")\n",
    "from models.matching import Matching\n",
    "from models.superpoint import SuperPoint\n",
    "from models.superglue import SuperGlue\n",
    "from models.utils import (compute_pose_error, compute_epipolar_error,\n",
    "                          estimate_pose, make_matching_plot,\n",
    "                          error_colormap, AverageTimer, pose_auc, read_image,\n",
    "                          process_resize, frame2tensor,\n",
    "                          rotate_intrinsics, rotate_pose_inplane,\n",
    "                          scale_intrinsics)\n",
    "\n",
    "from torch.nn import functional as torchF  # For resizing tensor\n",
    "\n",
    "def sg_imread(path):\n",
    "    image = cv2.imread(str(path), cv2.IMREAD_GRAYSCALE)\n",
    "    return image\n",
    "\n",
    "# Preprocess\n",
    "def sg_read_image(image, device, resize):\n",
    "    w, h = image.shape[1], image.shape[0]\n",
    "    w_new, h_new = process_resize(w, h, [resize,])\n",
    "    \n",
    "    unit_shape = 8\n",
    "    w_new = w_new // unit_shape * unit_shape\n",
    "    h_new = h_new // unit_shape * unit_shape\n",
    "    \n",
    "    scales = (float(w) / float(w_new), float(h) / float(h_new))\n",
    "    image = cv2.resize(image.astype('float32'), (w_new, h_new))\n",
    "\n",
    "    inp = frame2tensor(image, \"cpu\")\n",
    "    return image, inp, scales, (h, w)\n",
    "\n",
    "class SGDataset(Dataset):\n",
    "    def __init__(self, img_fnames, resize_to, device):\n",
    "        self.img_fnames = img_fnames\n",
    "        self.resize_to = resize_to\n",
    "        self.device = device\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_fnames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.img_fnames[idx]\n",
    "        im = cv2.imread(fname, cv2.IMREAD_GRAYSCALE)\n",
    "        _, image, scale, ori_shape = sg_read_image(im, self.device, self.resize_to)\n",
    "        return image, torch.tensor([idx]), torch.tensor(ori_shape)\n",
    "\n",
    "def get_superglue_dataloader(img_fnames, resize_to, device, batch_size=1):\n",
    "    dataset = SGDataset(img_fnames, resize_to, device)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def detect_superglue(\n",
    "    img_fnames, index_pairs, feature_dir, device, sg_config, file_keypoints, \n",
    "    resize_to=750, min_matches=15\n",
    "):    \n",
    "    t=time()\n",
    "\n",
    "    fnames1, fnames2, idxs1, idxs2 = [], [], [], []\n",
    "    for pair_idx in progress_bar(index_pairs):\n",
    "        idx1, idx2 = pair_idx\n",
    "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "        fnames1.append(fname1)\n",
    "        fnames2.append(fname2)\n",
    "        idxs1.append(idx1)\n",
    "        idxs2.append(idx2)\n",
    "        \n",
    "    dataloader = get_superglue_dataloader( img_fnames, resize_to, device)\n",
    "\n",
    "    #####################################################\n",
    "    # Extract keypoints and descriptions\n",
    "    #####################################################\n",
    "    superpoint = SuperPoint(sg_config[\"superpoint\"]).eval().to(device)\n",
    "    dict_features_cuda = {}\n",
    "    dict_shapes = {}\n",
    "    dict_images = {}\n",
    "    for X in dataloader:\n",
    "        image, idx, ori_shape = X\n",
    "        image = image[0].to(device)\n",
    "        fname = img_fnames[idx]\n",
    "        key = fname.split('/')[-1]\n",
    "        \n",
    "        with torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            pred = superpoint({'image': image})\n",
    "            dict_features_cuda[key] = pred\n",
    "            dict_shapes[key] = ori_shape\n",
    "            dict_images[key] = image.half()\n",
    "    del superpoint\n",
    "    gc.collect()\n",
    "    \n",
    "    #####################################################\n",
    "    # Matching keypoints\n",
    "    #####################################################\n",
    "    superglue = SuperGlue(sg_config[\"superglue\"]).eval().to(device)\n",
    "    weights = sg_config[\"superglue\"][\"weights\"]\n",
    "    cnt_pairs = 0\n",
    "    \n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:\n",
    "        for idx, (fname1, fname2) in enumerate(zip(fnames1, fnames2)):\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "\n",
    "            data = {\"image0\": dict_images[key1], \"image1\": dict_images[key2]}\n",
    "            data = {**data, **{k+'0': v for k, v in dict_features_cuda[key1].items()}}\n",
    "            data = {**data, **{k+'1': v for k, v in dict_features_cuda[key2].items()}}\n",
    "            for k in data:\n",
    "                if isinstance(data[k], (list, tuple)):\n",
    "                    data[k] = torch.stack(data[k])\n",
    "            with torch.no_grad(), torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "                pred = {**data, **superglue(data)}\n",
    "                pred = {k: v[0].detach().cpu().numpy().copy() for k, v in pred.items()}\n",
    "            mkpts1, mkpts2 = pred[\"keypoints0\"], pred[\"keypoints1\"]\n",
    "            matches, conf = pred[\"matches0\"], pred[\"matching_scores0\"]\n",
    "\n",
    "            valid = matches > -1\n",
    "            mkpts1 = mkpts1[valid]\n",
    "            mkpts2 = mkpts2[matches[valid]]\n",
    "            mconf = conf[valid]\n",
    "\n",
    "            ori_shape_1 = dict_shapes[key1][0].numpy()\n",
    "            ori_shape_2 = dict_shapes[key2][0].numpy()\n",
    "            \n",
    "            # Scaling coords\n",
    "            mkpts1[:,0] = mkpts1[:,0] * ori_shape_1[1] / dict_images[key1].shape[3]   # X\n",
    "            mkpts1[:,1] = mkpts1[:,1] * ori_shape_1[0] / dict_images[key1].shape[2]   # Y\n",
    "            mkpts2[:,0] = mkpts2[:,0] * ori_shape_2[1] / dict_images[key2].shape[3]   # X\n",
    "            mkpts2[:,1] = mkpts2[:,1] * ori_shape_2[0] / dict_images[key2].shape[2]   # Y  \n",
    "            \n",
    "            n_matches = mconf.shape[0]\n",
    "            \n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=np.concatenate([mkpts1, mkpts2], axis=1).astype(np.float32))\n",
    "                cnt_pairs+=1\n",
    "                print (f'{key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair(superglue/{resize_to}/{weights})')            \n",
    "            else:\n",
    "                print (f'{key1}-{key2}: {n_matches} matches --> skipped')            \n",
    "\n",
    "    del superglue\n",
    "    del dict_features_cuda\n",
    "    del dict_images\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0270dbed",
   "metadata": {
    "papermill": {
     "duration": 0.013013,
     "end_time": "2024-05-16T05:46:17.239565",
     "exception": false,
     "start_time": "2024-05-16T05:46:17.226552",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints: DKM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf543a29",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-16T05:46:17.267366Z",
     "iopub.status.busy": "2024-05-16T05:46:17.266880Z",
     "iopub.status.idle": "2024-05-16T05:46:17.294163Z",
     "shell.execute_reply": "2024-05-16T05:46:17.293499Z"
    },
    "papermill": {
     "duration": 0.043347,
     "end_time": "2024-05-16T05:46:17.296074",
     "exception": false,
     "start_time": "2024-05-16T05:46:17.252727",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DKMDataset(Dataset):\n",
    "    def __init__(self, fnames1, fnames2, resize_to, device):\n",
    "        self.fnames1 = fnames1\n",
    "        self.fnames2 = fnames2\n",
    "        self.resize_to = resize_to\n",
    "        self.device = device\n",
    "        self.test_transform = get_tuple_transform_ops(\n",
    "            resize=self.resize_to, normalize=True\n",
    "        )\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fnames1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname1 = self.fnames1[idx]\n",
    "        fname2 = self.fnames2[idx]\n",
    "                \n",
    "        im1, im2 = Image.open(fname1), Image.open(fname2)\n",
    "        ori_shape_1 = im1.size\n",
    "        ori_shape_2 = im2.size\n",
    "        image1, image2 = self.test_transform((im1, im2))\n",
    "        return image1, image2, torch.tensor([idx]), torch.tensor(ori_shape_1), torch.tensor(ori_shape_2)\n",
    "\n",
    "def get_dkm_dataloader(images1, images2, resize_to, device, batch_size=4):\n",
    "    dataset = DKMDataset(images1, images2, resize_to, device)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def get_dkm_mkpts(dkm_model, bimgs1, bimgs2, shapes1, shapes2, detection_threshold=0.5, num_features = 2000, min_matches=15):\n",
    "    dense_matches, dense_certainty = dkm_model.match(bimgs1, bimgs2, batched=True)\n",
    "    print(\"***\", dense_matches.shape, dense_certainty.shape)\n",
    "\n",
    "    store_mkpts1, store_mkpts2, store_mconf = [], [], []\n",
    "    # drop low confidence pairs\n",
    "    for b in range(dense_matches.shape[0]):\n",
    "        u_dense_matches = dense_matches[b, dense_certainty[b,...].sqrt() >= detection_threshold, :]\n",
    "        u_dense_certainty = dense_certainty[b, dense_certainty[b,...].sqrt() >= detection_threshold]\n",
    "    \n",
    "        if u_dense_matches.shape[0] > num_features:\n",
    "            u_dense_matches, u_dense_certainty = dkm_model.sample( u_dense_matches, u_dense_certainty, num=num_features)\n",
    "        \n",
    "        u_dense_matches = u_dense_matches.reshape((-1, 4))\n",
    "        u_dense_certainty = u_dense_certainty.reshape((-1,))\n",
    "    \n",
    "        mkpts1 = u_dense_matches[:, :2]\n",
    "        mkpts2 = u_dense_matches[:, 2:]\n",
    "        \n",
    "        w1, h1 = shapes1[b, :]\n",
    "        w2, h2 = shapes2[b, :]\n",
    "\n",
    "        mkpts1[:, 0] = ((mkpts1[:, 0] + 1)/2) * w1\n",
    "        mkpts1[:, 1] = ((mkpts1[:, 1] + 1)/2) * h1\n",
    "\n",
    "        mkpts2[:, 0] = ((mkpts2[:, 0] + 1)/2) * w2\n",
    "        mkpts2[:, 1] = ((mkpts2[:, 1] + 1)/2) * h2\n",
    "\n",
    "        mkpts1 = mkpts1.cpu().detach().numpy()\n",
    "        mkpts2 = mkpts2.cpu().detach().numpy()\n",
    "        mconf  = u_dense_certainty.sqrt().cpu().detach().numpy()\n",
    "\n",
    "        \n",
    "        if mconf.shape[0] > min_matches:\n",
    "            try:\n",
    "                # calc Fundamental matrix from keypoints\n",
    "                F, inliers = cv2.findFundamentalMat(mkpts1, mkpts2, cv2.USAC_MAGSAC, 0.200, 0.999, 2000)\n",
    "                inliers = inliers > 0\n",
    "                mkpts1 = mkpts1[inliers[:,0]]\n",
    "                mkpts2 = mkpts2[inliers[:,0]]\n",
    "                mconf  = mconf[inliers[:,0]]\n",
    "                #print(\"---\", mconf.shape)\n",
    "                if mconf.shape[0] > 3000:\n",
    "                    rand_idx = np.random.choice(range(mconf.shape[0]), 3000, replace=False)\n",
    "                    mkpts1 = mkpts1[rand_idx, :]\n",
    "                    mkpts2 = mkpts2[rand_idx, :]\n",
    "                    mconf  = mconf[rand_idx]\n",
    "            except:\n",
    "                mkpts1 = np.empty((0,2))\n",
    "                mkpts2 = np.empty((0,2))\n",
    "                mconf = np.empty((0,))\n",
    "        \n",
    "        store_mkpts1.append(mkpts1)\n",
    "        store_mkpts2.append(mkpts2)\n",
    "        store_mconf.append(mconf)\n",
    "    return store_mkpts1, store_mkpts2, store_mconf\n",
    "\n",
    "def detect_dkm(\n",
    "    img_fnames, index_pairs, feature_dir, device, \n",
    "    resize_to=(540, 720), \n",
    "    detection_threshold=0.4, \n",
    "    num_features=2000, \n",
    "    min_matches=15,\n",
    "):\n",
    "    t=time()\n",
    "    dkm_model = DKMv3_outdoor(device=device)\n",
    "    dkm_model.upsample_preds=False\n",
    "\n",
    "    fnames1, fnames2 = [], []\n",
    "    for pair_idx in progress_bar(index_pairs):\n",
    "        idx1, idx2 = pair_idx\n",
    "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "        fnames1.append(fname1)\n",
    "        fnames2.append(fname2)\n",
    "        \n",
    "    cnt_pairs = 0\n",
    "    with h5py.File(f'{feature_dir}/matches_dkm.h5', mode='w') as f_match:    \n",
    "        dataloader = get_dkm_dataloader(fnames1, fnames2, resize_to, device, batch_size=4)\n",
    "        for X in tqdm(dataloader):\n",
    "            images1, images2, idxs, shapes1, shapes2 = X\n",
    "            store_mkpts1, store_mkpts2, store_mconf = get_dkm_mkpts(\n",
    "                dkm_model, images1.to(device), images2.to(device), shapes1, shapes2, \n",
    "                detection_threshold=detection_threshold, num_features = num_features, min_matches=min_matches,\n",
    "            )\n",
    "            \n",
    "            for b in range(images1.shape[0]):\n",
    "                mkpts1 = store_mkpts1[b]\n",
    "                mkpts2 = store_mkpts2[b]\n",
    "                mconf = store_mconf[b]\n",
    "                file1 = fnames1[idxs[b]]\n",
    "                file2 = fnames2[idxs[b]]\n",
    "                key1, key2 = file1.split('/')[-1], file2.split('/')[-1]\n",
    "            \n",
    "                n_matches = mconf.shape[0]\n",
    "                print (f'{key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair(dkm)')            \n",
    "\n",
    "                group  = f_match.require_group(key1)\n",
    "                if n_matches >= min_matches:\n",
    "                    group.create_dataset(key2, data=np.concatenate([mkpts1, mkpts2], axis=1).astype(np.float32))\n",
    "                    cnt_pairs+=1\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e03e24",
   "metadata": {
    "papermill": {
     "duration": 0.012591,
     "end_time": "2024-05-16T05:46:17.321616",
     "exception": false,
     "start_time": "2024-05-16T05:46:17.309025",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints: LoFTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6c48174",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-16T05:46:17.350151Z",
     "iopub.status.busy": "2024-05-16T05:46:17.349751Z",
     "iopub.status.idle": "2024-05-16T05:46:17.378438Z",
     "shell.execute_reply": "2024-05-16T05:46:17.377587Z"
    },
    "papermill": {
     "duration": 0.045838,
     "end_time": "2024-05-16T05:46:17.380619",
     "exception": false,
     "start_time": "2024-05-16T05:46:17.334781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LoFTRDataset(Dataset):\n",
    "    def __init__(self, fnames1, fnames2, idxs1, idxs2, resize_small_edge_to, device):\n",
    "        self.fnames1 = fnames1\n",
    "        self.fnames2 = fnames2\n",
    "        self.keys1 = [ fname.split('/')[-1] for fname in fnames1 ]\n",
    "        self.keys2 = [ fname.split('/')[-1] for fname in fnames2 ]\n",
    "        self.idxs1 = idxs1\n",
    "        self.idxs2 = idxs2\n",
    "        self.resize_small_edge_to = resize_small_edge_to\n",
    "        self.device = device\n",
    "        self.round_unit = 16\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images1)\n",
    "\n",
    "    def load_torch_image(self, fname, device):\n",
    "        img = cv2.imread(fname)\n",
    "        original_shape = img.shape\n",
    "        ratio = self.resize_small_edge_to / min([img.shape[0], img.shape[1]])\n",
    "        w = int(img.shape[1] * ratio) # int( (img.shape[1] * ratio) // self.round_unit * self.round_unit )\n",
    "        h = int(img.shape[0] * ratio) # int( (img.shape[0] * ratio) // self.round_unit * self.round_unit )\n",
    "        img_resized = cv2.resize(img, (w, h))\n",
    "        img_resized = K.image_to_tensor(img_resized, False).float() /255.\n",
    "        img_resized = K.color.bgr_to_rgb(img_resized)\n",
    "        img_resized = K.color.rgb_to_grayscale(img_resized)\n",
    "        return img_resized.to(device), original_shape\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname1 = self.fnames1[idx]\n",
    "        fname2 = self.fnames2[idx]\n",
    "        image1, ori_shape_1 = self.load_torch_image(fname1, device)\n",
    "        image2, ori_shape_2 = self.load_torch_image(fname2, device)\n",
    "\n",
    "        return image1, image2, self.keys1[idx], self.keys2[idx], self.idxs1[idx], self.idxs2[idx], ori_shape_1, ori_shape_2\n",
    "\n",
    "def get_loftr_dataloader(images1, images2, idxs1, idxs2, resize_small_edge_to, device, batch_size=1):\n",
    "    dataset = LoFTRDataset(images1, images2, idxs1, idxs2, resize_small_edge_to, device)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return dataset\n",
    "    \n",
    "def detect_loftr(img_fnames, index_pairs, feature_dir, device, file_keypoints, resize_small_edge_to=750, min_matches=15):\n",
    "    t=time()\n",
    "\n",
    "    matcher = LoFTR(pretrained=None)\n",
    "    matcher.load_state_dict(torch.load(\"../input/loftr/pytorch/outdoor/1/loftr_outdoor.ckpt\")['state_dict'])\n",
    "    matcher = matcher.to(device).eval()\n",
    "\n",
    "    fnames1, fnames2, idxs1, idxs2 = [], [], [], []\n",
    "    for pair_idx in progress_bar(index_pairs):\n",
    "        idx1, idx2 = pair_idx\n",
    "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "        fnames1.append(fname1)\n",
    "        fnames2.append(fname2)\n",
    "        idxs1.append(idx1)\n",
    "        idxs2.append(idx2)\n",
    "        \n",
    "        \n",
    "    dataloader = get_loftr_dataloader( fnames1, fnames2, idxs1, idxs2, resize_small_edge_to, device)\n",
    "\n",
    "    cnt_pairs = 0\n",
    "\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:    \n",
    "        store_mkpts = {}\n",
    "        for X in tqdm(dataloader):\n",
    "            image1, image2, key1, key2, idx1, idx2, ori_shape_1, ori_shape_2 = X\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                correspondences = matcher( {\"image0\": image1.to(device),\"image1\": image2.to(device)} )\n",
    "                mkpts1 = correspondences['keypoints0'].cpu().numpy()\n",
    "                mkpts2 = correspondences['keypoints1'].cpu().numpy()\n",
    "                mconf  = correspondences['confidence'].cpu().numpy()\n",
    "\n",
    "            mkpts1[:,0] *= (float(ori_shape_1[1]) / float(image1.shape[3]))\n",
    "            mkpts1[:,1] *= (float(ori_shape_1[0]) / float(image1.shape[2]))\n",
    "\n",
    "            mkpts2[:,0] *= (float(ori_shape_2[1]) / float(image2.shape[3]))\n",
    "            mkpts2[:,1] *= (float(ori_shape_2[0]) / float(image2.shape[2]))\n",
    "            \n",
    "            n_matches = mconf.shape[0]\n",
    "            \n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=np.concatenate([mkpts1, mkpts2], axis=1).astype(np.float32))\n",
    "                cnt_pairs+=1\n",
    "                print (f'{key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair(loftr)')\n",
    "            else:\n",
    "                print (f'{key1}-{key2}: {n_matches} matches --> skipped')\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b66d23f",
   "metadata": {
    "papermill": {
     "duration": 0.016534,
     "end_time": "2024-05-16T05:46:17.414765",
     "exception": false,
     "start_time": "2024-05-16T05:46:17.398231",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints: DKM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2c0c98f",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-16T05:46:17.452957Z",
     "iopub.status.busy": "2024-05-16T05:46:17.452656Z",
     "iopub.status.idle": "2024-05-16T05:46:17.480341Z",
     "shell.execute_reply": "2024-05-16T05:46:17.479333Z"
    },
    "papermill": {
     "duration": 0.048272,
     "end_time": "2024-05-16T05:46:17.483174",
     "exception": false,
     "start_time": "2024-05-16T05:46:17.434902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DKMDataset(Dataset):\n",
    "    def __init__(self, fnames1, fnames2, resize_to, device):\n",
    "        self.fnames1 = fnames1\n",
    "        self.fnames2 = fnames2\n",
    "        self.resize_to = resize_to\n",
    "        self.device = device\n",
    "        self.test_transform = get_tuple_transform_ops(\n",
    "            resize=self.resize_to, normalize=True\n",
    "        )\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.fnames1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname1 = self.fnames1[idx]\n",
    "        fname2 = self.fnames2[idx]\n",
    "                \n",
    "        im1, im2 = Image.open(fname1), Image.open(fname2)\n",
    "        ori_shape_1 = im1.size\n",
    "        ori_shape_2 = im2.size\n",
    "        image1, image2 = self.test_transform((im1, im2))\n",
    "        return image1, image2, torch.tensor([idx]), torch.tensor(ori_shape_1), torch.tensor(ori_shape_2)\n",
    "\n",
    "def get_dkm_dataloader(images1, images2, resize_to, device, batch_size=4):\n",
    "    dataset = DKMDataset(images1, images2, resize_to, device)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "def get_dkm_mkpts(dkm_model, bimgs1, bimgs2, shapes1, shapes2, detection_threshold=0.5, num_features = 2000, min_matches=15):\n",
    "    dense_matches, dense_certainty = dkm_model.match(bimgs1, bimgs2, batched=True)\n",
    "\n",
    "    store_mkpts1, store_mkpts2, store_mconf = [], [], []\n",
    "    # drop low confidence pairs\n",
    "    for b in range(dense_matches.shape[0]):\n",
    "        u_dense_matches = dense_matches[b, dense_certainty[b,...].sqrt() >= detection_threshold, :]\n",
    "        u_dense_certainty = dense_certainty[b, dense_certainty[b,...].sqrt() >= detection_threshold]\n",
    "    \n",
    "        if u_dense_matches.shape[0] > num_features:\n",
    "            u_dense_matches, u_dense_certainty = dkm_model.sample( u_dense_matches, u_dense_certainty, num=num_features)\n",
    "        \n",
    "        u_dense_matches = u_dense_matches.reshape((-1, 4))\n",
    "        u_dense_certainty = u_dense_certainty.reshape((-1,))\n",
    "    \n",
    "        mkpts1 = u_dense_matches[:, :2]\n",
    "        mkpts2 = u_dense_matches[:, 2:]\n",
    "        \n",
    "        w1, h1 = shapes1[b, :]\n",
    "        w2, h2 = shapes2[b, :]\n",
    "\n",
    "        mkpts1[:, 0] = ((mkpts1[:, 0] + 1)/2) * w1\n",
    "        mkpts1[:, 1] = ((mkpts1[:, 1] + 1)/2) * h1\n",
    "\n",
    "        mkpts2[:, 0] = ((mkpts2[:, 0] + 1)/2) * w2\n",
    "        mkpts2[:, 1] = ((mkpts2[:, 1] + 1)/2) * h2\n",
    "\n",
    "        mkpts1 = mkpts1.cpu().detach().numpy()\n",
    "        mkpts2 = mkpts2.cpu().detach().numpy()\n",
    "        mconf  = u_dense_certainty.sqrt().cpu().detach().numpy()\n",
    "\n",
    "        if mconf.shape[0] > min_matches:\n",
    "            try:\n",
    "                # calc Fundamental matrix from keypoints\n",
    "                F, inliers = cv2.findFundamentalMat(mkpts1, mkpts2, cv2.USAC_MAGSAC, 0.200, 0.999, 2000)\n",
    "                inliers = inliers > 0\n",
    "                mkpts1 = mkpts1[inliers[:,0]]\n",
    "                mkpts2 = mkpts2[inliers[:,0]]\n",
    "                mconf  = mconf[inliers[:,0]]\n",
    "            except:\n",
    "                pass\n",
    "        store_mkpts1.append(mkpts1)\n",
    "        store_mkpts2.append(mkpts2)\n",
    "        store_mconf.append(mconf)\n",
    "    return store_mkpts1, store_mkpts2, store_mconf\n",
    "\n",
    "def detect_dkm(\n",
    "    img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "    resize_to=(540, 720), \n",
    "    detection_threshold=0.4, \n",
    "    num_features=2000, \n",
    "    min_matches=15\n",
    "):\n",
    "    t=time()\n",
    "    dkm_model = DKMv3_outdoor(device=device)\n",
    "    dkm_model.upsample_preds=False\n",
    "\n",
    "    fnames1, fnames2 = [], []\n",
    "    for pair_idx in progress_bar(index_pairs):\n",
    "        idx1, idx2 = pair_idx\n",
    "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "        fnames1.append(fname1)\n",
    "        fnames2.append(fname2)\n",
    "        \n",
    "    cnt_pairs = 0\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:    \n",
    "        dataloader = get_dkm_dataloader(fnames1, fnames2, resize_to, device, batch_size=4)\n",
    "        for X in tqdm(dataloader):\n",
    "            images1, images2, idxs, shapes1, shapes2 = X\n",
    "            store_mkpts1, store_mkpts2, store_mconf = get_dkm_mkpts(\n",
    "                dkm_model, images1.to(device), images2.to(device), shapes1, shapes2, \n",
    "                detection_threshold=detection_threshold, num_features = num_features, min_matches=min_matches,\n",
    "            )\n",
    "            \n",
    "            for b in range(images1.shape[0]):\n",
    "                mkpts1 = store_mkpts1[b]\n",
    "                mkpts2 = store_mkpts2[b]\n",
    "                mconf = store_mconf[b]\n",
    "                file1 = fnames1[idxs[b]]\n",
    "                file2 = fnames2[idxs[b]]\n",
    "                key1, key2 = file1.split('/')[-1], file2.split('/')[-1]\n",
    "            \n",
    "                n_matches = mconf.shape[0]\n",
    "\n",
    "                group  = f_match.require_group(key1)\n",
    "                if n_matches >= min_matches:\n",
    "                    group.create_dataset(key2, data=np.concatenate([mkpts1, mkpts2], axis=1).astype(np.float32))\n",
    "                    cnt_pairs+=1\n",
    "                    print (f'{key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair(dkm)')\n",
    "                else:\n",
    "                    print (f'{key1}-{key2}: {n_matches} matches --> skipped')\n",
    "\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2462da",
   "metadata": {
    "papermill": {
     "duration": 0.013165,
     "end_time": "2024-05-16T05:46:17.518102",
     "exception": false,
     "start_time": "2024-05-16T05:46:17.504937",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints: MatchFormer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15828c09",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-16T05:46:17.546669Z",
     "iopub.status.busy": "2024-05-16T05:46:17.546312Z",
     "iopub.status.idle": "2024-05-16T05:46:17.578888Z",
     "shell.execute_reply": "2024-05-16T05:46:17.577828Z"
    },
    "papermill": {
     "duration": 0.049757,
     "end_time": "2024-05-16T05:46:17.581030",
     "exception": false,
     "start_time": "2024-05-16T05:46:17.531273",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MatchFormerDataset(Dataset):\n",
    "    def __init__(self, fnames1, fnames2, idxs1, idxs2, resize_to, device):\n",
    "        self.fnames1 = fnames1\n",
    "        self.fnames2 = fnames2\n",
    "        self.keys1 = [ fname.split('/')[-1] for fname in fnames1 ]\n",
    "        self.keys2 = [ fname.split('/')[-1] for fname in fnames2 ]\n",
    "        self.idxs1 = idxs1\n",
    "        self.idxs2 = idxs2\n",
    "        self.resize_to = resize_to\n",
    "        self.device = device\n",
    "        self.round_unit = 16\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images1)\n",
    "\n",
    "    def load_torch_image(self, fname, device):\n",
    "        img = cv2.imread(fname)\n",
    "        original_shape = img.shape\n",
    "        #ratio = self.resize_long_edge_to / max([img.shape[0], img.shape[1]])\n",
    "        #w = int(img.shape[1] * ratio)\n",
    "        #h = int(img.shape[0] * ratio)\n",
    "        img_resized = cv2.resize(img, self.resize_to)\n",
    "        img_resized = K.image_to_tensor(img_resized, False).float() /255.\n",
    "        img_resized = K.color.bgr_to_rgb(img_resized)\n",
    "        img_resized = K.color.rgb_to_grayscale(img_resized)\n",
    "        return img_resized.to(device), original_shape\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        fname1 = self.fnames1[idx]\n",
    "        fname2 = self.fnames2[idx]\n",
    "        image1, ori_shape_1 = self.load_torch_image(fname1, device)\n",
    "        image2, ori_shape_2 = self.load_torch_image(fname2, device)\n",
    "\n",
    "        return image1, image2, self.keys1[idx], self.keys2[idx], self.idxs1[idx], self.idxs2[idx], ori_shape_1, ori_shape_2\n",
    "\n",
    "def get_matchformer_dataloader(images1, images2, idxs1, idxs2, resize_to, device, batch_size=1):\n",
    "    dataset = MatchFormerDataset(images1, images2, idxs1, idxs2, resize_to, device)\n",
    "    dataloader = DataLoader(\n",
    "        dataset=dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=batch_size,\n",
    "        pin_memory=True,\n",
    "        num_workers=2,\n",
    "        drop_last=False\n",
    "    )\n",
    "    return dataset\n",
    "    \n",
    "def detect_matchformer(\n",
    "    img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "    resize_to=(560, 750), \n",
    "    detection_threshold=0.4, \n",
    "    num_features=2000, \n",
    "    min_matches=15\n",
    "):\n",
    "    t=time()\n",
    "\n",
    "    sys.path.append('/kaggle/input/matchformer/MatchFormer-main')\n",
    "\n",
    "    from yacs.config import CfgNode as CN\n",
    "    from model.matchformer import Matchformer\n",
    "    from config import defaultmf\n",
    "\n",
    "    cfg = defaultmf.get_cfg_defaults()\n",
    "    cfg.MATCHFORMER.BACKBONE_TYPE = 'largela'\n",
    "    cfg.MATCHFORMER.SCENS = 'outdoor'\n",
    "    cfg.MATCHFORMER.RESOLUTION = (8,2)\n",
    "    cfg.MATCHFORMER.MATCH_COARSE.THR = detection_threshold\n",
    "\n",
    "    def lower_config(yacs_cfg):\n",
    "        if not isinstance(yacs_cfg, CN):\n",
    "            return yacs_cfg\n",
    "        return {k.lower(): lower_config(v) for k, v in yacs_cfg.items()}\n",
    "\n",
    "    _cfg = lower_config(cfg)\n",
    "\n",
    "    matcher_mf = Matchformer(_cfg['matchformer'])\n",
    "\n",
    "    pretrained_ckpt = '/kaggle/input/matchformer/outdoor-large-LA.ckpt'\n",
    "    matcher_mf.load_state_dict({k.replace('matcher.',''):v  for k,v in torch.load(pretrained_ckpt, map_location='cpu').items()})\n",
    "    matcher_mf = matcher_mf.to(device).eval()\n",
    "    \n",
    "    \n",
    "    fnames1, fnames2, idxs1, idxs2 = [], [], [], []\n",
    "    for pair_idx in progress_bar(index_pairs):\n",
    "        idx1, idx2 = pair_idx\n",
    "        fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "        fnames1.append(fname1)\n",
    "        fnames2.append(fname2)\n",
    "        idxs1.append(idx1)\n",
    "        idxs2.append(idx2)\n",
    "        \n",
    "    cnt_pairs = 0\n",
    "    with h5py.File(file_keypoints, mode='w') as f_match:    \n",
    "        dataloader = get_matchformer_dataloader(fnames1, fnames2, idxs1, idxs2, resize_to, device, batch_size=1)\n",
    "        for X in tqdm(dataloader):\n",
    "            image1, image2, key1, key2, idx1, idx2, ori_shape_1, ori_shape_2 = X\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            #print(image1.shape, image2.shape)\n",
    "            input_dict = {\n",
    "                \"image0\": image1, \n",
    "                \"image1\": image2\n",
    "            }\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                matcher_mf(input_dict)\n",
    "\n",
    "            conf = input_dict['mconf'].to('cpu').numpy()\n",
    "            mkpts1 = input_dict['mkpts0_f'].to('cpu').numpy()\n",
    "            mkpts2 = input_dict['mkpts1_f'].to('cpu').numpy()\n",
    "\n",
    "            sorted_idx = np.argsort(-conf)\n",
    "            if len(conf) > num_features:\n",
    "                mkpts1 = mkpts1[sorted_idx[:num_features], :]\n",
    "                mkpts2 = mkpts2[sorted_idx[:num_features], :]\n",
    "\n",
    "            mkpts1[:,0] = mkpts1[:,0] * ori_shape_1[1] / image1.shape[3]\n",
    "            mkpts1[:,1] = mkpts1[:,1] * ori_shape_1[0] / image1.shape[2]\n",
    "\n",
    "            mkpts2[:,0] = mkpts2[:,0] * ori_shape_2[1] / image2.shape[3]\n",
    "            mkpts2[:,1] = mkpts2[:,1] * ori_shape_2[0] / image2.shape[2]\n",
    "                \n",
    "            n_matches = mkpts1.shape[0]\n",
    "\n",
    "            group  = f_match.require_group(key1)\n",
    "            if n_matches >= min_matches:\n",
    "                group.create_dataset(key2, data=np.concatenate([mkpts1, mkpts2], axis=1).astype(np.float32))\n",
    "                cnt_pairs+=1\n",
    "                print (f'{key1}-{key2}: {n_matches} matches @ {cnt_pairs}th pair(MatchFormer)')\n",
    "            else:\n",
    "                print (f'{key1}-{key2}: {n_matches} matches --> skipped')\n",
    "\n",
    "    gc.collect()\n",
    "    t=time() -t \n",
    "    print(f'Features matched in  {t:.4f} sec')\n",
    "    return t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ff6791",
   "metadata": {
    "papermill": {
     "duration": 0.014945,
     "end_time": "2024-05-16T05:46:17.613049",
     "exception": false,
     "start_time": "2024-05-16T05:46:17.598104",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints merger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1eee6ed",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-16T05:46:17.646127Z",
     "iopub.status.busy": "2024-05-16T05:46:17.645752Z",
     "iopub.status.idle": "2024-05-16T05:46:17.698597Z",
     "shell.execute_reply": "2024-05-16T05:46:17.697612Z"
    },
    "papermill": {
     "duration": 0.072265,
     "end_time": "2024-05-16T05:46:17.700708",
     "exception": false,
     "start_time": "2024-05-16T05:46:17.628443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_unique_idxs(A, dim=0):\n",
    "    # https://stackoverflow.com/questions/72001505/how-to-get-unique-elements-and-their-firstly-appeared-indices-of-a-pytorch-tenso\n",
    "    unique, idx, counts = torch.unique(A, dim=dim, sorted=True, return_inverse=True, return_counts=True)\n",
    "    _, ind_sorted = torch.sort(idx, stable=True)\n",
    "    cum_sum = counts.cumsum(0)\n",
    "    cum_sum = torch.cat((torch.tensor([0],device=cum_sum.device), cum_sum[:-1]))\n",
    "    first_indices = ind_sorted[cum_sum]\n",
    "    return first_indices\n",
    "\n",
    "def get_keypoint_from_h5(fp, key1, key2):\n",
    "    rc = -1\n",
    "    try:\n",
    "        kpts = np.array(fp[key1][key2])\n",
    "        rc = 0\n",
    "        return (rc, kpts)\n",
    "    except:\n",
    "        return (rc, None)\n",
    "\n",
    "def get_keypoint_from_multi_h5(fps, key1, key2):\n",
    "    list_mkpts = []\n",
    "    for fp in fps:\n",
    "        rc, mkpts = get_keypoint_from_h5(fp, key1, key2)\n",
    "        if rc == 0:\n",
    "            list_mkpts.append(mkpts)\n",
    "    if len(list_mkpts) > 0:\n",
    "        list_mkpts = np.concatenate(list_mkpts, axis=0)\n",
    "    else:\n",
    "        list_mkpts = None\n",
    "    return list_mkpts\n",
    "\n",
    "def matches_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    save_file,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "):\n",
    "    # open h5 files\n",
    "    fps = [ h5py.File(file, mode=\"r\") for file in files_keypoints ]\n",
    "\n",
    "    with h5py.File(save_file, mode='w') as f_match:\n",
    "        counter = 0\n",
    "        for pair_idx in progress_bar(index_pairs):\n",
    "            idx1, idx2 = pair_idx\n",
    "            fname1, fname2 = img_fnames[idx1], img_fnames[idx2]\n",
    "            key1, key2 = fname1.split('/')[-1], fname2.split('/')[-1]\n",
    "\n",
    "            # extract keypoints\n",
    "            mkpts = get_keypoint_from_multi_h5(fps, key1, key2)\n",
    "            if mkpts is None:\n",
    "                print(f\"skipped key1={key1}, key2={key2}\")\n",
    "                continue\n",
    "\n",
    "            ori_size = mkpts.shape[0]\n",
    "            if mkpts.shape[0] < CONFIG.MERGE_PARAMS[\"min_matches\"]:\n",
    "                continue\n",
    "            \n",
    "            if filter_FundamentalMatrix:\n",
    "                store_inliers = { idx:0 for idx in range(mkpts.shape[0]) }\n",
    "                idxs = np.array(range(mkpts.shape[0]))\n",
    "                for iter in range(filter_iterations):\n",
    "                    try:\n",
    "                        Fm, inliers = cv2.findFundamentalMat(\n",
    "                            mkpts[:,:2], mkpts[:,2:4], cv2.USAC_MAGSAC, 0.15, 0.9999, 20000)\n",
    "                        if Fm is not None:\n",
    "                            inliers = inliers > 0\n",
    "                            inlier_idxs = idxs[inliers[:, 0]]\n",
    "                            #print(inliers.shape, inlier_idxs[:5])\n",
    "                            for idx in inlier_idxs:\n",
    "                                store_inliers[idx] += 1\n",
    "                    except:\n",
    "                        print(f\"Failed to cv2.findFundamentalMat. mkpts.shape={mkpts.shape}\")\n",
    "                inliers = np.array([ count for (idx, count) in store_inliers.items() ]) >= filter_threshold\n",
    "                mkpts = mkpts[inliers]\n",
    "                if mkpts.shape[0] < 15:\n",
    "                    print(f\"skipped key1={key1}, key2={key2}: mkpts.shape={mkpts.shape} after filtered.\")\n",
    "                    continue\n",
    "                #print(f\"filter_FundamentalMatrix: {len(store_inliers)} matches --> {mkpts.shape[0]} matches\")\n",
    "            \n",
    "            \n",
    "            print (f'{key1}-{key2}: {ori_size} --> {mkpts.shape[0]} matches')            \n",
    "            # regist tmp file\n",
    "            group  = f_match.require_group(key1)\n",
    "            group.create_dataset(key2, data=mkpts)\n",
    "            counter += 1\n",
    "    print( f\"Ensembled pairs : {counter} pairs\" )\n",
    "    for fp in fps:\n",
    "        fp.close()\n",
    "\n",
    "def keypoints_merger(\n",
    "    img_fnames,\n",
    "    index_pairs,\n",
    "    files_keypoints,\n",
    "    feature_dir = 'featureout',\n",
    "    filter_FundamentalMatrix = False,\n",
    "    filter_iterations = 10,\n",
    "    filter_threshold = 8,\n",
    "):\n",
    "    save_file = f'{feature_dir}/merge_tmp.h5'\n",
    "    !rm -rf {save_file}\n",
    "    matches_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        save_file,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = filter_FundamentalMatrix,\n",
    "        filter_iterations = filter_iterations,\n",
    "        filter_threshold = filter_threshold,\n",
    "    )\n",
    "        \n",
    "    # Let's find unique loftr pixels and group them together.\n",
    "    kpts = defaultdict(list)\n",
    "    match_indexes = defaultdict(dict)\n",
    "    total_kpts=defaultdict(int)\n",
    "    with h5py.File(save_file, mode='r') as f_match:\n",
    "        for k1 in f_match.keys():\n",
    "            group  = f_match[k1]\n",
    "            for k2 in group.keys():\n",
    "                matches = group[k2][...]\n",
    "                total_kpts[k1]\n",
    "                kpts[k1].append(matches[:, :2])\n",
    "                kpts[k2].append(matches[:, 2:])\n",
    "                current_match = torch.arange(len(matches)).reshape(-1, 1).repeat(1, 2)\n",
    "                current_match[:, 0]+=total_kpts[k1]\n",
    "                current_match[:, 1]+=total_kpts[k2]\n",
    "                total_kpts[k1]+=len(matches)\n",
    "                total_kpts[k2]+=len(matches)\n",
    "                match_indexes[k1][k2]=current_match\n",
    "\n",
    "    for k in kpts.keys():\n",
    "        kpts[k] = np.round(np.concatenate(kpts[k], axis=0))\n",
    "    unique_kpts = {}\n",
    "    unique_match_idxs = {}\n",
    "    out_match = defaultdict(dict)\n",
    "    for k in kpts.keys():\n",
    "        uniq_kps, uniq_reverse_idxs = torch.unique(torch.from_numpy(kpts[k]),dim=0, return_inverse=True)\n",
    "        unique_match_idxs[k] = uniq_reverse_idxs\n",
    "        unique_kpts[k] = uniq_kps.numpy()\n",
    "    for k1, group in match_indexes.items():\n",
    "        for k2, m in group.items():\n",
    "            m2 = deepcopy(m)\n",
    "            m2[:,0] = unique_match_idxs[k1][m2[:,0]]\n",
    "            m2[:,1] = unique_match_idxs[k2][m2[:,1]]\n",
    "            mkpts = np.concatenate([unique_kpts[k1][ m2[:,0]],\n",
    "                                    unique_kpts[k2][  m2[:,1]],\n",
    "                                   ],\n",
    "                                   axis=1)\n",
    "            unique_idxs_current = get_unique_idxs(torch.from_numpy(mkpts), dim=0)\n",
    "            m2_semiclean = m2[unique_idxs_current]\n",
    "            unique_idxs_current1 = get_unique_idxs(m2_semiclean[:, 0], dim=0)\n",
    "            m2_semiclean = m2_semiclean[unique_idxs_current1]\n",
    "            unique_idxs_current2 = get_unique_idxs(m2_semiclean[:, 1], dim=0)\n",
    "            m2_semiclean2 = m2_semiclean[unique_idxs_current2]\n",
    "            out_match[k1][k2] = m2_semiclean2.numpy()\n",
    "    with h5py.File(f'{feature_dir}/keypoints.h5', mode='w') as f_kp:\n",
    "        for k, kpts1 in unique_kpts.items():\n",
    "            f_kp[k] = kpts1\n",
    "    \n",
    "    with h5py.File(f'{feature_dir}/matches.h5', mode='w') as f_match:\n",
    "        for k1, gr in out_match.items():\n",
    "            group  = f_match.require_group(k1)\n",
    "            for k2, match in gr.items():\n",
    "                group[k2] = match\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795288ee",
   "metadata": {
    "papermill": {
     "duration": 0.013591,
     "end_time": "2024-05-16T05:46:17.729785",
     "exception": false,
     "start_time": "2024-05-16T05:46:17.716194",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Keypoints wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3adfed0b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T05:46:17.759855Z",
     "iopub.status.busy": "2024-05-16T05:46:17.759489Z",
     "iopub.status.idle": "2024-05-16T05:46:17.798823Z",
     "shell.execute_reply": "2024-05-16T05:46:17.797794Z"
    },
    "papermill": {
     "duration": 0.057691,
     "end_time": "2024-05-16T05:46:17.801639",
     "exception": false,
     "start_time": "2024-05-16T05:46:17.743948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def wrapper_keypoints(\n",
    "    img_fnames, index_pairs, feature_dir, device, timings, rots\n",
    "):\n",
    "    #############################################################\n",
    "    # get keypoints\n",
    "    #############################################################\n",
    "    files_keypoints = []\n",
    "    \n",
    "    if CONFIG.use_superglue:\n",
    "        for params_sg in CONFIG.params_sgs:\n",
    "            resize_to = params_sg[\"resize_to\"]\n",
    "            file_keypoints = f\"{feature_dir}/matches_superglue_{resize_to}pix.h5\"\n",
    "            !rm -rf {file_keypoints}\n",
    "            t = detect_superglue(\n",
    "                img_fnames, index_pairs, feature_dir, device, \n",
    "                params_sg[\"sg_config\"], file_keypoints, \n",
    "                resize_to=params_sg[\"resize_to\"], \n",
    "                min_matches=params_sg[\"min_matches\"],\n",
    "            )\n",
    "            gc.collect()\n",
    "            files_keypoints.append( file_keypoints )\n",
    "            timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_aliked_lightglue:\n",
    "        model_name = \"aliked\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_aliked_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_aliked_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_aliked_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_aliked_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_doghardnet_lightglue:\n",
    "        model_name = \"doghardnet\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_doghardnet_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_doghardnet_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_doghardnet_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_doghardnet_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_superpoint_lightglue:\n",
    "        model_name = \"superpoint\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_superpoint_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_superpoint_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_superpoint_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_superpoint_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_disk_lightglue:\n",
    "        model_name = \"disk\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_disk_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_disk_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_disk_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_disk_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_sift_lightglue:\n",
    "        model_name = \"sift\"\n",
    "        file_keypoints = f'{feature_dir}/matches_lightglue_{model_name}.h5'\n",
    "        t = detect_lightglue_common(\n",
    "            img_fnames, model_name, index_pairs, feature_dir, device, file_keypoints, rots,\n",
    "            resize_to=CONFIG.params_sift_lightglue[\"resize_to\"],\n",
    "            detection_threshold=CONFIG.params_sift_lightglue[\"detection_threshold\"],\n",
    "            num_features=CONFIG.params_sift_lightglue[\"num_features\"],\n",
    "            min_matches=CONFIG.params_sift_lightglue[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_loftr:\n",
    "        file_keypoints = f'{feature_dir}/matches_loftr_{CONFIG.params_loftr[\"resize_small_edge_to\"]}pix.h5'\n",
    "        t = detect_loftr(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_small_edge_to=CONFIG.params_loftr[\"resize_small_edge_to\"],\n",
    "            min_matches=CONFIG.params_loftr[\"min_matches\"],\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append( file_keypoints )\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_dkm:\n",
    "        file_keypoints = f'{feature_dir}/matches_dkm.h5'\n",
    "        t = detect_dkm(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_to=CONFIG.params_dkm[\"resize_to\"], \n",
    "            detection_threshold=CONFIG.params_dkm[\"detection_threshold\"], \n",
    "            num_features=CONFIG.params_dkm[\"num_features\"], \n",
    "            min_matches=CONFIG.params_dkm[\"min_matches\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append(file_keypoints)\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    if CONFIG.use_matchformer:\n",
    "        file_keypoints = f'{feature_dir}/matches_matchformer_{CONFIG.params_matchformer[\"resize_to\"]}pix.h5'\n",
    "        t = detect_matchformer(\n",
    "            img_fnames, index_pairs, feature_dir, device, file_keypoints,\n",
    "            resize_to=CONFIG.params_matchformer[\"resize_to\"],\n",
    "            num_features=CONFIG.params_matchformer[\"num_features\"], \n",
    "            min_matches=CONFIG.params_matchformer[\"min_matches\"]\n",
    "        )\n",
    "        gc.collect()\n",
    "        files_keypoints.append( file_keypoints )\n",
    "        timings['feature_matching'].append(t)\n",
    "\n",
    "    #############################################################\n",
    "    # merge keypoints\n",
    "    #############################################################\n",
    "    keypoints_merger(\n",
    "        img_fnames,\n",
    "        index_pairs,\n",
    "        files_keypoints,\n",
    "        feature_dir = feature_dir,\n",
    "        filter_FundamentalMatrix = CONFIG.MERGE_PARAMS[\"filter_FundamentalMatrix\"],\n",
    "        filter_iterations = CONFIG.MERGE_PARAMS[\"filter_iterations\"],\n",
    "        filter_threshold = CONFIG.MERGE_PARAMS[\"filter_threshold\"],\n",
    "    )    \n",
    "    return timings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48071c97",
   "metadata": {
    "papermill": {
     "duration": 0.013375,
     "end_time": "2024-05-16T05:46:17.830775",
     "exception": false,
     "start_time": "2024-05-16T05:46:17.817400",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reconstruction wrapper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f9a83ff",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-16T05:46:17.860172Z",
     "iopub.status.busy": "2024-05-16T05:46:17.859231Z",
     "iopub.status.idle": "2024-05-16T05:46:17.879019Z",
     "shell.execute_reply": "2024-05-16T05:46:17.878086Z"
    },
    "papermill": {
     "duration": 0.036811,
     "end_time": "2024-05-16T05:46:17.881091",
     "exception": false,
     "start_time": "2024-05-16T05:46:17.844280",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reconstruct_from_db(dataset, scene, feature_dir, img_dir, timings, image_paths):\n",
    "    scene_result = {}\n",
    "    #############################################################\n",
    "    # regist keypoints from h5 into colmap db\n",
    "    #############################################################\n",
    "    database_path = f'{feature_dir}/colmap.db'\n",
    "    if os.path.isfile(database_path):\n",
    "        os.remove(database_path)\n",
    "    gc.collect()\n",
    "    import_into_colmap(img_dir, feature_dir=feature_dir, database_path=database_path)\n",
    "    output_path = f'{feature_dir}/colmap_rec'\n",
    "\n",
    "    #############################################################\n",
    "    # Calculate fundamental matrix with colmap api\n",
    "    #############################################################\n",
    "    t=time()\n",
    "    options = pycolmap.SiftMatchingOptions()\n",
    "    options.confidence = 0.9999\n",
    "    options.max_num_trials = 20000\n",
    "    pycolmap.match_exhaustive(database_path, sift_options=options)\n",
    "    t=time() - t \n",
    "    timings['RANSAC'].append(t)\n",
    "    print(f'RANSAC in  {t:.4f} sec')\n",
    "\n",
    "    #############################################################\n",
    "    # Execute bundle adjustmnet with colmap api\n",
    "    # --> Bundle adjustment Calcs Camera matrix, R and t\n",
    "    #############################################################\n",
    "    t=time()\n",
    "    # By default colmap does not generate a reconstruction if less than 10 images are registered. Lower it to 3.\n",
    "    mapper_options = pycolmap.IncrementalMapperOptions()\n",
    "    mapper_options.min_model_size = 3\n",
    "    os.makedirs(output_path, exist_ok=True)\n",
    "    maps = pycolmap.incremental_mapping(database_path=database_path, image_path=img_dir, output_path=output_path, options=mapper_options)\n",
    "    print(maps)\n",
    "    clear_output(wait=False)\n",
    "    t=time() - t\n",
    "    timings['Reconstruction'].append(t)\n",
    "    print(f'Reconstruction done in  {t:.4f} sec')\n",
    "\n",
    "    #############################################################\n",
    "    # Extract R,t from maps \n",
    "    #############################################################            \n",
    "    imgs_registered  = 0\n",
    "    best_idx = None\n",
    "    list_num_images = []            \n",
    "    print (\"Looking for the best reconstruction\")\n",
    "    if isinstance(maps, dict):\n",
    "        for idx1, rec in maps.items():\n",
    "            print (idx1, rec.summary())\n",
    "            list_num_images.append( len(rec.images) )\n",
    "            if len(rec.images) > imgs_registered:\n",
    "                imgs_registered = len(rec.images)\n",
    "                best_idx = idx1\n",
    "    list_num_images = np.array(list_num_images)\n",
    "    print(f\"list_num_images = {list_num_images}\")\n",
    "    if best_idx is not None:\n",
    "        print (maps[best_idx].summary())\n",
    "        for k, im in maps[best_idx].images.items():\n",
    "            key1 = f'test/{dataset}/images/{im.name}'\n",
    "            scene_result[key1] = {}\n",
    "            scene_result[key1][\"R\"] = deepcopy(im.rotmat())\n",
    "            scene_result[key1][\"t\"] = deepcopy(np.array(im.tvec))\n",
    "\n",
    "    print(f'Registered: {dataset} / {scene} -> {len(scene_result)} images')\n",
    "    print(f'Total: {dataset} / {scene} -> {len(image_paths)} images')\n",
    "    print(timings)\n",
    "    return scene_result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dad4248",
   "metadata": {
    "papermill": {
     "duration": 0.012811,
     "end_time": "2024-05-16T05:46:17.907136",
     "exception": false,
     "start_time": "2024-05-16T05:46:17.894325",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submission utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f704cec",
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-05-16T05:46:17.939384Z",
     "iopub.status.busy": "2024-05-16T05:46:17.938691Z",
     "iopub.status.idle": "2024-05-16T05:46:17.951276Z",
     "shell.execute_reply": "2024-05-16T05:46:17.950282Z"
    },
    "papermill": {
     "duration": 0.03278,
     "end_time": "2024-05-16T05:46:17.953447",
     "exception": false,
     "start_time": "2024-05-16T05:46:17.920667",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def arr_to_str(a):\n",
    "    return ';'.join([str(x) for x in a.reshape(-1)])\n",
    "\n",
    "# Function to create a submission file.\n",
    "def create_submission(out_results, data_dict):\n",
    "    with open(f'submission.csv', 'w') as f:\n",
    "        f.write('image_path,dataset,scene,rotation_matrix,translation_vector\\n')\n",
    "        for dataset in data_dict:\n",
    "            if dataset in out_results:\n",
    "                res = out_results[dataset]\n",
    "            else:\n",
    "                res = {}\n",
    "            for scene in data_dict[dataset]:\n",
    "                if scene in res:\n",
    "                    scene_res = res[scene]\n",
    "                else:\n",
    "                    scene_res = {\"R\":{}, \"t\":{}}\n",
    "                for image in data_dict[dataset][scene]:\n",
    "                    if image in scene_res:\n",
    "                        print (image)\n",
    "                        R = scene_res[image]['R'].reshape(-1)\n",
    "                        T = scene_res[image]['t'].reshape(-1)\n",
    "                    else:\n",
    "                        R = np.eye(3).reshape(-1)\n",
    "                        T = np.zeros((3))\n",
    "                    f.write(f'{image},{dataset},{scene},{arr_to_str(R)},{arr_to_str(T)}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7a7752",
   "metadata": {
    "papermill": {
     "duration": 0.015689,
     "end_time": "2024-05-16T05:46:18.039860",
     "exception": false,
     "start_time": "2024-05-16T05:46:18.024171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8988c908",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T05:46:18.076762Z",
     "iopub.status.busy": "2024-05-16T05:46:18.076033Z",
     "iopub.status.idle": "2024-05-16T05:46:18.087914Z",
     "shell.execute_reply": "2024-05-16T05:46:18.086629Z"
    },
    "papermill": {
     "duration": 0.034414,
     "end_time": "2024-05-16T05:46:18.090086",
     "exception": false,
     "start_time": "2024-05-16T05:46:18.055672",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "church / church -> 41 images\n"
     ]
    }
   ],
   "source": [
    "src = '/kaggle/input/image-matching-challenge-2024'\n",
    "\n",
    "# Get data from csv.\n",
    "data_dict = {}\n",
    "with open(f'{src}/sample_submission.csv', 'r') as f:\n",
    "    for i, l in enumerate(f):\n",
    "        # Skip header.\n",
    "        if l and i > 0:\n",
    "            image, dataset, scene, _, _ = l.strip().split(',')\n",
    "            if dataset not in data_dict:\n",
    "                data_dict[dataset] = {}\n",
    "            if scene not in data_dict[dataset]:\n",
    "                data_dict[dataset][scene] = []\n",
    "            data_dict[dataset][scene].append(image)\n",
    "            \n",
    "            if CONFIG.DRY_RUN:\n",
    "                if len(data_dict[dataset][scene]) == CONFIG.DRY_RUN_MAX_IMAGES:\n",
    "                    break\n",
    "                    \n",
    "for dataset in data_dict:\n",
    "    for scene in data_dict[dataset]:\n",
    "        print(f'{dataset} / {scene} -> {len(data_dict[dataset][scene])} images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a2dfa78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T05:46:18.122842Z",
     "iopub.status.busy": "2024-05-16T05:46:18.122490Z",
     "iopub.status.idle": "2024-05-16T06:01:46.045297Z",
     "shell.execute_reply": "2024-05-16T06:01:46.044336Z"
    },
    "papermill": {
     "duration": 927.941516,
     "end_time": "2024-05-16T06:01:46.047540",
     "exception": false,
     "start_time": "2024-05-16T05:46:18.106024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction done in  315.5012 sec\n",
      "Looking for the best reconstruction\n",
      "0 Reconstruction:\n",
      "\tnum_reg_images = 38\n",
      "\tnum_cameras = 38\n",
      "\tnum_points3D = 22327\n",
      "\tnum_observations = 129843\n",
      "\tmean_track_length = 5.81551\n",
      "\tmean_observations_per_image = 3416.92\n",
      "\tmean_reprojection_error = 0.947748\n",
      "list_num_images = [38]\n",
      "Reconstruction:\n",
      "\tnum_reg_images = 38\n",
      "\tnum_cameras = 38\n",
      "\tnum_points3D = 22327\n",
      "\tnum_observations = 129843\n",
      "\tmean_track_length = 5.81551\n",
      "\tmean_observations_per_image = 3416.92\n",
      "\tmean_reprojection_error = 0.947748\n",
      "Registered: church / church -> 38 images\n",
      "Total: church / church -> 41 images\n",
      "{'rotation_detection': [5.245208740234375e-06], 'shortlisting': [13.213722467422485], 'feature_detection': [], 'feature_matching': [581.216726064682], 'RANSAC': [7.556228876113892], 'Reconstruction': [315.5012471675873]}\n",
      "  => Merged observations: 0\n",
      "  => Filtered observations: 0\n",
      "  => Changed observations: 0.000031\n",
      "  => Filtered images: 3\n",
      "\n",
      "==============================================================================\n",
      "Finding good initial image pair\n",
      "==============================================================================\n",
      "\n",
      "  => No good initial image pair found.\n",
      "\n",
      "Elapsed time: 5.258 [minutes]\n",
      "test/church/images/00046.png\n",
      "test/church/images/00090.png\n",
      "test/church/images/00092.png\n",
      "test/church/images/00087.png\n",
      "test/church/images/00050.png\n",
      "test/church/images/00068.png\n",
      "test/church/images/00083.png\n",
      "test/church/images/00096.png\n",
      "test/church/images/00069.png\n",
      "test/church/images/00081.png\n",
      "test/church/images/00042.png\n",
      "test/church/images/00018.png\n",
      "test/church/images/00030.png\n",
      "test/church/images/00024.png\n",
      "test/church/images/00032.png\n",
      "test/church/images/00026.png\n",
      "test/church/images/00037.png\n",
      "test/church/images/00008.png\n",
      "test/church/images/00035.png\n",
      "test/church/images/00021.png\n",
      "test/church/images/00010.png\n",
      "test/church/images/00039.png\n",
      "test/church/images/00011.png\n",
      "test/church/images/00013.png\n",
      "test/church/images/00006.png\n",
      "test/church/images/00012.png\n",
      "test/church/images/00029.png\n",
      "test/church/images/00001.png\n",
      "test/church/images/00072.png\n",
      "test/church/images/00066.png\n",
      "test/church/images/00058.png\n",
      "test/church/images/00059.png\n",
      "test/church/images/00111.png\n",
      "test/church/images/00061.png\n",
      "test/church/images/00074.png\n",
      "test/church/images/00102.png\n",
      "test/church/images/00076.png\n",
      "test/church/images/00063.png\n"
     ]
    }
   ],
   "source": [
    "out_results = {}\n",
    "timings = {\n",
    "    \"rotation_detection\" : [],\n",
    "    \"shortlisting\":[],\n",
    "   \"feature_detection\": [],\n",
    "   \"feature_matching\":[],\n",
    "   \"RANSAC\": [],\n",
    "   \"Reconstruction\": []\n",
    "}\n",
    "\n",
    "gc.collect()\n",
    "datasets = []\n",
    "for dataset in data_dict:\n",
    "    datasets.append(dataset)\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor(max_workers=CONFIG.NUM_CORES) as executors:\n",
    "    futures = defaultdict(dict)\n",
    "    for dataset in datasets:\n",
    "        print(dataset)\n",
    "        if dataset not in out_results:\n",
    "            out_results[dataset] = {}\n",
    "        for scene in data_dict[dataset]:\n",
    "            print(scene)\n",
    "            # Fail gently if the notebook has not been submitted and the test data is not populated.\n",
    "            # You may want to run this on the training data in that case?\n",
    "            img_dir = f'{src}/test/{dataset}/images'\n",
    "            if not os.path.exists(img_dir):\n",
    "                continue\n",
    "\n",
    "            out_results[dataset][scene] = {}\n",
    "            img_fnames = [f'{src}/{x}' for x in data_dict[dataset][scene]]\n",
    "            print (f\"Got {len(img_fnames)} images\")\n",
    "            feature_dir = f'featureout/{dataset}_{scene}'\n",
    "            if not os.path.isdir(feature_dir):\n",
    "                os.makedirs(feature_dir, exist_ok=True)\n",
    "\n",
    "            #############################################################\n",
    "            # get image rotations\n",
    "            #############################################################\n",
    "            t = time()\n",
    "            if CONFIG.ROTATION_CORRECTION:\n",
    "                rots = exec_rotation_detection(img_fnames, device)\n",
    "            else:\n",
    "                rots = [ 0 for fname in img_fnames ]\n",
    "            t = time()-t\n",
    "            timings['rotation_detection'].append(t)\n",
    "            print (f'rotation_detection for {len(img_fnames)} images : {t:.4f} sec')\n",
    "            gc.collect()\n",
    "            \n",
    "            #############################################################\n",
    "            # get image pairs\n",
    "            #############################################################\n",
    "            t=time()\n",
    "            index_pairs = get_image_pairs_shortlist(img_fnames,\n",
    "                                  sim_th = 1.0, # should be strict\n",
    "                                  min_pairs = 50, # we select at least min_pairs PER IMAGE with biggest similarity\n",
    "                                  exhaustive_if_less = 50,\n",
    "                                  device=device)\n",
    "            t=time() -t \n",
    "            timings['shortlisting'].append(t)\n",
    "            print (f'{len(index_pairs)}, pairs to match, {t:.4f} sec')\n",
    "            gc.collect()\n",
    "\n",
    "            #############################################################\n",
    "            # get keypoints\n",
    "            #############################################################            \n",
    "            keypoints_timings = wrapper_keypoints(\n",
    "                img_fnames, index_pairs, feature_dir, device, timings, rots\n",
    "            )\n",
    "            timings['feature_matching'] = keypoints_timings['feature_matching']\n",
    "            gc.collect()\n",
    "\n",
    "            #############################################################\n",
    "            # kick COLMAP reconstruction\n",
    "            #############################################################            \n",
    "            futures[dataset][scene] = executors.submit(\n",
    "                reconstruct_from_db, \n",
    "                dataset, scene, feature_dir, img_dir, timings, data_dict[dataset][scene])\n",
    "                \n",
    "    #############################################################\n",
    "    # reconstruction results\n",
    "    #############################################################            \n",
    "    for dataset in datasets:\n",
    "        for scene in data_dict[dataset]:\n",
    "            # wait to complete COLMAP reconstruction\n",
    "            result = futures[dataset][scene].result()\n",
    "            if result is not None:\n",
    "                out_results[dataset][scene] = result   # get R and t from result\n",
    "    \n",
    "    create_submission(out_results, data_dict)\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b1931f",
   "metadata": {
    "papermill": {
     "duration": 0.014472,
     "end_time": "2024-05-16T06:01:46.078691",
     "exception": false,
     "start_time": "2024-05-16T06:01:46.064219",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3fffd8b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T06:01:46.109148Z",
     "iopub.status.busy": "2024-05-16T06:01:46.108857Z",
     "iopub.status.idle": "2024-05-16T06:01:47.067554Z",
     "shell.execute_reply": "2024-05-16T06:01:47.066390Z"
    },
    "papermill": {
     "duration": 0.976555,
     "end_time": "2024-05-16T06:01:47.069826",
     "exception": false,
     "start_time": "2024-05-16T06:01:46.093271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_path,dataset,scene,rotation_matrix,translation_vector\r\n",
      "test/church/images/00046.png,church,church,-0.192965091671778;-0.32446504953550304;-0.9260058882242739;0.3792921886234335;0.8457153630963922;-0.37537043073740284;0.9049319913923635;-0.4236601896008889;-0.04012648379799466,5.442829363102201;4.762564966883927;10.743593649479173\r\n",
      "test/church/images/00090.png,church,church,0.9995818620081286;-0.0036689043694573134;-0.028681706453599014;-0.0011715045258764757;0.9859645871734581;-0.16695047294637438;0.02889167218262898;0.16691426535979487;0.9855480197828472,-0.020753668490384607;0.028516881363942093;-0.5349595778308743\r\n",
      "test/church/images/00092.png,church,church,0.9470938797773196;-0.1207327280462928;-0.29738323972080866;0.14479073723952057;0.9876316428260364;0.06016128737303164;0.2864416612470682;-0.10003672559237578;0.9528608650977201,0.08749221829253297;-0.06431302255299409;-0.5506430914577364\r\n",
      "test/church/images/00087.png,church,church,0.8920822446831179;-0.15807089441169733;-0.423323589067531;0.18439607685063428;0.982613069826133;0.021671221676641823;0.41253770199004214;-0.09739172114070685;0.9057193257792537,0.7741597151903021;-0.020755858224121005;-0.6067410475608975\r\n",
      "test/church/images/00050.png,church,church,0.9230811116435598;0.1723362564789117;0.3438335004471274;-0.20172543577078872;0.978096379862746;0.051325629684166416;-0.32745703518167907;-0.11673768201495847;0.9376269000555599,-2.826441321360291;0.4314066190290923;0.998881972884397\r\n",
      "test/church/images/00068.png,church,church,0.5741634284836163;-0.20727654326794773;-0.7920686788422012;0.5255318936521592;0.8351285713602038;0.16240842974501243;0.6278157262655301;-0.5095063335315427;0.5884307180500772,4.620954314107653;0.4219457804980956;0.6806464376666473\r\n",
      "test/church/images/00083.png,church,church,0.9359709750902758;-0.14176851311645;-0.32227321092096567;0.17595690635122535;0.9811904671749063;0.07940046745703193;0.30495491616936493;-0.13102273014191093;0.943310947296964,1.3806991612567432;-0.04093237782380738;-0.5761666395862917\r\n",
      "test/church/images/00096.png,church,church,0.942772849456295;0.08822282726290137;0.321552619454108;-0.011714410993592925;0.972530055811648;-0.23248239313562238;-0.3332298409553552;0.2154112886845375;0.9179084103570113,-0.9942305164260475;0.015076283179781857;-0.9642468485608949\r\n",
      "test/church/images/00069.png,church,church,0.5867132216892375;-0.20407386242520614;-0.7836590165179584;0.34684520617494724;0.9377948807598202;0.015464946625610876;0.7317554225617287;-0.2808818618129919;0.6209986966620568,4.648393771555719;0.21121442700690302;0.7803013221047728\r\n",
      "test/church/images/00081.png,church,church,0.8906588218117005;-0.12957254149390884;-0.4358185627297244;0.1608192545252602;0.98634854450514;0.03540785402324707;0.4252811193821483;-0.10162433391369054;0.8993377920746274,1.4366470111301812;0.017695048476259755;-0.3316531833991241\r\n",
      "test/church/images/00042.png,church,church,0.7264849379232377;0.19133878644493119;0.6600068967612767;-0.2502055569096962;0.9681783854571449;-0.005271928025400191;-0.6400131360075185;-0.16130741686412697;0.7512410418783337,-4.753180140701536;2.5273013402937066;6.967707982748018\r\n",
      "test/church/images/00018.png,church,church,0.9846042518752535;-0.05867267903899519;-0.16465717088411164;0.08280753763861133;0.9861401454181343;0.1437724775641392;0.15393953000903454;-0.15519384758774424;0.9758163202014555,0.2223443803403765;1.2198765773574773;2.8966656344397794\r\n",
      "test/church/images/00030.png,church,church,0.94650328708274;0.07295048645702348;0.31434018843802325;-0.15516782234718998;0.9569926375654874;0.24512861634962715;-0.2829389942188938;-0.2807905236490994;0.9171162452924195,-2.752128069288049;0.24133448414021513;0.7760764963220587\r\n",
      "test/church/images/00024.png,church,church,0.8120798133811922;0.209237779062111;0.5447439109452471;-0.22852181936411725;0.9729775144450173;-0.033053509025666485;-0.5369396192993042;-0.09764378217557576;0.8379507963055868,0.010652138846766718;0.5554027886748788;2.0586291873035596\r\n",
      "test/church/images/00032.png,church,church,0.9744360130201535;0.08200012639938876;0.20916604839187342;-0.10490610065402627;0.9893540074741302;0.10086306529372402;-0.19866848410091265;-0.12022739773181065;0.9726644880222998,-1.045079555268675;0.5579117361100971;1.6460128355074037\r\n",
      "test/church/images/00026.png,church,church,0.9647596987253086;0.13775210306952643;0.224194294787745;-0.18322980683983628;0.9631986301589128;0.19666020630882605;-0.18885328061987172;-0.2308089187183119;0.9544955114819533,-2.099864403883713;0.6188970961869446;1.1047885426421875\r\n",
      "test/church/images/00037.png,church,church,-0.7840161970415633;0.19287277123379631;0.5900158446118887;-0.08607489778890048;0.9075415094113738;-0.41104685945273395;-0.6147436170852977;-0.37305294907281056;-0.6949257387958545,-0.5497534652692928;0.39206574695388147;3.4176424186192276\r\n",
      "test/church/images/00008.png,church,church,0.9999688418623008;3.367579403285458e-05;0.007893932512353883;-3.6912138560676985e-05;0.9999999153370323;0.00040983340817216183;-0.007893918042564686;-0.00041011202045708093;0.9999687584450166,-0.8208781311522172;1.4006340387566019;5.242617032913109\r\n",
      "test/church/images/00035.png,church,church,-0.4662839771559766;0.20809285679440231;0.8598119652567934;-0.34774584334818065;0.8505800944304029;-0.39444433243846894;-0.8134199905839955;-0.48291910907512364;-0.3242484433400128,-0.5897690173480993;1.1604518200249214;3.3901427632337757\r\n",
      "test/church/images/00021.png,church,church,0.9702876646491588;0.13520688737571876;0.20065130310030663;-0.1508378903891842;0.9864387430849533;0.0647034540339084;-0.18918186660624178;-0.0930467825727862;0.9775236660051924,-1.7782830939129188;0.7796765329681263;2.0450310306389685\r\n",
      "test/church/images/00010.png,church,church,0.997200026796416;-0.018063234135498717;-0.07256601222193089;0.013285534306940119;0.9977449988336843;-0.06579066712346492;0.07359076800391735;0.06464237677358099;0.9951913192897414,-0.577230326411413;0.9693689357599299;4.924401532914146\r\n",
      "test/church/images/00039.png,church,church,0.582242674315951;0.24328796428162247;0.7757605523878573;-0.33462845388964424;0.9413197537340826;-0.044055863144788196;-0.7409569933895191;-0.23394035065740598;0.629487606138083,-1.8381997803298566;0.7845000829569027;2.02267662026628\r\n",
      "test/church/images/00011.png,church,church,0.9994591741642693;0.01253007911321365;0.030403228386035347;-0.01287876072831359;0.9998532132788869;0.011299974247208192;-0.03025717602453451;-0.01168541883294423;0.9994738387200133,0.27960926500057026;1.1614879191563332;4.265147754617713\r\n",
      "test/church/images/00013.png,church,church,0.994055807191671;0.019987579185634763;0.10702125427602384;-0.00789583185314423;0.9936500766483003;-0.1122371641488412;-0.10858502672036437;0.11072498297640433;0.9879014475730931,0.6217316563535393;0.6205590358293419;4.365604786201704\r\n",
      "test/church/images/00006.png,church,church,0.9709617732789494;-0.07293538418919487;0.227845703413895;0.08807969796497311;0.99448075738022;-0.05700868360928056;-0.22243021745571614;0.07542183326887834;0.9720269262880347,0.20876439419244655;1.0441425380607887;5.370365364645326\r\n",
      "test/church/images/00012.png,church,church,0.9990809832639079;0.007731407352070599;-0.04215939065948699;-0.010050255006384132;0.9984318916011594;-0.055070411366153364;0.04166750838158601;0.05544351336348601;0.9975920185983764,0.18898862825989504;0.5322471627030124;2.852590401614842\r\n",
      "test/church/images/00029.png,church,church,0.9980052929589169;-0.006911292959987778;0.06275085063652636;0.005756288163133961;0.9998110225896625;0.018568367047114207;-0.06286732356772778;-0.01817011661587829;0.997856475897008,-0.9575179217463377;0.4356019291292951;1.998536128226897\r\n",
      "test/church/images/00001.png,church,church,0.9465218365931606;0.10463079127201669;0.30520290033042263;-0.12228779242054141;0.9917176703764642;0.039265227466414865;-0.2985667574885402;-0.07448798413751312;0.9514774992304909,1.2833130932785772;1.658438087824698;5.106750429208697\r\n",
      "test/church/images/00098.png,church,church,1.0;0.0;0.0;0.0;1.0;0.0;0.0;0.0;1.0,0.0;0.0;0.0\r\n",
      "test/church/images/00072.png,church,church,0.7487787969590425;-0.1773209843168846;-0.6386607720421384;0.3277680004128213;0.9365518470947034;0.12425286962030327;0.5761062845777245;-0.30237047842735976;0.7593903098181196,3.259937745986133;0.3810111237492951;0.4908783018027015\r\n",
      "test/church/images/00066.png,church,church,0.5436311136343706;-0.23408473997014395;-0.8060208104024141;0.3482528707119425;0.9366644863230935;-0.037142672251555486;0.7636656011171025;-0.2605071487967374;0.5907206404860501,4.612137829915951;0.38265024154588034;0.9592141062597206\r\n",
      "test/church/images/00104.png,church,church,1.0;0.0;0.0;0.0;1.0;0.0;0.0;0.0;1.0,0.0;0.0;0.0\r\n",
      "test/church/images/00058.png,church,church,0.9070527092726436;0.2718505459076158;0.32148509030884836;0.07154279047145204;0.6529618793897902;-0.754004252770043;-0.41489397649606224;0.7069215407338245;0.572821895106848,0.4748917136683144;-0.34253018692168;0.49184321363657096\r\n",
      "test/church/images/00059.png,church,church,-0.14601603654908946;-0.26815307617752415;-0.9522464202122399;0.5694705009196797;0.764307621433803;-0.30255116658259906;0.8089392224714932;-0.5864535681149304;0.04110409702794193,5.815774558763092;2.6100892604077885;4.108654324351488\r\n",
      "test/church/images/00111.png,church,church,0.8652287400058418;0.18026348074171591;0.46785072937720684;-0.26895898081206204;0.9543815564401409;0.12968003457522329;-0.42313153286943794;-0.23803554826490783;0.8742418336214439,-3.5362758142444357;0.9684318084461708;1.4737986175470807\r\n",
      "test/church/images/00061.png,church,church,0.12368224352729607;0.33073287150187086;0.9355845607662524;-0.4316173017993519;0.8668966801212281;-0.24939256360653608;-0.8935374683937444;-0.37296905193718144;0.2499697559177475,-5.535779551279355;2.259270957746468;3.94203438062246\r\n",
      "test/church/images/00060.png,church,church,1.0;0.0;0.0;0.0;1.0;0.0;0.0;0.0;1.0,0.0;0.0;0.0\r\n",
      "test/church/images/00074.png,church,church,0.8553894883085095;-0.16472401213544086;-0.4910955335953581;0.1556117494373555;0.9860134815288951;-0.059685824785394796;0.49405850536991275;-0.025365608001195063;0.8690585591330297,3.3637747337459256;0.14319341883791106;0.22157944736467985\r\n",
      "test/church/images/00102.png,church,church,0.9380300061362259;-0.06858218355012893;-0.33970014967257295;0.02289261836024437;0.9903442805491571;-0.1367264934392179;0.3457971017991392;0.1204769275964021;0.9305426773148274,-0.44618868034093934;0.5216400286005206;2.51827815054768\r\n",
      "test/church/images/00076.png,church,church,0.46341103200956535;-0.24386006941460725;-0.851928683609686;0.3083610844532646;0.9456809000567734;-0.10296153098423469;0.8307608903930397;-0.2149881434240359;0.5134359173065807,4.404090109463464;1.1623239136587415;3.1885697436503366\r\n",
      "test/church/images/00063.png,church,church,0.8818148163199119;-0.12163661576546708;-0.45563929091311284;0.3947627049509063;0.7189594977076963;0.572066121559161;0.2580020087102499;-0.6843257808683093;0.6820067368732181,0.16595074558910924;0.2739904647767758;-0.053753090148423825\r\n"
     ]
    }
   ],
   "source": [
    "!cat submission.csv"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8143495,
     "sourceId": 71885,
     "sourceType": "competition"
    },
    {
     "datasetId": 2058261,
     "sourceId": 3414836,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3117886,
     "sourceId": 5373920,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4628051,
     "sourceId": 7884485,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 170475544,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 170565695,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 174129945,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 175679956,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 175684111,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 176463227,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 2663,
     "sourceId": 3736,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 2742,
     "sourceId": 3840,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 2747,
     "sourceId": 3846,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 14317,
     "sourceId": 17191,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 14611,
     "sourceId": 17555,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1078.719617,
   "end_time": "2024-05-16T06:01:49.747705",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-16T05:43:51.028088",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
