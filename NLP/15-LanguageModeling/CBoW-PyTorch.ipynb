{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXTSugt6ieXh"
      },
      "source": [
        "## Training CBoW Model\n",
        "\n",
        "This notebooks is a part of [AI for Beginners Curriculum](http://aka.ms/ai-beginners)\n",
        "\n",
        "In this example, we will look at training CBoW language model to get our own Word2Vec embedding space. We will use AG News dataset as the source of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "q-UiiJUKaxHj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchtext\n",
        "import os\n",
        "import collections\n",
        "import builtins\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TFbR8CZaTZ1q"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIwC7lI5T-ov"
      },
      "source": [
        "First let's load our dataset and define tokenizer and vocabulary. We will set `vocab_size` to 5000 to limit computations a bit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wdZuygtgiuLG"
      },
      "outputs": [],
      "source": [
        "def load_dataset(ngrams = 1, min_freq = 1, vocab_size = 5000 , lines_cnt = 500):\n",
        "    tokenizer = torchtext.data.utils.get_tokenizer('basic_english')\n",
        "    print(\"Loading dataset...\")\n",
        "    test_dataset, train_dataset  = torchtext.datasets.AG_NEWS(root='./data')\n",
        "    train_dataset = list(train_dataset)\n",
        "    test_dataset = list(test_dataset)\n",
        "    classes = ['World', 'Sports', 'Business', 'Sci/Tech']\n",
        "    print('Building vocab...')\n",
        "    counter = collections.Counter()\n",
        "    for i, (_, line) in enumerate(train_dataset):\n",
        "        counter.update(torchtext.data.utils.ngrams_iterator(tokenizer(line),ngrams=ngrams))\n",
        "        if i == lines_cnt:\n",
        "            break\n",
        "    vocab = torchtext.vocab.Vocab({k: i for i, k in enumerate(dict(counter.most_common(vocab_size)).keys())})\n",
        "    return train_dataset, test_dataset, classes, vocab, tokenizer, counter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from typing import List\n",
        "\n",
        "class Vocab(nn.Module):\n",
        "    def __init__ (self, vocab_list):\n",
        "        self.vocab = {k: i for i, k in enumerate(vocab_list)}\n",
        "        self.vocab.update({'<unk>': len(self.vocab), '<pad>': len(self.vocab) + 1})\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.vocab)\n",
        "    \n",
        "    def __getitem__(self, token: str) -> int:\n",
        "        return self.vocab[token] if token in self.vocab.keys() else self.vocab['<unk>']\n",
        "    \n",
        "    def forward(self, tokens: List[str]) -> List[int]:\n",
        "        return [self.vocab[token] for token in tokens]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading dataset...\n",
            "Building vocab...\n"
          ]
        }
      ],
      "source": [
        "train_dataset, test_dataset, _, vocab, tokenizer, counter = load_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Filter the counter to include only the top 5000 most common tokens\n",
        "most_common_tokens = dict(counter.most_common(5000))\n",
        "\n",
        "# Create the vocabulary\n",
        "vocab = torchtext.vocab.Vocab(collections.Counter(most_common_tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 5000\n"
          ]
        }
      ],
      "source": [
        "vocab\n",
        "print(f\"Vocabulary size: {len(vocab)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "vocab = torchtext.vocab.Vocab(collections.Counter(dict(counter.most_common(5000))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Vocab()"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "1XDYNhG8ToFV"
      },
      "outputs": [],
      "source": [
        "def encode(x, vocabulary, tokenizer = tokenizer):\n",
        "    return [vocabulary[s] for s in tokenizer(x)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'.': 0,\n",
              " 'the': 1,\n",
              " ',': 2,\n",
              " 'to': 3,\n",
              " 'a': 4,\n",
              " 'in': 5,\n",
              " 'of': 6,\n",
              " 's': 7,\n",
              " 'and': 8,\n",
              " 'on': 9,\n",
              " 'for': 10,\n",
              " '(': 11,\n",
              " ')': 12,\n",
              " '-': 13,\n",
              " \"'\": 14,\n",
              " '#39': 15,\n",
              " 'at': 16,\n",
              " 'as': 17,\n",
              " 'reuters': 18,\n",
              " 'that': 19,\n",
              " 'ap': 20,\n",
              " 'it': 21,\n",
              " 'with': 22,\n",
              " 'new': 23,\n",
              " 'its': 24,\n",
              " 'said': 25,\n",
              " 'by': 26,\n",
              " 'is': 27,\n",
              " 'has': 28,\n",
              " 'from': 29,\n",
              " 'an': 30,\n",
              " 'after': 31,\n",
              " 'his': 32,\n",
              " 'will': 33,\n",
              " 'have': 34,\n",
              " 'but': 35,\n",
              " 'athens': 36,\n",
              " 'u': 37,\n",
              " 'olympic': 38,\n",
              " '--': 39,\n",
              " 'he': 40,\n",
              " 'gold': 41,\n",
              " 'monday': 42,\n",
              " 'us': 43,\n",
              " 'their': 44,\n",
              " 'was': 45,\n",
              " 'oil': 46,\n",
              " 'over': 47,\n",
              " 'up': 48,\n",
              " 'more': 49,\n",
              " 'are': 50,\n",
              " 'first': 51,\n",
              " 'tuesday': 52,\n",
              " 'be': 53,\n",
              " 'wednesday': 54,\n",
              " 'they': 55,\n",
              " 'prices': 56,\n",
              " 'two': 57,\n",
              " 'who': 58,\n",
              " '&lt': 59,\n",
              " 'into': 60,\n",
              " 'inc': 61,\n",
              " 'sunday': 62,\n",
              " 'china': 63,\n",
              " 'afp': 64,\n",
              " 'team': 65,\n",
              " 'one': 66,\n",
              " 'quot': 67,\n",
              " 'were': 68,\n",
              " 'out': 69,\n",
              " 'off': 70,\n",
              " 'about': 71,\n",
              " 'can': 72,\n",
              " 'would': 73,\n",
              " 'medal': 74,\n",
              " 'night': 75,\n",
              " 'president': 76,\n",
              " 'not': 77,\n",
              " 'united': 78,\n",
              " 'win': 79,\n",
              " 't': 80,\n",
              " 'york': 81,\n",
              " 'this': 82,\n",
              " '?': 83,\n",
              " 'com': 84,\n",
              " 'million': 85,\n",
              " 'company': 86,\n",
              " 'thursday': 87,\n",
              " 'time': 88,\n",
              " 'yesterday': 89,\n",
              " 'iraq': 90,\n",
              " 'won': 91,\n",
              " 'games': 92,\n",
              " 'all': 93,\n",
              " 'second': 94,\n",
              " 'against': 95,\n",
              " 'end': 96,\n",
              " 'or': 97,\n",
              " 'record': 98,\n",
              " 'world': 99,\n",
              " 'last': 100,\n",
              " 'city': 101,\n",
              " 'says': 102,\n",
              " 'american': 103,\n",
              " 'olympics': 104,\n",
              " 'when': 105,\n",
              " 'could': 106,\n",
              " 'corp': 107,\n",
              " 'cut': 108,\n",
              " 'najaf': 109,\n",
              " 'states': 110,\n",
              " 'than': 111,\n",
              " 'some': 112,\n",
              " 'plans': 113,\n",
              " 'security': 114,\n",
              " 'government': 115,\n",
              " 'three': 116,\n",
              " 'no': 117,\n",
              " 'deal': 118,\n",
              " 'women': 119,\n",
              " 'men': 120,\n",
              " 'which': 121,\n",
              " 'percent': 122,\n",
              " 'people': 123,\n",
              " 'major': 124,\n",
              " 'another': 125,\n",
              " 'price': 126,\n",
              " 'announced': 127,\n",
              " 'day': 128,\n",
              " 'make': 129,\n",
              " 'had': 130,\n",
              " 'years': 131,\n",
              " 'market': 132,\n",
              " 'been': 133,\n",
              " 'google': 134,\n",
              " 'officials': 135,\n",
              " 'week': 136,\n",
              " 'take': 137,\n",
              " 'talks': 138,\n",
              " 'music': 139,\n",
              " '000': 140,\n",
              " 'just': 141,\n",
              " '1': 142,\n",
              " 'back': 143,\n",
              " 'south': 144,\n",
              " 'dollar': 145,\n",
              " 'state': 146,\n",
              " 'greece': 147,\n",
              " 'aug': 148,\n",
              " 'bank': 149,\n",
              " 'space': 150,\n",
              " 'friday': 151,\n",
              " 'british': 152,\n",
              " 'year': 153,\n",
              " 'help': 154,\n",
              " 'before': 155,\n",
              " 'home': 156,\n",
              " 'west': 157,\n",
              " 'bush': 158,\n",
              " 'internet': 159,\n",
              " 'high': 160,\n",
              " 'there': 161,\n",
              " 'chicago': 162,\n",
              " 'what': 163,\n",
              " 'now': 164,\n",
              " 'i': 165,\n",
              " 'police': 166,\n",
              " 'set': 167,\n",
              " 'get': 168,\n",
              " 'game': 169,\n",
              " 'hit': 170,\n",
              " 'saturday': 171,\n",
              " 'between': 172,\n",
              " 'other': 173,\n",
              " 'consumer': 174,\n",
              " 'country': 175,\n",
              " 'london': 176,\n",
              " 'search': 177,\n",
              " 'north': 178,\n",
              " 'four': 179,\n",
              " 'profit': 180,\n",
              " 'n': 181,\n",
              " 'may': 182,\n",
              " 'network': 183,\n",
              " 'service': 184,\n",
              " 'services': 185,\n",
              " 'giant': 186,\n",
              " 'still': 187,\n",
              " 'shares': 188,\n",
              " 'stock': 189,\n",
              " 'phelps': 190,\n",
              " 'troops': 191,\n",
              " 'run': 192,\n",
              " 'data': 193,\n",
              " 'buy': 194,\n",
              " 'court': 195,\n",
              " 'americans': 196,\n",
              " 'federal': 197,\n",
              " 'if': 198,\n",
              " 'your': 199,\n",
              " 'according': 200,\n",
              " 'final': 201,\n",
              " 'michael': 202,\n",
              " 'sox': 203,\n",
              " 'press': 204,\n",
              " 'here': 205,\n",
              " 'israel': 206,\n",
              " 'johnson': 207,\n",
              " 'took': 208,\n",
              " 'investors': 209,\n",
              " 'former': 210,\n",
              " 'workers': 211,\n",
              " 'say': 212,\n",
              " 'wins': 213,\n",
              " 'agency': 214,\n",
              " 'open': 215,\n",
              " 'war': 216,\n",
              " 'during': 217,\n",
              " 'go': 218,\n",
              " 'apple': 219,\n",
              " 'software': 220,\n",
              " 'users': 221,\n",
              " 'retailer': 222,\n",
              " 'latest': 223,\n",
              " 'ibm': 224,\n",
              " 'system': 225,\n",
              " 'red': 226,\n",
              " 'like': 227,\n",
              " 'month': 228,\n",
              " 'canadian': 229,\n",
              " 'fighting': 230,\n",
              " 'chavez': 231,\n",
              " 'national': 232,\n",
              " 'afghan': 233,\n",
              " 'korea': 234,\n",
              " 'investor': 235,\n",
              " 'international': 236,\n",
              " '2': 237,\n",
              " 'washington': 238,\n",
              " 'href=http': 239,\n",
              " '//www': 240,\n",
              " '/a&gt': 241,\n",
              " '#146': 242,\n",
              " 'big': 243,\n",
              " 'russian': 244,\n",
              " '10': 245,\n",
              " '2004': 246,\n",
              " 'five': 247,\n",
              " 'higher': 248,\n",
              " 'beat': 249,\n",
              " 'australia': 250,\n",
              " 'them': 251,\n",
              " 'through': 252,\n",
              " 'shot': 253,\n",
              " 'chips': 254,\n",
              " 'public': 255,\n",
              " 'most': 256,\n",
              " '8': 257,\n",
              " 'best': 258,\n",
              " 'down': 259,\n",
              " 'lead': 260,\n",
              " 'claims': 261,\n",
              " 'many': 262,\n",
              " 'attack': 263,\n",
              " 'reported': 264,\n",
              " 'capital': 265,\n",
              " 'victory': 266,\n",
              " 'conference': 267,\n",
              " 'report': 268,\n",
              " 'cink': 269,\n",
              " 'today': 270,\n",
              " 'minister': 271,\n",
              " 'credit': 272,\n",
              " 'com/fullquote': 273,\n",
              " 'aspx': 274,\n",
              " 'target=/stocks/quickinfo/fullquote&gt': 275,\n",
              " 'energy': 276,\n",
              " 'august': 277,\n",
              " 'soldiers': 278,\n",
              " 'greek': 279,\n",
              " 'hamm': 280,\n",
              " 'rebel': 281,\n",
              " 'fourth': 282,\n",
              " '18': 283,\n",
              " 'unit': 284,\n",
              " 'rules': 285,\n",
              " 'next': 286,\n",
              " 'so': 287,\n",
              " 'microsoft': 288,\n",
              " 'least': 289,\n",
              " 'key': 290,\n",
              " 'used': 291,\n",
              " 'source': 292,\n",
              " 'group': 293,\n",
              " 'computer': 294,\n",
              " 'scientists': 295,\n",
              " 'growth': 296,\n",
              " 'due': 297,\n",
              " 'largest': 298,\n",
              " 'nasa': 299,\n",
              " 'way': 300,\n",
              " 'earnings': 301,\n",
              " 'rise': 302,\n",
              " 'san': 303,\n",
              " 'news': 304,\n",
              " 'holy': 305,\n",
              " 'paul': 306,\n",
              " 'afghanistan': 307,\n",
              " 'hurricane': 308,\n",
              " 'made': 309,\n",
              " 'phone': 310,\n",
              " 'maker': 311,\n",
              " 'under': 312,\n",
              " 'sales': 313,\n",
              " 'britain': 314,\n",
              " 'likely': 315,\n",
              " 'champion': 316,\n",
              " '100': 317,\n",
              " 'top': 318,\n",
              " 'pay': 319,\n",
              " 'ahead': 320,\n",
              " 'coach': 321,\n",
              " 'products': 322,\n",
              " 'eight': 323,\n",
              " 'meters': 324,\n",
              " 'darfur': 325,\n",
              " 'support': 326,\n",
              " 'close': 327,\n",
              " 'basketball': 328,\n",
              " 'blue': 329,\n",
              " 'plan': 330,\n",
              " 'business': 331,\n",
              " 'warned': 332,\n",
              " 'loss': 333,\n",
              " 'firm': 334,\n",
              " 'race': 335,\n",
              " 'got': 336,\n",
              " 'little': 337,\n",
              " 'technology': 338,\n",
              " 'we': 339,\n",
              " 'level': 340,\n",
              " 'fraud': 341,\n",
              " 'life': 342,\n",
              " 'sun': 343,\n",
              " 'minutes': 344,\n",
              " 'man': 345,\n",
              " 'shrine': 346,\n",
              " 'venezuela': 347,\n",
              " 'vote': 348,\n",
              " 'inning': 349,\n",
              " 'boston': 350,\n",
              " 'cleric': 351,\n",
              " 'silver': 352,\n",
              " 'newsfactor': 353,\n",
              " 'round': 354,\n",
              " 'charged': 355,\n",
              " 'co': 356,\n",
              " 'behind': 357,\n",
              " 'results': 358,\n",
              " 'around': 359,\n",
              " 'crisis': 360,\n",
              " 'released': 361,\n",
              " 'near': 362,\n",
              " 'held': 363,\n",
              " 'seven': 364,\n",
              " '17': 365,\n",
              " 'billion': 366,\n",
              " 'england': 367,\n",
              " 'o': 368,\n",
              " 'quarterly': 369,\n",
              " 'agreed': 370,\n",
              " 'evidence': 371,\n",
              " 'peace': 372,\n",
              " 'georgian': 373,\n",
              " 'ossetia': 374,\n",
              " 'georgia': 375,\n",
              " 'election': 376,\n",
              " 'n&lt': 377,\n",
              " 'nearly': 378,\n",
              " 'amazon': 379,\n",
              " 'marathon': 380,\n",
              " 'overtime': 381,\n",
              " 'study': 382,\n",
              " 'also': 383,\n",
              " 'led': 384,\n",
              " 'months': 385,\n",
              " 'though': 386,\n",
              " 'chief': 387,\n",
              " 'card': 388,\n",
              " 'times': 389,\n",
              " 'free': 390,\n",
              " 'keep': 391,\n",
              " 'these': 392,\n",
              " 'part': 393,\n",
              " 'future': 394,\n",
              " 'did': 395,\n",
              " 'western': 396,\n",
              " 'hp': 397,\n",
              " 'update': 398,\n",
              " 'per': 399,\n",
              " 'operating': 400,\n",
              " 'individual': 401,\n",
              " 'because': 402,\n",
              " 'any': 403,\n",
              " 'settlement': 404,\n",
              " 'while': 405,\n",
              " 'recall': 406,\n",
              " 'military': 407,\n",
              " 'far': 408,\n",
              " 'ninth': 409,\n",
              " 'students': 410,\n",
              " 'leading': 411,\n",
              " 'chinese': 412,\n",
              " 'right': 413,\n",
              " 'hong': 414,\n",
              " 'kong': 415,\n",
              " 'accused': 416,\n",
              " 'league': 417,\n",
              " 'running': 418,\n",
              " 'cup': 419,\n",
              " 'sports': 420,\n",
              " 'appeal': 421,\n",
              " 'broadband': 422,\n",
              " 'online': 423,\n",
              " 'agreement': 424,\n",
              " 'research': 425,\n",
              " 'downer': 426,\n",
              " 'great': 427,\n",
              " 'kerry': 428,\n",
              " 'found': 429,\n",
              " 'survey': 430,\n",
              " 'intel': 431,\n",
              " 'quarter': 432,\n",
              " 'militia': 433,\n",
              " 'scandal': 434,\n",
              " 'her': 435,\n",
              " 'sudan': 436,\n",
              " '3': 437,\n",
              " 'patterson': 438,\n",
              " 'terror': 439,\n",
              " 'away': 440,\n",
              " 'mark': 441,\n",
              " 'too': 442,\n",
              " 'past': 443,\n",
              " 'gaza': 444,\n",
              " 'killed': 445,\n",
              " 'net': 446,\n",
              " 'work': 447,\n",
              " 'might': 448,\n",
              " 'number': 449,\n",
              " 'launch': 450,\n",
              " '#36': 451,\n",
              " 'better': 452,\n",
              " 'helps': 453,\n",
              " 'forecast': 454,\n",
              " 'nation': 455,\n",
              " 'department': 456,\n",
              " 'industry': 457,\n",
              " 'well': 458,\n",
              " 'you': 459,\n",
              " 'find': 460,\n",
              " 'do': 461,\n",
              " 'same': 462,\n",
              " 'senior': 463,\n",
              " 'lot': 464,\n",
              " 'think': 465,\n",
              " 'video': 466,\n",
              " 'digital': 467,\n",
              " 'hits': 468,\n",
              " 'mars': 469,\n",
              " 'even': 470,\n",
              " 'biggest': 471,\n",
              " 'since': 472,\n",
              " 'early': 473,\n",
              " 'rivals': 474,\n",
              " 'turn': 475,\n",
              " '4': 476,\n",
              " 'seconds': 477,\n",
              " 'field': 478,\n",
              " 'much': 479,\n",
              " 'ready': 480,\n",
              " 'rest': 481,\n",
              " 'david': 482,\n",
              " 'bill': 483,\n",
              " 'runs': 484,\n",
              " 'him': 485,\n",
              " 'several': 486,\n",
              " 'amid': 487,\n",
              " 'tour': 488,\n",
              " 'visit': 489,\n",
              " 'missed': 490,\n",
              " 'hugo': 491,\n",
              " 'authorities': 492,\n",
              " 'competition': 493,\n",
              " 'beijing': 494,\n",
              " 'trial': 495,\n",
              " 'euro': 496,\n",
              " 'baghdad': 497,\n",
              " 'shiite': 498,\n",
              " 'iraqi': 499,\n",
              " 'rally': 500,\n",
              " 'captain': 501,\n",
              " 'stewart': 502,\n",
              " 'only': 503,\n",
              " 'ever': 504,\n",
              " 'event': 505,\n",
              " 'killing': 506,\n",
              " 'assault': 507,\n",
              " 'linux': 508,\n",
              " 'america': 509,\n",
              " 'administration': 510,\n",
              " 'face': 511,\n",
              " 'among': 512,\n",
              " 'chip': 513,\n",
              " 'japanese': 514,\n",
              " 'sprinters': 515,\n",
              " 'consumers': 516,\n",
              " 'leader': 517,\n",
              " 'office': 518,\n",
              " 'fuel': 519,\n",
              " 'days': 520,\n",
              " 'african': 521,\n",
              " 'yet': 522,\n",
              " 'cuts': 523,\n",
              " 'head': 524,\n",
              " 'hours': 525,\n",
              " 'carly': 526,\n",
              " 'contract': 527,\n",
              " 'crude': 528,\n",
              " 'again': 529,\n",
              " 'heart': 530,\n",
              " 'match': 531,\n",
              " 'homes': 532,\n",
              " 'risk': 533,\n",
              " 'israeli': 534,\n",
              " 'committee': 535,\n",
              " 'tests': 536,\n",
              " 'season': 537,\n",
              " 'hopes': 538,\n",
              " 'gymnastics': 539,\n",
              " 'threat': 540,\n",
              " 'usatoday': 541,\n",
              " 'real': 542,\n",
              " '23': 543,\n",
              " 'debut': 544,\n",
              " 'perfect': 545,\n",
              " 'stores': 546,\n",
              " 'demand': 547,\n",
              " 'threatened': 548,\n",
              " 'show': 549,\n",
              " 'airlines': 550,\n",
              " 'flights': 551,\n",
              " 'putin': 552,\n",
              " 'six': 553,\n",
              " 'winning': 554,\n",
              " 'amp': 555,\n",
              " 'athletics': 556,\n",
              " 'hundreds': 557,\n",
              " 'japan': 558,\n",
              " 'economic': 559,\n",
              " 'third': 560,\n",
              " 'p&gt': 561,\n",
              " '/p&gt': 562,\n",
              " 'cuba': 563,\n",
              " 'unknown': 564,\n",
              " 'opposition': 565,\n",
              " 'checkpoint': 566,\n",
              " 'gatlin': 567,\n",
              " 'effect': 568,\n",
              " 'university': 569,\n",
              " 'bring': 570,\n",
              " 'california': 571,\n",
              " 'air': 572,\n",
              " 'germany': 573,\n",
              " 'both': 574,\n",
              " 'properties': 575,\n",
              " 'wireless': 576,\n",
              " 'los': 577,\n",
              " 'angeles': 578,\n",
              " 'companies': 579,\n",
              " 'standard': 580,\n",
              " 'discount': 581,\n",
              " 'dutch': 582,\n",
              " 'beats': 583,\n",
              " 'download': 584,\n",
              " 'retail': 585,\n",
              " 'europe': 586,\n",
              " 'claim': 587,\n",
              " 'whether': 588,\n",
              " 'ago': 589,\n",
              " 'analysts': 590,\n",
              " 'being': 591,\n",
              " 'process': 592,\n",
              " 'wall': 593,\n",
              " 'try': 594,\n",
              " 'francisco': 595,\n",
              " 'low': 596,\n",
              " '400': 597,\n",
              " 'huge': 598,\n",
              " 'football': 599,\n",
              " 'play': 600,\n",
              " 'ortiz': 601,\n",
              " 'family': 602,\n",
              " 'pulled': 603,\n",
              " 'cash': 604,\n",
              " '50': 605,\n",
              " 'john': 606,\n",
              " 'produce': 607,\n",
              " 'jobs': 608,\n",
              " 'official': 609,\n",
              " 'army': 610,\n",
              " 'kabul': 611,\n",
              " 'base': 612,\n",
              " 'charley': 613,\n",
              " 'fell': 614,\n",
              " 'mobile': 615,\n",
              " 'customers': 616,\n",
              " 'double': 617,\n",
              " 'eighth': 618,\n",
              " 'pc': 619,\n",
              " 'left': 620,\n",
              " 'put': 621,\n",
              " 'continued': 622,\n",
              " 'bid': 623,\n",
              " 'wants': 624,\n",
              " 'businesses': 625,\n",
              " 'rising': 626,\n",
              " 'economy': 627,\n",
              " 'kept': 628,\n",
              " 'stocks': 629,\n",
              " 'foreign': 630,\n",
              " 'ryder': 631,\n",
              " 'bronze': 632,\n",
              " 'nasdaq': 633,\n",
              " 'un': 634,\n",
              " 'bangladesh': 635,\n",
              " 'star': 636,\n",
              " 'charges': 637,\n",
              " 'trade': 638,\n",
              " 'center': 639,\n",
              " '2005': 640,\n",
              " 'focus': 641,\n",
              " 'girafa': 642,\n",
              " 'use': 643,\n",
              " 'russia': 644,\n",
              " 'sex': 645,\n",
              " 'alexander': 646,\n",
              " 'going': 647,\n",
              " 'democratic': 648,\n",
              " 'series': 649,\n",
              " 'australian': 650,\n",
              " 'raising': 651,\n",
              " 'offering': 652,\n",
              " 'injured': 653,\n",
              " 'delegation': 654,\n",
              " 'july': 655,\n",
              " 'sharp': 656,\n",
              " 'costs': 657,\n",
              " 'arab': 658,\n",
              " 'sector': 659,\n",
              " 'fire': 660,\n",
              " 'drugs': 661,\n",
              " 'straight': 662,\n",
              " 'index': 663,\n",
              " 'small': 664,\n",
              " 'region': 665,\n",
              " 'web': 666,\n",
              " 'effort': 667,\n",
              " 'sp2': 668,\n",
              " 'release': 669,\n",
              " 'windows': 670,\n",
              " 'ohio': 671,\n",
              " 'those': 672,\n",
              " 'performance': 673,\n",
              " 'systems': 674,\n",
              " 'ipod': 675,\n",
              " '#145': 676,\n",
              " 'longer': 677,\n",
              " '\\\\$1': 678,\n",
              " 'barrel': 679,\n",
              " 'expected': 680,\n",
              " 'where': 681,\n",
              " 'forces': 682,\n",
              " 'jays': 683,\n",
              " 'swimming': 684,\n",
              " 'medals': 685,\n",
              " 'justin': 686,\n",
              " 'defense': 687,\n",
              " '5': 688,\n",
              " 'tokyo': 689,\n",
              " 'll': 690,\n",
              " 'initial': 691,\n",
              " 'agree': 692,\n",
              " 'rebels': 693,\n",
              " 'posted': 694,\n",
              " 'potential': 695,\n",
              " 'nepal': 696,\n",
              " 'scheduled': 697,\n",
              " 'cycling': 698,\n",
              " 'soccer': 699,\n",
              " 'protest': 700,\n",
              " 'white': 701,\n",
              " 'hollywood': 702,\n",
              " 'without': 703,\n",
              " 'should': 704,\n",
              " 'palestinian': 705,\n",
              " 'above': 706,\n",
              " 'yukos': 707,\n",
              " '#151': 708,\n",
              " '11': 709,\n",
              " 'cancer': 710,\n",
              " 'gets': 711,\n",
              " 'ulmer': 712,\n",
              " 'obesity': 713,\n",
              " 'fears': 714,\n",
              " 'sets': 715,\n",
              " 'date': 716,\n",
              " 'human': 717,\n",
              " 'building': 718,\n",
              " 'starts': 719,\n",
              " 'strike': 720,\n",
              " 'pick': 721,\n",
              " 'went': 722,\n",
              " 'copyright': 723,\n",
              " 'campaign': 724,\n",
              " 'me': 725,\n",
              " 'something': 726,\n",
              " '70': 727,\n",
              " 'roundup': 728,\n",
              " 'confirmed': 729,\n",
              " 'program': 730,\n",
              " 'foaf': 731,\n",
              " 'pgp': 732,\n",
              " 'distribution': 733,\n",
              " 'fingerprint': 734,\n",
              " 'within': 735,\n",
              " 'warns': 736,\n",
              " 'targeted': 737,\n",
              " 'estimates': 738,\n",
              " 'including': 739,\n",
              " 'o&gt': 740,\n",
              " 'networking': 741,\n",
              " 'current': 742,\n",
              " 'local': 743,\n",
              " 'threaten': 744,\n",
              " 'groups': 745,\n",
              " 'irregular': 746,\n",
              " 'relay': 747,\n",
              " 'images': 748,\n",
              " 'begin': 749,\n",
              " 'debate': 750,\n",
              " 'long': 751,\n",
              " 'looking': 752,\n",
              " 'ipo': 753,\n",
              " 'street': 754,\n",
              " 'movement': 755,\n",
              " 'reach': 756,\n",
              " 'law': 757,\n",
              " 'easy': 758,\n",
              " 'see': 759,\n",
              " 'why': 760,\n",
              " 'side': 761,\n",
              " 'continue': 762,\n",
              " 'climb': 763,\n",
              " 've': 764,\n",
              " 'charge': 765,\n",
              " 'al': 766,\n",
              " 'central': 767,\n",
              " 'beating': 768,\n",
              " 'martinez': 769,\n",
              " 'died': 770,\n",
              " 'cp': 771,\n",
              " 'defending': 772,\n",
              " 'sources': 773,\n",
              " 'told': 774,\n",
              " 'vehicles': 775,\n",
              " 'collapse': 776,\n",
              " 'france': 777,\n",
              " 'reserve': 778,\n",
              " 'called': 779,\n",
              " 'iran': 780,\n",
              " 'violence': 781,\n",
              " 'mets': 782,\n",
              " 'sell': 783,\n",
              " 'sent': 784,\n",
              " 'drew': 785,\n",
              " 'braves': 786,\n",
              " 'injury': 787,\n",
              " 'st': 788,\n",
              " 'caracas': 789,\n",
              " 'referendum': 790,\n",
              " 'taiwan': 791,\n",
              " 'player': 792,\n",
              " 'force': 793,\n",
              " 'fighters': 794,\n",
              " 'alaska': 795,\n",
              " 'korean': 796,\n",
              " 'host': 797,\n",
              " 'following': 798,\n",
              " 'briefly': 799,\n",
              " 'four-week': 800,\n",
              " 'worries': 801,\n",
              " 'health': 802,\n",
              " 'india': 803,\n",
              " 'regional': 804,\n",
              " 'ltd': 805,\n",
              " 'step': 806,\n",
              " 'radical': 807,\n",
              " 'al-sadr': 808,\n",
              " 'gains': 809,\n",
              " 'enough': 810,\n",
              " 'haas': 811,\n",
              " 'swim': 812,\n",
              " '0': 813,\n",
              " 'oracle': 814,\n",
              " 'indian': 815,\n",
              " 'water': 816,\n",
              " 'gas': 817,\n",
              " 'across': 818,\n",
              " 'thousands': 819,\n",
              " 'sexual': 820,\n",
              " 'enterprise': 821,\n",
              " 'sign': 822,\n",
              " 'thumbnail': 823,\n",
              " 'cassini': 824,\n",
              " 'moons': 825,\n",
              " 'spacecraft': 826,\n",
              " 'jury': 827,\n",
              " 'attempt': 828,\n",
              " 'aid': 829,\n",
              " 'tell': 830,\n",
              " 'story': 831,\n",
              " 'beach': 832,\n",
              " 'inflation': 833,\n",
              " 'product': 834,\n",
              " 'political': 835,\n",
              " 'kmart': 836,\n",
              " 'majority': 837,\n",
              " '6': 838,\n",
              " 'ian': 839,\n",
              " 'coalition': 840,\n",
              " 'carriers': 841,\n",
              " 'giants': 842,\n",
              " 'province': 843,\n",
              " 'profits': 844,\n",
              " 'tight': 845,\n",
              " 'interest': 846,\n",
              " 'history': 847,\n",
              " 'depot': 848,\n",
              " 'xinhuanet': 849,\n",
              " 'offers': 850,\n",
              " 'dismissed': 851,\n",
              " 'drop': 852,\n",
              " 'soaring': 853,\n",
              " 'crowd': 854,\n",
              " 'order': 855,\n",
              " 'global': 856,\n",
              " 'financial': 857,\n",
              " 'makers': 858,\n",
              " 'total': 859,\n",
              " 'how': 860,\n",
              " 'battery': 861,\n",
              " 'general': 862,\n",
              " 'producer': 863,\n",
              " 'suspects': 864,\n",
              " 'district': 865,\n",
              " 'usa': 866,\n",
              " 'ossetian': 867,\n",
              " 'school': 868,\n",
              " 'jewish': 869,\n",
              " 'period': 870,\n",
              " 'presidential': 871,\n",
              " 'came': 872,\n",
              " 'become': 873,\n",
              " 'all-around': 874,\n",
              " 'return': 875,\n",
              " 'strip': 876,\n",
              " 'palestinians': 877,\n",
              " 'limited': 878,\n",
              " 'every': 879,\n",
              " 'philippine': 880,\n",
              " 'prisoners': 881,\n",
              " 'saying': 882,\n",
              " '7': 883,\n",
              " 'ross': 884,\n",
              " 'less': 885,\n",
              " 'zijlaard-van': 886,\n",
              " 'moorsel': 887,\n",
              " 'hare': 888,\n",
              " 'withdrawal': 889,\n",
              " 'medtronic': 890,\n",
              " 'devices': 891,\n",
              " 'trading': 892,\n",
              " 'strong': 893,\n",
              " 'kill': 894,\n",
              " 'negotiations': 895,\n",
              " 'viduka': 896,\n",
              " 'preliminary': 897,\n",
              " '2008': 898,\n",
              " '19': 899,\n",
              " 'arafat': 900,\n",
              " 'houston': 901,\n",
              " 'siege': 902,\n",
              " 'carolina': 903,\n",
              " 'shipments': 904,\n",
              " 'arm': 905,\n",
              " 'berlusconi': 906,\n",
              " 'chance': 907,\n",
              " '20': 908,\n",
              " 'qantas': 909,\n",
              " 'drug': 910,\n",
              " 'empty': 911,\n",
              " 'changed': 912,\n",
              " 'ran': 913,\n",
              " 'sues': 914,\n",
              " 'electronics': 915,\n",
              " 'cisco': 916,\n",
              " 'recalls': 917,\n",
              " 'batteries': 918,\n",
              " 'davenport': 919,\n",
              " 'republican': 920,\n",
              " 'clemens': 921,\n",
              " 'sudanese': 922,\n",
              " 'fight': 923,\n",
              " 're': 924,\n",
              " 'give': 925,\n",
              " 'oven': 926,\n",
              " 'housing': 927,\n",
              " 'classic': 928,\n",
              " 'nesterenko': 929,\n",
              " 'party': 930,\n",
              " 'cubs': 931,\n",
              " 'hearings': 932,\n",
              " 'executives': 933,\n",
              " 'venezuelan': 934,\n",
              " 'finally': 935,\n",
              " 'panama': 936,\n",
              " 'sap': 937,\n",
              " 'private': 938,\n",
              " 'toronto': 939,\n",
              " 'canada': 940,\n",
              " 'flight': 941,\n",
              " 'short': 942,\n",
              " 'wildfires': 943,\n",
              " 'barely': 944,\n",
              " 'shift': 945,\n",
              " 'already': 946,\n",
              " 'places': 947,\n",
              " 'calif': 948,\n",
              " 'southern': 949,\n",
              " 'reduce': 950,\n",
              " 'education': 951,\n",
              " 'recently': 952,\n",
              " 'launched': 953,\n",
              " 'popular': 954,\n",
              " 'our': 955,\n",
              " 'thing': 956,\n",
              " 'area': 957,\n",
              " 'terrorism': 958,\n",
              " 'jaschan': 959,\n",
              " 'netsky': 960,\n",
              " '25': 961,\n",
              " 'bloom': 962,\n",
              " 'whitelist': 963,\n",
              " 'include': 964,\n",
              " 'file': 965,\n",
              " 'based': 966,\n",
              " 'targets': 967,\n",
              " 'stolen': 968,\n",
              " 'texas': 969,\n",
              " 'launches': 970,\n",
              " 'designed': 971,\n",
              " 'let': 972,\n",
              " 'unveiled': 973,\n",
              " 'flagship': 974,\n",
              " 'pro': 975,\n",
              " 'amsterdam': 976,\n",
              " 'discovered': 977,\n",
              " 'ganymede': 978,\n",
              " 'european': 979,\n",
              " 'pictures': 980,\n",
              " 'joint': 981,\n",
              " 'capabilities': 982,\n",
              " 'storage': 983,\n",
              " 'servers': 984,\n",
              " 'share': 985,\n",
              " 'code': 986,\n",
              " 'bidding': 987,\n",
              " '28': 988,\n",
              " 'isn': 989,\n",
              " 'case': 990,\n",
              " 'sluggish': 991,\n",
              " 'power': 992,\n",
              " 'vulnerable': 993,\n",
              " 'components': 994,\n",
              " 'medley': 995,\n",
              " '26': 996,\n",
              " 'developed': 997,\n",
              " 'fat': 998,\n",
              " 'shoppach': 999,\n",
              " ...}"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "{k: i for i, k in enumerate(dict(counter.most_common(5000)).keys())}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torchtext._torchtext.Vocab at 0x7f9ba0842970>"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab.vocab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIlQk6_PaHVY"
      },
      "source": [
        "## CBoW Model\n",
        "\n",
        "CBoW learns to predict a word based on the $2N$ neighboring words. For example, when $N=1$, we will get the following pairs from the sentence *I like to train networks*: (like,I), (I, like), (to, like), (like,to), (train,to), (to, train), (networks, train), (train,networks). Here, first word is the neighboring word used as an input, and second word is the one we are predicting.\n",
        "\n",
        "To build a network to predict next word, we will need to supply neighboring word as input, and get word number as output. The architecture of CBoW network is the following:\n",
        "\n",
        "* Input word is passed through the embedding layer. This very embedding layer would be our Word2Vec embedding, thus we will define it separately as `embedder` variable. We will use embedding size = 30 in this example, even though you might want to experiment with higher dimensions (real word2vec has 300)\n",
        "* Embedding vector would then be passed to a linear layer that will predict output word. Thus it has the `vocab_size` neurons.\n",
        "\n",
        "For the output, if we use `CrossEntropyLoss` as loss function, we would also have to provide just word numbers as expected results, without one-hot encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "akKTcKQKkfl2",
        "outputId": "da687e3e-a8ec-4c1a-e456-ab8cd6ac7dad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sequential(\n",
            "  (0): Embedding(5000, 30)\n",
            "  (1): Linear(in_features=30, out_features=5000, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "vocab_size = len(vocab)\n",
        "\n",
        "embedder = torch.nn.Embedding(num_embeddings = vocab_size, embedding_dim = 30)\n",
        "model = torch.nn.Sequential(\n",
        "    embedder,\n",
        "    torch.nn.Linear(in_features = 30, out_features = vocab_size),\n",
        ")\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nud6jgGPaHVa"
      },
      "source": [
        "## Preparing Training Data\n",
        "\n",
        "Now let's program the main function that will compute CBoW word pairs from text. This function will allow us to specify window size, and will return a set of pairs - input and output word. Note that this function can be used on words, as well as on vectors/tensors - which will allow us to encode the text, before passing it to `to_cbow` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-dsXygOieXn",
        "outputId": "c2218280-e540-40ba-9546-efe48d0d714f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['like', 'I'], ['to', 'I'], ['I', 'like'], ['to', 'like'], ['train', 'like'], ['I', 'to'], ['like', 'to'], ['train', 'to'], ['networks', 'to'], ['like', 'train'], ['to', 'train'], ['networks', 'train'], ['to', 'networks'], ['train', 'networks']]\n",
            "[[11, 14], [530, 14], [14, 11], [530, 11], [0, 11], [14, 530], [11, 530], [0, 530], [3, 530], [11, 0], [530, 0], [3, 0], [530, 3], [0, 3]]\n"
          ]
        }
      ],
      "source": [
        "def to_cbow(sent,window_size=2):\n",
        "    res = []\n",
        "    for i,x in enumerate(sent):\n",
        "        for j in range(max(0,i-window_size),min(i+window_size+1,len(sent))):\n",
        "            if i!=j:\n",
        "                res.append([sent[j],x])\n",
        "    return res\n",
        "\n",
        "print(to_cbow(['I','like','to','train','networks']))\n",
        "print(to_cbow(encode('I like to train networks', vocab)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[['like', 'I'],\n",
              " ['to', 'I'],\n",
              " ['I', 'like'],\n",
              " ['to', 'like'],\n",
              " ['train', 'like'],\n",
              " ['I', 'to'],\n",
              " ['like', 'to'],\n",
              " ['train', 'to'],\n",
              " ['networks', 'to'],\n",
              " ['like', 'train'],\n",
              " ['to', 'train'],\n",
              " ['networks', 'train'],\n",
              " ['to', 'networks'],\n",
              " ['train', 'networks']]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "to_cbow(['I','like','to','train','networks'], window_size=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[14, 11, 530, 0, 3]"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encode('I like to train networks', vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVaaDLjaaHVb"
      },
      "source": [
        "Let's prepare the training dataset. We will go through all news, call `to_cbow` to get the list of word pairs, and add those pairs to `X` and `Y`. For the sake of time, we will only consider first 10k news items - you can easily remove the limitation in case you have more time to wait, and want to get better embeddings :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "54b-Gd9TieXo"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "Y = []\n",
        "for i, x in zip(range(10000), train_dataset):\n",
        "    for w1, w2 in to_cbow(encode(x[1], vocab), window_size = 5):\n",
        "        X.append(w1)\n",
        "        Y.append(w2)\n",
        "\n",
        "X = torch.tensor(X)\n",
        "Y = torch.tensor(Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwWy0PzXWhN5"
      },
      "source": [
        "We will also convert that data to one dataset, and create dataloader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "mfoAcGPFZU8p"
      },
      "outputs": [],
      "source": [
        "class SimpleIterableDataset(torch.utils.data.IterableDataset):\n",
        "    def __init__(self, X, Y):\n",
        "        super(SimpleIterableDataset).__init__()\n",
        "        self.data = []\n",
        "        for i in range(len(X)):\n",
        "            self.data.append( (Y[i], X[i]) )\n",
        "        random.shuffle(self.data)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4NQ_-5waHVc"
      },
      "source": [
        "We will also convert that data to one dataset, and create dataloader:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "AbLUcojlieXo"
      },
      "outputs": [],
      "source": [
        "ds = SimpleIterableDataset(X, Y)\n",
        "dl = torch.utils.data.DataLoader(ds, batch_size = 256)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKQr7sXeaHVc"
      },
      "source": [
        "Now let's do the actual training. We will use `SGD` optimizer with pretty high learning rate. You can also try playing around with other optimizers, such as `Adam`. We will train for 10 epochs to begin with - and you can re-run this cell if you want even lower loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "HeeCYKr_KF1w"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def train_epoch(net, dataloader, optimizer, loss_fn, epochs, device=device, report_freq=1):\n",
        "    # Move model to the specified device (GPU or CPU)\n",
        "    net = net.to(device)\n",
        "    loss_fn = loss_fn.to(device)  # Ensure the loss function is also on the same device\n",
        "    net.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0.0\n",
        "        batch_count = 0\n",
        "\n",
        "        for labels, features in dataloader:\n",
        "            # Move data to the specified device\n",
        "            features, labels = features.to(device), labels.to(device)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = net(features)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = loss_fn(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss\n",
        "            total_loss += loss.item()\n",
        "            batch_count += 1\n",
        "\n",
        "        # Report average loss for the epoch\n",
        "        if epoch % report_freq == 0:\n",
        "            print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {total_loss / batch_count:.4f}\")\n",
        "\n",
        "    return total_loss / batch_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVgwGtDHgDlT",
        "outputId": "2447833f-f0e3-4566-c33d-addbfe2f451d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 3.9329\n",
            "Epoch [2/10], Loss: 3.6152\n",
            "Epoch [3/10], Loss: 3.6075\n",
            "Epoch [4/10], Loss: 3.6048\n",
            "Epoch [5/10], Loss: 3.6033\n",
            "Epoch [6/10], Loss: 3.6023\n",
            "Epoch [7/10], Loss: 3.6016\n",
            "Epoch [8/10], Loss: 3.6010\n",
            "Epoch [9/10], Loss: 3.6005\n",
            "Epoch [10/10], Loss: 3.6002\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "3.6001607200101673"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_epoch(net = model, dataloader = dl, optimizer = torch.optim.SGD(model.parameters(), lr = 0.1), loss_fn = torch.nn.CrossEntropyLoss(), epochs = 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8u2qXZmaHVd"
      },
      "source": [
        "## Trying out Word2Vec\n",
        "\n",
        "To use Word2Vec, let's extract vectors corresponding to all words in our vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torchtext.vocab import vocab as Vocab\n",
        "from collections import Counter\n",
        "\n",
        "# Assuming `counter` is your word frequency counter\n",
        "# Create a vocabulary from the counter\n",
        "most_common_tokens = Counter(dict(counter.most_common(5000)))\n",
        "vocab = Vocab(most_common_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens = vocab.get_itos()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "r8TatcXjkU_t"
      },
      "outputs": [],
      "source": [
        "vectors = torch.stack(\n",
        "    [embedder(torch.tensor(vocab[token]).to(device)) for token in tokens], dim=0\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OcX21UOaHVd"
      },
      "source": [
        "Let's see, for example, how the word **Paris** is encoded into a vector:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bz6tAeLzieXp",
        "outputId": "5b20850e-4342-45e9-f840-cfac2b4d61d8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([-1.5799, -0.6555, -0.3048,  0.7853, -1.3623,  3.3791,  0.8314, -0.4976,\n",
            "         0.0909, -0.0541,  0.7252,  0.4757,  2.4713,  0.4871, -0.7350, -0.1321,\n",
            "        -0.4680,  0.2433, -2.4521,  0.7610,  0.6885,  0.7293, -0.7003, -1.4590,\n",
            "         1.1957, -0.7732,  0.3247,  0.7781, -0.3802, -0.4512], device='cuda:0',\n",
            "       grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ],
      "source": [
        "paris_vec = embedder(torch.tensor(vocab['paris']).to(device))\n",
        "print(paris_vec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHTJlaeYaHVd"
      },
      "source": [
        "It is interesting to use Word2Vec to look for synonyms. The following function will return `n` closest words to a given input. To find them, we compute the norm of $|w_i - v|$, where $v$ is the vector corresponding to our input word, and $w_i$ is the encoding of $i$-th word in the vocabulary. We then sort the array and return corresponding indices using `argsort`, and take first `n` elements of the list, which encode positions of closest words in the vocabulary.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NlZyi-_olFar",
        "outputId": "b5dbb163-88c4-4d5a-eaf2-6751f700e98c"
      },
      "outputs": [],
      "source": [
        "def close_words(x, n=5):\n",
        "    # Ensure all computations are performed on the same device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    \n",
        "    # Move vectors to the correct device\n",
        "    vec = embedder(torch.tensor(vocab[x]).to(device))\n",
        "    vectors_on_device = vectors.to(device)\n",
        "    \n",
        "    # Compute distances and find closest words\n",
        "    top5 = np.linalg.norm(\n",
        "        vectors_on_device.detach().cpu().numpy() - vec.detach().cpu().numpy(),\n",
        "        axis=1\n",
        "    ).argsort()[:n]\n",
        "    \n",
        "    # Map indices to tokens using the appropriate method\n",
        "    try:\n",
        "        tokens = vocab.get_itos()  # Use for modern versions\n",
        "    except AttributeError:\n",
        "        tokens = vocab.itos  # Use for older versions\n",
        "    \n",
        "    # Return the closest tokens\n",
        "    return [tokens[x] for x in top5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dQq7xeAln0U",
        "outputId": "66f768c3-c248-4bfd-ce4f-c8ffc6d0dd0d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['basketball', 'qualifying', 'tang', 'expand', 'holding']"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "close_words('basketball')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJXqK26b29sa",
        "outputId": "78f0baba-ffd0-485a-dd87-0a12bedfd7fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['funds', 'pitch', 'weary', 'maoist', 'm']"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "close_words('funds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "My0VeTDd3Ji8"
      },
      "source": [
        "## Takeaway\n",
        "\n",
        "Using clever techniques such as CBoW, we can train Word2Vec model. You may also try to train skip-gram model that is trained to predict the neighboring word given the central one, and see how well it performs. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "CBoW-PyTorch.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "interpreter": {
      "hash": "16af2a8bbb083ea23e5e41c7f5787656b2ce26968575d8763f2c4b17f9cd711f"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('py38')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
