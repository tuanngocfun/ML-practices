{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "large gpu ram, but utilization = 10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 386\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mActions are implied by policy, reward =\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m-\u001b[39mT_opt)\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 386\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 356\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    352\u001b[0m rewards_log \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_iterations):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;66;03m# sample a batch\u001b[39;00m\n\u001b[0;32m--> 356\u001b[0m     states_buf, actions_buf, probs_buf, lengths \u001b[38;5;241m=\u001b[39m \u001b[43msample_batch_episodes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_steps\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;66;03m# off-policy update\u001b[39;00m\n\u001b[1;32m    361\u001b[0m     offpolicy_update(states_buf, actions_buf, probs_buf, lengths)\n",
      "Cell \u001b[0;32mIn[17], line 210\u001b[0m, in \u001b[0;36msample_batch_episodes\u001b[0;34m(batch_size, epsilon, noise, max_steps)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# update each environment if not done\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m b \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done[b]:\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;66;03m# if final_actions[b] == -1 => skip\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# GPU Setup\n",
    "# ------------------------------------------------------\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Environment/Task Parameters\n",
    "# ------------------------------------------------------\n",
    "ROWS, COLS = 32, 17\n",
    "VEL_LEN = 5  # velocities from 0..4\n",
    "ACTIONS = [\n",
    "    (0, 0),  (0, 1),  (0, -1),\n",
    "    (1, 0),  (1, 1),  (1, -1),\n",
    "    (-1, 0), (-1, 1), (-1, -1)\n",
    "]\n",
    "ACT_LEN = len(ACTIONS)\n",
    "\n",
    "# Monte Carlo control parameters\n",
    "epsilon = 0.1\n",
    "gamma = 1.0\n",
    "R_step = -1  # reward per step\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Create track on GPU (1 = boundary/off-track)\n",
    "# ------------------------------------------------------\n",
    "track = torch.zeros((ROWS, COLS), dtype=torch.int, device=device)\n",
    "track[31, 0:3] = 1\n",
    "track[30, 0:2] = 1\n",
    "track[29, 0:2] = 1\n",
    "track[28, 0]   = 1\n",
    "track[0:18, 0] = 1\n",
    "track[0:10, 1] = 1\n",
    "track[0:3,  2] = 1\n",
    "track[0:26, 9:] = 1\n",
    "track[25, 9]   = 0\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Define starting columns and finish set\n",
    "# (Keep finish as a Python set for membership checks)\n",
    "# ------------------------------------------------------\n",
    "start_cols = list(range(3, 9))  # [3,4,5,6,7,8]\n",
    "finish_cells = {\n",
    "    (26, COLS-1), (27, COLS-1), (28, COLS-1),\n",
    "    (29, COLS-1), (30, COLS-1), (31, COLS-1)\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Precompute valid actions for each (h,v)\n",
    "# (Still a Python list-of-lists for quick referencing)\n",
    "# ------------------------------------------------------\n",
    "valid_actions = []\n",
    "for h in range(VEL_LEN):\n",
    "    row = []\n",
    "    for v in range(VEL_LEN):\n",
    "        vacts = []\n",
    "        for i, (dh, dv) in enumerate(ACTIONS):\n",
    "            nh = h + dh\n",
    "            nv = v + dv\n",
    "            # velocity must remain in [0..4], cannot be (0,0) except at start\n",
    "            if 0 <= nh < VEL_LEN and 0 <= nv < VEL_LEN and not (nh == 0 and nv == 0):\n",
    "                vacts.append(i)\n",
    "        row.append(vacts)\n",
    "    valid_actions.append(row)\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Initialize Q, C, and policy on GPU\n",
    "#   Q, C ~ shape (ROWS, COLS, VEL_LEN, VEL_LEN, ACT_LEN)\n",
    "#   policy ~ shape (ROWS, COLS, VEL_LEN, VEL_LEN)\n",
    "# ------------------------------------------------------\n",
    "Q = (torch.rand((ROWS, COLS, VEL_LEN, VEL_LEN, ACT_LEN), device=device) * 400) - 500\n",
    "C = torch.zeros((ROWS, COLS, VEL_LEN, VEL_LEN, ACT_LEN), device=device)\n",
    "policy = torch.zeros((ROWS, COLS, VEL_LEN, VEL_LEN), dtype=torch.long, device=device)\n",
    "\n",
    "# Initialize policy with argmax over Q\n",
    "for r in range(ROWS):\n",
    "    for c in range(COLS):\n",
    "        for h in range(VEL_LEN):\n",
    "            for v in range(VEL_LEN):\n",
    "                best_action = torch.argmax(Q[r, c, h, v]).item()\n",
    "                policy[r, c, h, v] = best_action\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Utility: create random start states in parallel\n",
    "#   Each start state: row=0, col in [3..8], velocity=(1,1)\n",
    "# ------------------------------------------------------\n",
    "def random_start_states(batch_size):\n",
    "    \"\"\"\n",
    "    Return a (batch_size, 4) LongTensor of states (row, col, h, v).\n",
    "    On GPU, with row=0, col random from [3..8], h=1, v=1.\n",
    "    \"\"\"\n",
    "    states = torch.zeros((batch_size, 4), dtype=torch.long, device=device)\n",
    "    states[:, 0] = 0  # row=0\n",
    "    # random columns from [3..8]\n",
    "    cols = torch.randint(low=3, high=9, size=(batch_size,), device=device)\n",
    "    states[:, 1] = cols\n",
    "    # h=1, v=1\n",
    "    states[:, 2] = 1\n",
    "    states[:, 3] = 1\n",
    "    return states\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# We want to sample many episodes in parallel (batch).\n",
    "# We'll store the entire trajectory in GPU tensors of shape [batch_size, max_steps].\n",
    "# Then do the backward MC update.\n",
    "#\n",
    "# \"Episode\" ends for an environment when it crosses finish line.\n",
    "# If it hits boundary, it resets to a new start, continuing the same \"episode\".\n",
    "# This is the usual racetrack logic, but in parallel.\n",
    "# ------------------------------------------------------\n",
    "\n",
    "def sample_batch_episodes(batch_size, epsilon, noise=True, max_steps=2000):\n",
    "    \"\"\"\n",
    "    Simulate 'batch_size' environments in parallel up to max_steps.\n",
    "    Return:\n",
    "      states_buffer:  (batch_size, max_steps, 4)   int\n",
    "      actions_buffer: (batch_size, max_steps)      int\n",
    "      probs_buffer:   (batch_size, max_steps)      float\n",
    "      lengths:        (batch_size,) how many steps each env took before finishing\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    states = random_start_states(batch_size)  # shape [batch_size, 4]\n",
    "    done = torch.zeros(batch_size, dtype=torch.bool, device=device)\n",
    "    lengths = torch.zeros(batch_size, dtype=torch.long, device=device)  # track how many steps each env used\n",
    "\n",
    "    # Buffers to store transitions\n",
    "    states_buffer = torch.zeros((batch_size, max_steps, 4), dtype=torch.long, device=device)\n",
    "    actions_buffer = torch.zeros((batch_size, max_steps), dtype=torch.long, device=device)\n",
    "    probs_buffer = torch.zeros((batch_size, max_steps), dtype=torch.float, device=device)\n",
    "\n",
    "    for step_i in range(max_steps):\n",
    "        # record current states\n",
    "        states_buffer[:, step_i] = states\n",
    "\n",
    "        # pick the policy action (greedy)\n",
    "        chosen_actions = policy[\n",
    "            states[:, 0],\n",
    "            states[:, 1],\n",
    "            states[:, 2],\n",
    "            states[:, 3]\n",
    "        ].clone()  # shape [batch_size], int\n",
    "\n",
    "        # We'll do epsilon-soft + noise\n",
    "        # 1) figure out valid actions for each environment\n",
    "        # 2) decide if we pick random or policy\n",
    "        # 3) handle noise override with prob=0.1\n",
    "\n",
    "        # random mask for epsilon\n",
    "        rand_vals = torch.rand(batch_size, device=device)  # in [0,1)\n",
    "        noise_vals = torch.rand(batch_size, device=device)  # for 0.1 noise\n",
    "\n",
    "        # We'll store final action + prob in these arrays\n",
    "        final_actions = chosen_actions.clone()\n",
    "        final_probs = torch.zeros(batch_size, device=device)\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            if done[b]:\n",
    "                # If env is already done, no more transitions\n",
    "                final_actions[b] = -1\n",
    "                final_probs[b] = 1.0\n",
    "                continue\n",
    "\n",
    "            r = states[b, 0].item()\n",
    "            c = states[b, 1].item()\n",
    "            h = states[b, 2].item()\n",
    "            v = states[b, 3].item()\n",
    "            vacts = valid_actions[h][v]  # Python list of valid action indices\n",
    "\n",
    "            # If the policy-chosen action is not valid, forcibly random\n",
    "            if chosen_actions[b].item() not in vacts:\n",
    "                # random among valid\n",
    "                final_actions[b] = vacts[torch.randint(len(vacts), (1,)).item()]\n",
    "                final_probs[b] = 1.0 / len(vacts)\n",
    "            else:\n",
    "                # Epsilon-soft\n",
    "                if rand_vals[b].item() < epsilon:\n",
    "                    # random\n",
    "                    ra = vacts[torch.randint(len(vacts), (1,)).item()]\n",
    "                    final_actions[b] = ra\n",
    "                    final_probs[b] = epsilon / len(vacts)\n",
    "                else:\n",
    "                    # policy\n",
    "                    final_actions[b] = chosen_actions[b]\n",
    "                    final_probs[b] = (1 - epsilon) + (epsilon / len(vacts))\n",
    "\n",
    "            # noise override with prob=0.1\n",
    "            if noise and noise_vals[b].item() < 0.1:\n",
    "                final_actions[b] = 0  # index 0 => (0,0)\n",
    "                final_probs[b] = 0.1\n",
    "\n",
    "        actions_buffer[:, step_i] = final_actions\n",
    "        probs_buffer[:, step_i] = final_probs\n",
    "\n",
    "        # Now compute next states for each environment\n",
    "        # skip those done\n",
    "        dh_dv = torch.tensor(ACTIONS, device=device, dtype=torch.long)[final_actions.clamp(min=0)]\n",
    "        # shape [batch_size, 2], but for \"done\" or -1 actions, we clamp to index 0 => (0,0).\n",
    "\n",
    "        # new velocity\n",
    "        new_hv = states[:, 2:4] + dh_dv\n",
    "        # new row/col\n",
    "        new_row = states[:, 0] + (new_hv[:, 1] - 1)\n",
    "        new_col = states[:, 1] + (new_hv[:, 0] - 1)\n",
    "\n",
    "        # update each environment if not done\n",
    "        for b in range(batch_size):\n",
    "            if done[b]:\n",
    "                continue\n",
    "\n",
    "            # if final_actions[b] == -1 => skip\n",
    "            if final_actions[b] < 0:\n",
    "                # means we didn't actually pick an action because done\n",
    "                continue\n",
    "\n",
    "            nr = new_row[b].item()\n",
    "            nc = new_col[b].item()\n",
    "            nh = new_hv[b, 0].item()\n",
    "            nv = new_hv[b, 1].item()\n",
    "\n",
    "            # check finish\n",
    "            if (nr, nc) in finish_cells:\n",
    "                # done\n",
    "                done[b] = True\n",
    "                lengths[b] = step_i + 1\n",
    "            else:\n",
    "                # check boundary\n",
    "                if (nr < 0 or nr >= ROWS or nc < 0 or nc >= COLS or track[nr, nc].item() == 1):\n",
    "                    # crash => reset\n",
    "                    reset = random_start_states(1)[0]\n",
    "                    states[b] = reset\n",
    "                else:\n",
    "                    # normal\n",
    "                    states[b, 0] = nr\n",
    "                    states[b, 1] = nc\n",
    "                    states[b, 2] = nh\n",
    "                    states[b, 3] = nv\n",
    "\n",
    "        if torch.all(done):\n",
    "            break\n",
    "\n",
    "    # If some environments never finished, set length to max_steps\n",
    "    lengths[~done] = max_steps\n",
    "    return states_buffer, actions_buffer, probs_buffer, lengths\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Off-policy MC update in a batched manner\n",
    "#   We'll do a backward pass for each environment's trajectory.\n",
    "#   The difference: we store transitions in GPU arrays,\n",
    "#   so we can do the backward pass more quickly than a Python list-of-lists.\n",
    "# ------------------------------------------------------\n",
    "def offpolicy_update(states_buffer, actions_buffer, probs_buffer, lengths):\n",
    "    \"\"\"\n",
    "    states_buffer:  (batch_size, max_steps, 4)\n",
    "    actions_buffer: (batch_size, max_steps)\n",
    "    probs_buffer:   (batch_size, max_steps)\n",
    "    lengths:        (batch_size,) - how many steps each env took\n",
    "    \"\"\"\n",
    "    batch_size, max_steps = actions_buffer.shape\n",
    "\n",
    "    # We'll do the standard MC backward pass:\n",
    "    #   G <- 0, W <- 1\n",
    "    #   for t in T-1 down to 0:\n",
    "    #       G = gamma*G + R_step\n",
    "    #       C[state, action] += W\n",
    "    #       Q[state, action] += W*(G - Q[state, action]) / C[state, action]\n",
    "    #       policy[state] = argmax(Q[state, valid_acts])\n",
    "    #       if action != policy[state], break\n",
    "    #       W = W / prob\n",
    "    #\n",
    "    # We do this for each environment in the batch.\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        T = lengths[b].item()  # how many steps\n",
    "        if T == 0:\n",
    "            continue\n",
    "\n",
    "        G = 0.0\n",
    "        W = 1.0\n",
    "        for t in reversed(range(T)):\n",
    "            # read from buffer\n",
    "            r, c, h, v = states_buffer[b, t].tolist()\n",
    "            a = actions_buffer[b, t].item()\n",
    "            prob = probs_buffer[b, t].item()\n",
    "\n",
    "            # If a < 0 => means no action (done), skip\n",
    "            if a < 0:\n",
    "                continue\n",
    "\n",
    "            G = gamma * G + R_step\n",
    "            C[r, c, h, v, a] += W\n",
    "            old_q = Q[r, c, h, v, a]\n",
    "            Q[r, c, h, v, a] = old_q + W * (G - old_q) / C[r, c, h, v, a]\n",
    "\n",
    "            # Update policy => greedy among valid\n",
    "            vacts = valid_actions[h][v]\n",
    "            q_slice = Q[r, c, h, v, vacts]\n",
    "            best_idx = torch.argmax(q_slice).item()\n",
    "            best_action = vacts[best_idx]\n",
    "            policy[r, c, h, v] = best_action\n",
    "\n",
    "            # off-policy break\n",
    "            if a != best_action:\n",
    "                break\n",
    "\n",
    "            # update W\n",
    "            W = W / prob if prob > 0 else 1.0\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Evaluate the current policy by a single (noise-free) episode\n",
    "# ------------------------------------------------------\n",
    "def evaluate_trajectory():\n",
    "    \"\"\"\n",
    "    Run a single (noise-free) episode from a random start,\n",
    "    following the current greedy policy. Return (length, path).\n",
    "    \"\"\"\n",
    "    state = random_start_states(1)[0]  # shape [4]\n",
    "    path = []\n",
    "    step_count = 0\n",
    "    while True:\n",
    "        r, c, h, v = state.tolist()\n",
    "        path.append((r, c, h, v))\n",
    "        if (r, c) in finish_cells:\n",
    "            return step_count, path\n",
    "\n",
    "        # pick the policy action\n",
    "        a = policy[r, c, h, v].item()\n",
    "        dh, dv = ACTIONS[a]\n",
    "        nh = h + dh\n",
    "        nv = v + dv\n",
    "        nr = r + (nv - 1)\n",
    "        nc = c + (nh - 1)\n",
    "\n",
    "        # boundary check\n",
    "        if nr < 0 or nr >= ROWS or nc < 0 or nc >= COLS or track[nr, nc].item() == 1:\n",
    "            # crash => reset\n",
    "            state = random_start_states(1)[0]\n",
    "        else:\n",
    "            state = torch.tensor([nr, nc, nh, nv], device=device)\n",
    "        step_count += 1\n",
    "\n",
    "# ------------------------------------------------------\n",
    "# Main training loop\n",
    "# ------------------------------------------------------\n",
    "def main():\n",
    "    num_iterations = 16384     # number of batch iterations\n",
    "    batch_size = 65536          # increase for more GPU usage\n",
    "    max_steps = 4096          # max steps per parallel rollout\n",
    "\n",
    "    rewards_log = []\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # sample a batch\n",
    "        states_buf, actions_buf, probs_buf, lengths = sample_batch_episodes(\n",
    "            batch_size, epsilon, noise=True, max_steps=max_steps\n",
    "        )\n",
    "\n",
    "        # off-policy update\n",
    "        offpolicy_update(states_buf, actions_buf, probs_buf, lengths)\n",
    "\n",
    "        # every so often, do a noise-free evaluation\n",
    "        if i % 50 == 0:\n",
    "            T_eval, path = evaluate_trajectory()\n",
    "            # return = -T_eval if each step costs -1\n",
    "            rewards_log.append(-T_eval)\n",
    "            print(f\"Iteration {i}: T_eval={T_eval}, Return={-T_eval}\")\n",
    "\n",
    "    # Plot the learning curve\n",
    "    plt.plot(rewards_log)\n",
    "    plt.xlabel(\"Evaluation checkpoint (every 50 iterations)\")\n",
    "    plt.ylabel(\"Return (negative of steps)\")\n",
    "    plt.title(\"Off-Policy MC Control (Batch) on Racetrack (GPU)\")\n",
    "    plt.show()\n",
    "\n",
    "    # Show final example trajectories\n",
    "    for i in range(3):\n",
    "        T_opt, path = evaluate_trajectory()\n",
    "        print(f\"\\nOptimal (noise-free) trajectory #{i+1}, length={T_opt}\")\n",
    "        print(\"Path:\", path)\n",
    "        print(\"Actions are implied by policy, reward =\", -T_opt)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "too long to run the model(Monte Carlo Off-policy control) training-> to be honest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thinking about next step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Episode 50/3000, steps=13082, eps=0.050, ep_reward=-14\n",
      "Episode 100/3000, steps=15188, eps=0.050, ep_reward=-12\n",
      "Episode 150/3000, steps=16368, eps=0.050, ep_reward=-11\n",
      "Episode 200/3000, steps=17503, eps=0.050, ep_reward=-28\n",
      "Episode 250/3000, steps=18709, eps=0.050, ep_reward=-15\n",
      "Episode 300/3000, steps=20077, eps=0.050, ep_reward=-13\n",
      "Episode 350/3000, steps=21129, eps=0.050, ep_reward=-29\n",
      "Episode 400/3000, steps=22296, eps=0.050, ep_reward=-60\n",
      "Episode 450/3000, steps=23125, eps=0.050, ep_reward=-12\n",
      "Episode 500/3000, steps=24111, eps=0.050, ep_reward=-72\n",
      "Episode 550/3000, steps=25007, eps=0.050, ep_reward=-32\n",
      "Episode 600/3000, steps=25860, eps=0.050, ep_reward=-13\n",
      "Episode 650/3000, steps=26870, eps=0.050, ep_reward=-44\n",
      "Episode 700/3000, steps=27767, eps=0.050, ep_reward=-16\n",
      "Episode 750/3000, steps=28718, eps=0.050, ep_reward=-14\n",
      "Episode 800/3000, steps=29590, eps=0.050, ep_reward=-29\n",
      "Episode 850/3000, steps=30469, eps=0.050, ep_reward=-14\n",
      "Episode 900/3000, steps=31398, eps=0.050, ep_reward=-12\n",
      "Episode 950/3000, steps=32265, eps=0.050, ep_reward=-14\n",
      "Episode 1000/3000, steps=33176, eps=0.050, ep_reward=-12\n",
      "Episode 1050/3000, steps=33974, eps=0.050, ep_reward=-22\n",
      "Episode 1100/3000, steps=34922, eps=0.050, ep_reward=-14\n",
      "Episode 1150/3000, steps=35773, eps=0.050, ep_reward=-14\n",
      "Episode 1200/3000, steps=36601, eps=0.050, ep_reward=-14\n",
      "Episode 1250/3000, steps=37431, eps=0.050, ep_reward=-15\n",
      "Episode 1300/3000, steps=38341, eps=0.050, ep_reward=-17\n",
      "Episode 1350/3000, steps=39161, eps=0.050, ep_reward=-15\n",
      "Episode 1400/3000, steps=40053, eps=0.050, ep_reward=-13\n",
      "Episode 1450/3000, steps=40906, eps=0.050, ep_reward=-21\n",
      "Episode 1500/3000, steps=41713, eps=0.050, ep_reward=-16\n",
      "Episode 1550/3000, steps=42550, eps=0.050, ep_reward=-16\n",
      "Episode 1600/3000, steps=43314, eps=0.050, ep_reward=-14\n",
      "Episode 1650/3000, steps=44067, eps=0.050, ep_reward=-26\n",
      "Episode 1700/3000, steps=44880, eps=0.050, ep_reward=-14\n",
      "Episode 1750/3000, steps=45684, eps=0.050, ep_reward=-11\n",
      "Episode 1800/3000, steps=46528, eps=0.050, ep_reward=-11\n",
      "Episode 1850/3000, steps=47274, eps=0.050, ep_reward=-33\n",
      "Episode 1900/3000, steps=48082, eps=0.050, ep_reward=-15\n",
      "Episode 1950/3000, steps=48843, eps=0.050, ep_reward=-14\n",
      "Episode 2000/3000, steps=49712, eps=0.050, ep_reward=-15\n",
      "Episode 2050/3000, steps=50514, eps=0.050, ep_reward=-11\n",
      "Episode 2100/3000, steps=51289, eps=0.050, ep_reward=-14\n",
      "Episode 2150/3000, steps=52080, eps=0.050, ep_reward=-13\n",
      "Episode 2200/3000, steps=52845, eps=0.050, ep_reward=-14\n",
      "Episode 2250/3000, steps=53568, eps=0.050, ep_reward=-15\n",
      "Episode 2300/3000, steps=54284, eps=0.050, ep_reward=-25\n",
      "Episode 2350/3000, steps=55089, eps=0.050, ep_reward=-12\n",
      "Episode 2400/3000, steps=55899, eps=0.050, ep_reward=-12\n",
      "Episode 2450/3000, steps=56626, eps=0.050, ep_reward=-12\n",
      "Episode 2500/3000, steps=57441, eps=0.050, ep_reward=-24\n",
      "Episode 2550/3000, steps=58222, eps=0.050, ep_reward=-11\n",
      "Episode 2600/3000, steps=58903, eps=0.050, ep_reward=-13\n",
      "Episode 2650/3000, steps=59721, eps=0.050, ep_reward=-24\n",
      "Episode 2700/3000, steps=60541, eps=0.050, ep_reward=-30\n",
      "Episode 2750/3000, steps=61205, eps=0.050, ep_reward=-11\n",
      "Episode 2800/3000, steps=61948, eps=0.050, ep_reward=-22\n",
      "Episode 2850/3000, steps=62686, eps=0.050, ep_reward=-16\n",
      "Episode 2900/3000, steps=63482, eps=0.050, ep_reward=-13\n",
      "Episode 2950/3000, steps=64235, eps=0.050, ep_reward=-22\n",
      "Episode 3000/3000, steps=64907, eps=0.050, ep_reward=-12\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating final policy with noise disabled (for 3 episodes):\n",
      "Eval Episode 1: steps=13, return=-13, path_len=13\n",
      "Eval Episode 2: steps=11, return=-11, path_len=11\n",
      "Eval Episode 3: steps=11, return=-11, path_len=11\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==============================================\n",
    "# Environment definition: RacetrackEnv\n",
    "# ==============================================\n",
    "class RacetrackEnv:\n",
    "    \"\"\"\n",
    "    A single-environment version of the Racetrack problem from Sutton & Barto (Exercise 5.12),\n",
    "    but with DQN-friendly step() interface.\n",
    "\n",
    "    Observations:\n",
    "      state = (row, col, v_row, v_col)\n",
    "        - row in [0..31], col in [0..16]\n",
    "        - v_row, v_col in [0..4]  (the \"velocity\" components),\n",
    "          but cannot both be 0 except at the starting line.\n",
    "    \n",
    "    Actions:\n",
    "      9 discrete actions = increments (dh, dv) in { -1, 0, +1 } x { -1, 0, +1 }\n",
    "      But we interpret them as velocity increments.\n",
    "      We also have a 10% chance to override the chosen increments with (0,0) (noise).\n",
    "\n",
    "    Reward:\n",
    "      -1 per step, until crossing finish line => episode done.\n",
    "\n",
    "    If we hit boundary (off-track), we reset to a random start state (the same episode continues).\n",
    "    If we cross the finish line, episode ends.\n",
    "    \"\"\"\n",
    "    ACTIONS = [\n",
    "        (0, 0),   # index 0\n",
    "        (0, 1),   # index 1\n",
    "        (0, -1),  # index 2\n",
    "        (1, 0),   # index 3\n",
    "        (1, 1),   # index 4\n",
    "        (1, -1),  # index 5\n",
    "        (-1, 0),  # index 6\n",
    "        (-1, 1),  # index 7\n",
    "        (-1, -1)  # index 8\n",
    "    ]\n",
    "    NUM_ACTIONS = len(ACTIONS)\n",
    "\n",
    "    def __init__(self, device='cpu', noise_prob=0.1):\n",
    "        self.rows = 32\n",
    "        self.cols = 17\n",
    "        self.vel_max = 4\n",
    "        self.noise_prob = noise_prob\n",
    "        self.device = device\n",
    "\n",
    "        # Track boundary: 1 => boundary/off-track, 0 => on track\n",
    "        self.track = np.zeros((self.rows, self.cols), dtype=int)\n",
    "        # Fill the boundaries (like in the problem statement)\n",
    "        self.track[31, 0:3] = 1\n",
    "        self.track[30, 0:2] = 1\n",
    "        self.track[29, 0:2] = 1\n",
    "        self.track[28, 0]   = 1\n",
    "        self.track[0:18, 0] = 1\n",
    "        self.track[0:10, 1] = 1\n",
    "        self.track[0:3,  2] = 1\n",
    "        self.track[0:26, 9:] = 1\n",
    "        self.track[25, 9]   = 0\n",
    "\n",
    "        # Starting columns, finish line\n",
    "        self.start_cols = list(range(3, 9))  # [3..8]\n",
    "        self.finish_cells = {\n",
    "            (26, self.cols-1), (27, self.cols-1), (28, self.cols-1),\n",
    "            (29, self.cols-1), (30, self.cols-1), (31, self.cols-1)\n",
    "        }\n",
    "\n",
    "        # We will keep current state\n",
    "        self.state = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset to a random start state on row=0, col in [3..8], velocity=(0,0).\n",
    "        Return the initial state as a PyTorch tensor (optionally).\n",
    "        \"\"\"\n",
    "        start_col = random.choice(self.start_cols)\n",
    "        self.state = (0, start_col, 0, 0)\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action_idx):\n",
    "        \"\"\"\n",
    "        Take one step in the environment using action index in [0..8].\n",
    "        Returns: (next_state, reward, done, info)\n",
    "        - If off-track, we reset to start line, same episode continues, reward is still -1, done=False\n",
    "        - If cross finish, done=True\n",
    "        \"\"\"\n",
    "        r, c, vr, vc = self.state\n",
    "        # Chosen increments\n",
    "        dh, dv = self.ACTIONS[action_idx]\n",
    "\n",
    "        # With probability noise_prob, override increments to (0,0)\n",
    "        if random.random() < self.noise_prob:\n",
    "            dh, dv = 0, 0\n",
    "\n",
    "        # Next velocity\n",
    "        new_vr = vr + dh\n",
    "        new_vc = vc + dv\n",
    "        # Clip velocities to [0..4]\n",
    "        new_vr = max(0, min(self.vel_max, new_vr))\n",
    "        new_vc = max(0, min(self.vel_max, new_vc))\n",
    "\n",
    "        # Next position\n",
    "        nr = r + new_vr\n",
    "        nc = c + new_vc\n",
    "\n",
    "        reward = -1\n",
    "        done = False\n",
    "\n",
    "        # Check finish line\n",
    "        if (nr, nc) in self.finish_cells:\n",
    "            # Episode ends\n",
    "            done = True\n",
    "            next_state = (nr, nc, new_vr, new_vc)\n",
    "            self.state = next_state\n",
    "            return self._get_obs(), reward, done, {}\n",
    "\n",
    "        # Check off-track\n",
    "        if nr < 0 or nr >= self.rows or nc < 0 or nc >= self.cols or self.track[nr, nc] == 1:\n",
    "            # Crash => reset to start line (0, random_col, 0,0), but done=False\n",
    "            start_col = random.choice(self.start_cols)\n",
    "            next_state = (0, start_col, 0, 0)\n",
    "            self.state = next_state\n",
    "            return self._get_obs(), reward, done, {}\n",
    "\n",
    "        # Otherwise normal\n",
    "        next_state = (nr, nc, new_vr, new_vc)\n",
    "        self.state = next_state\n",
    "        return self._get_obs(), reward, done, {}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        \"\"\"\n",
    "        Return the current state as a tensor on self.device: shape (4,)\n",
    "        [row, col, v_row, v_col].\n",
    "        \"\"\"\n",
    "        r, c, vr, vc = self.state\n",
    "        return torch.tensor([r, c, vr, vc], dtype=torch.float, device=self.device)\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# DQN Neural Network\n",
    "# ==============================================\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple feed-forward network mapping (row, col, vr, vc) -> Q-values for 9 actions.\n",
    "    You can expand this if you like (more layers, bigger layers).\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size=128):\n",
    "        super().__init__()\n",
    "        # Input dimension = 4, output dimension = 9\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(4, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 9)  # 9 actions\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, 4)\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# Replay Buffer\n",
    "# ==============================================\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device='cpu'):\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "        self.buffer = collections.deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        transitions = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*transitions)\n",
    "\n",
    "        states = torch.stack(states)  # each is shape (4,)\n",
    "        actions = torch.tensor(actions, dtype=torch.long, device=self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float, device=self.device)\n",
    "        next_states = torch.stack(next_states)\n",
    "        dones = torch.tensor(dones, dtype=torch.float, device=self.device)\n",
    "\n",
    "        return states, actions, rewards, next_states, dones\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# DQN Training Loop\n",
    "# ==============================================\n",
    "def train_dqn(\n",
    "    env,\n",
    "    qnet,\n",
    "    target_qnet,\n",
    "    buffer,\n",
    "    num_episodes=2000,\n",
    "    batch_size=128,\n",
    "    gamma=1.0,\n",
    "    lr=1e-3,\n",
    "    epsilon_start=1.0,\n",
    "    epsilon_end=0.05,\n",
    "    epsilon_decay=1000,\n",
    "    sync_target_steps=1000,\n",
    "    max_steps_per_episode=500,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the QNetwork with standard DQN + target network + replay buffer.\n",
    "\n",
    "    Returns: a list of episode rewards for plotting\n",
    "    \"\"\"\n",
    "    optimizer = optim.Adam(qnet.parameters(), lr=lr)\n",
    "    steps_done = 0\n",
    "    episode_rewards = []\n",
    "\n",
    "    def epsilon_by_step(step):\n",
    "        # simple linear schedule from epsilon_start down to epsilon_end\n",
    "        return max(epsilon_end, epsilon_start - (step / epsilon_decay)*(epsilon_start - epsilon_end))\n",
    "\n",
    "    for ep in range(num_episodes):\n",
    "        state = env.reset()  # shape (4,)\n",
    "        ep_reward = 0\n",
    "        for t in range(max_steps_per_episode):\n",
    "            steps_done += 1\n",
    "            # epsilon-greedy\n",
    "            eps = epsilon_by_step(steps_done)\n",
    "            if random.random() < eps:\n",
    "                action = random.randint(0, env.NUM_ACTIONS-1)\n",
    "            else:\n",
    "                with torch.no_grad():\n",
    "                    qvals = qnet(state.unsqueeze(0))  # shape (1,9)\n",
    "                    action = qvals.argmax(dim=1).item()\n",
    "\n",
    "            next_state, reward, done, _info = env.step(action)\n",
    "            ep_reward += reward\n",
    "\n",
    "            # store transition\n",
    "            buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            # DQN update\n",
    "            if len(buffer) >= batch_size:\n",
    "                states_b, actions_b, rewards_b, next_states_b, dones_b = buffer.sample(batch_size)\n",
    "\n",
    "                # Q(s,a)\n",
    "                qvals_b = qnet(states_b)  # shape (batch_size, 9)\n",
    "                chosen_qvals = qvals_b.gather(1, actions_b.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "                # Target\n",
    "                with torch.no_grad():\n",
    "                    # Double DQN style (optional):\n",
    "                    # a' = argmax(qnet(next_states_b))\n",
    "                    # target = rewards_b + gamma*(target_qnet(next_states_b).gather(1, a').squeeze(1))*(1-dones_b)\n",
    "                    # If you want simpler DQN:\n",
    "                    next_qvals = target_qnet(next_states_b).max(dim=1)[0]\n",
    "                    target = rewards_b + gamma * next_qvals * (1 - dones_b)\n",
    "\n",
    "                loss = nn.MSELoss()(chosen_qvals, target)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # sync target\n",
    "            if steps_done % sync_target_steps == 0:\n",
    "                target_qnet.load_state_dict(qnet.state_dict())\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        episode_rewards.append(ep_reward)\n",
    "        if (ep+1) % 50 == 0:\n",
    "            print(f\"Episode {ep+1}/{num_episodes}, steps={steps_done}, eps={eps:.3f}, ep_reward={ep_reward}\")\n",
    "\n",
    "    return episode_rewards\n",
    "\n",
    "\n",
    "# ==============================================\n",
    "# Putting it all together\n",
    "# ==============================================\n",
    "def main():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # 1) Create environment\n",
    "    env = RacetrackEnv(device=device, noise_prob=0.1)\n",
    "\n",
    "    # 2) Create networks\n",
    "    qnet = QNetwork(hidden_size=256).to(device)\n",
    "    target_qnet = QNetwork(hidden_size=256).to(device)\n",
    "    target_qnet.load_state_dict(qnet.state_dict())  # sync initially\n",
    "\n",
    "    # 3) Create replay buffer\n",
    "    replay_capacity = 100_000\n",
    "    buffer = ReplayBuffer(replay_capacity, device=device)\n",
    "\n",
    "    # 4) Train\n",
    "    num_episodes = 3000\n",
    "    batch_size = 256\n",
    "    sync_target_steps = 1000\n",
    "\n",
    "    rewards = train_dqn(\n",
    "        env, qnet, target_qnet, buffer,\n",
    "        num_episodes=num_episodes,\n",
    "        batch_size=batch_size,\n",
    "        gamma=1.0,\n",
    "        lr=1e-3,\n",
    "        epsilon_start=1.0,\n",
    "        epsilon_end=0.05,\n",
    "        epsilon_decay=10_000,  # how fast epsilon decays\n",
    "        sync_target_steps=sync_target_steps,\n",
    "        max_steps_per_episode=2000,\n",
    "    )\n",
    "\n",
    "    # 5) Plot training rewards\n",
    "    plt.plot(rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Return (sum of rewards)\")\n",
    "    plt.title(\"DQN on Racetrack (Exercise 5.12)\")\n",
    "    plt.show()\n",
    "\n",
    "    # 6) Evaluate final policy with noise-free episodes\n",
    "    print(\"\\nEvaluating final policy with noise disabled (for 3 episodes):\")\n",
    "    old_noise_prob = env.noise_prob\n",
    "    env.noise_prob = 0.0\n",
    "    for i in range(3):\n",
    "        state = env.reset()\n",
    "        ep_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        path = []\n",
    "        while not done and steps < 2000:\n",
    "            with torch.no_grad():\n",
    "                qvals = qnet(state.unsqueeze(0))  # shape (1,9)\n",
    "                action = qvals.argmax(dim=1).item()\n",
    "            next_state, reward, done, _info = env.step(action)\n",
    "            path.append((state[0].item(), state[1].item(), state[2].item(), state[3].item()))\n",
    "            ep_reward += reward\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "        print(f\"Eval Episode {i+1}: steps={steps}, return={ep_reward}, path_len={len(path)}\")\n",
    "    env.noise_prob = old_noise_prob\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we still see some episodes with 6060 or 7272 reward?\n",
    "\n",
    "- Occasionally the agent crashes repeatedly and resets, or random noise sends it off-track. This can produce longer episodes with bigger negative returns.\n",
    "- Even an optimal policy can have unlucky noise or resets.\n",
    "Overall: The results look pretty good. The agent has a stable policy that reliably finishes in about 1113 steps (noise-free). Thats likely near-optimal for this environment."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
