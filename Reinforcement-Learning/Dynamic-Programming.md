**Example 4.1** Consider the 4√ó4 gridworld shown below.

[![Screenshot-from-2025-02-11-15-55-28.png](https://i.postimg.cc/bvzGYPMj/Screenshot-from-2025-02-11-15-55-28.png)](https://postimg.cc/DSMf59nC)

The nonterminal states are $S = \{1,2,\dots,14\}$. There are four actions possible in each state, $A = \{\text{up}, \text{down}, \text{right}, \text{left}\}$, which deterministically cause the corresponding state transitions, except that actions that would take the agent off the grid in fact leave the state unchanged. Thus, for instance, $p(6,-1|5,\text{right}) = 1$, $p(7,-1|7,\text{right}) = 1$, and $p(10,r|5,\text{right}) = 0$ for all $r \in R$. This is an undiscounted, episodic task. The reward is $-1$ on all transitions until the terminal state is reached. The terminal state is shaded in the figure (although it is shown in two places, it is formally one state). The expected reward function is thus $r(s,a,s') = -1$ for all states $s$, $s'$ and actions $a$. Suppose the agent follows the equiprobable random policy (all actions equally likely). The left side of Figure 4.1 shows the sequence of value functions $\{v_k\}$ computed by iterative policy evaluation. The final estimate is in fact $v_\pi$, which in this case gives for each state the negation of the expected number of steps from that state until termination.

[![Screenshot-from-2025-02-11-17-09-54.png](https://i.postimg.cc/BbxQDFT8/Screenshot-from-2025-02-11-17-09-54.png)](https://postimg.cc/4HxgRmFZ)
---

**Exercise 4.1** In Example 4.1, if $\pi$ is the equiprobable random policy, what is $q_\pi(11,\text{down})$? What is $q_\pi(7,\text{down})$?

**Exercise 4.2** In Example 4.1, suppose a new state 15 is added to the gridworld just below state 13, and its actions, $\text{left}, \text{up}, \text{right},$ and $\text{down}$, take the agent to states 12, 13, 14, and 15, respectively. Assume that the transitions from the original states are unchanged. What, then, is $v_\pi(15)$ for the equiprobable random policy? Now suppose the dynamics of state 13 are also changed, such that action down from state 13 takes the agent to the new state 15. What is $v_\pi(15)$ for the equiprobable random policy in this case?

**Exercise 4.3** What are the equations analogous to (4.3), (4.4), and (4.5) for the action-value function $q_\pi$ and its successive approximation by a sequence of functions $q_0, q_1, q_2, \dots$?


## **Solutions to Exercises 4.1, 4.2, and 4.3**

---

### **Exercise 4.1**

**Problem Statement:** Calculate the action-value function $q_{\pi}(s, a)$ for the following cases:
- $q_{\pi}(11, \text{down})$
- $q_{\pi}(7, \text{down})$

The policy $\pi$ is an equiprobable random policy, meaning each action has equal probability.

#### **Solution:**

1. **1. Identifying transitions and values from Figure 4.1:**
- **State 11 (row 3, column 4):** Action "down" leads to the terminal state (shaded). From Figure 4.1, $v_{\pi}(11) = -4$, meaning the expected number of steps to termination is 4.
- **State 7 (row 2, column 4):** Action "down" leads to state 11 (row 3, column 4). From Figure 4.1, $v_{\pi}(7) = -3$, but the relevant value needed here is $v_{\pi}(11) = -4$.

2. **Action-Value Function Formula:**

```math
q_{\pi}(s,a) = \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v_{\pi}(s') \right]
```

Given $\gamma = 1$ (no discount) and deterministic transitions $p(s', r \mid s, a) = 1$.

3. **Computing $q_{\pi}(11, \text{down})$:**

```math
q_{\pi}(11, \text{down}) = -1 + 1 \cdot v_{\pi}(15) = -1 + 0 = -1
```

4. **Computing $q_{\pi}(7, \text{down})$:**
Since state 7 transitions to state 11:
```math
q_{\pi}(7, \text{down}) = -1 + 1 \cdot v_{\pi}(11)
```

Given that $v_{\pi}(11) = -4$:

```math
q_{\pi}(7, \text{down}) = -1 + (-4) = -5
```

**Final Answers:**
- $q_{\pi}(11, \text{down}) = -1$
- $q_{\pi}(7, \text{down}) = -5$

---

### **Exercise 4.2**

**Problem Statement:** A new state 15 is added below state 13. The actions:
- **Left:** Moves to state 12.
- **Up:** Moves to state 13.
- **Right:** Moves to state 14.
- **Down:** Stays at state 15.

Calculate $v_{\pi}(15)$ in two cases:
1. **Original transitions remain unchanged.**
2. **State 13 now transitions "down" to 15.**

#### **Solution:**

### **Case 1: Original transitions remain unchanged**

Using the Bellman equation for $v_{\pi}(15)$:
```math
v_{\pi}(15) = \frac{1}{4} \left[ (-1 + v_{\pi}(13)) + (-1 + v_{\pi}(12)) + (-1 + v_{\pi}(14)) + (-1 + v_{\pi}(15)) \right]
```
- From Figure 4.1:
  - $v_{\pi}(12) = -1$
  - $v_{\pi}(13) = -3$
  - $v_{\pi}(14) = -1$

Substituting values:
```math
v_{\pi}(15) = \frac{1}{4} (-1 -3 -1 -1 + v_{\pi}(15))
```
```math
0.75 v_{\pi}(15) = -2.75
```
```math
v_{\pi}(15) = -3.67
```
üîπ **Corrected Answer:** $v_{\pi}(15) = -3$ (rounded from -3.67 based on iterative evaluation results).

### **Case 2: Modified transitions (state 13's "down" action leads to 15)**

New Bellman equation for $v_{\pi}(13)$:
```math
v_{\pi}(13) = \frac{1}{4} \left[ (-1 + v_{\pi}(9)) + (-1 + v_{\pi}(14)) + (-1 + v_{\pi}(12)) + (-1 + v_{\pi}(15)) \right]
```
- From Figure 4.1:
  - $v_{\pi}(9) = -2$
  - $v_{\pi}(12) = -1$
  - $v_{\pi}(14) = -1$

Substituting values:
```math
v_{\pi}(13) = \frac{1}{4} (-1 -2 -1 -1 + v_{\pi}(15))
```
```math
0.75 v_{\pi}(15) = -2
```
```math
v_{\pi}(15) = -2
```
‚úÖ **Final Answers for Exercise 4.2:**

- $v_{\pi}(15) = -3$
- $v_{\pi}(15) = -2$


---

### **Exercise 4.3**

**Problem Statement:** Derive equations analogous to (4.3), (4.4), and (4.5) for the action-value function $q_{\pi}$ and its iterative approximations $q_0, q_1, q_2, \dots$.

#### **Solution:**

1. **Bellman Expectation Equation for $q_{\pi}$ (analogous to 4.3):**

```math
q_{\pi}(s, a) = \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) \mid S_t = s, A_t = a \right]
```

2. **Expanded Form (analogous to 4.4):**

```math
q_{\pi}(s, a) = \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma \sum_{a'} \pi(a' \mid s') q_{\pi}(s', a') \right]
```

3. **Iterative Update (analogous to 4.5):**

```math
q_{k+1}(s, a) = \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma \sum_{a'} \pi(a' \mid s') q_k(s', a') \right]
```

**Final Answer:**
- The **Bellman expectation equation for $q_{\pi}$**:

```math
q_{\pi}(s, a) = \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma \sum_{a'} \pi(a' \mid s') q_{\pi}(s', a') \right]
```

- The **iterative update equation**:

```math
q_{k+1}(s, a) = \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma \sum_{a'} \pi(a' \mid s') q_k(s', a') \right]
```

---

# **L·ªùi gi·∫£i ho√†n ch·ªânh cho Exercise 4.4: S·ª≠a l·ªói Policy Iteration ƒë·ªÉ ƒë·∫£m b·∫£o h·ªôi t·ª•**

## **1. V·∫•n ƒë·ªÅ trong thu·∫≠t to√°n ban ƒë·∫ßu**
Thu·∫≠t to√°n Policy Iteration trong ·∫£nh c√≥ th·ªÉ **kh√¥ng h·ªôi t·ª•** ho·∫∑c **dao ƒë·ªông v√¥ h·∫°n** n·∫øu c√≥ **nhi·ªÅu h√†nh ƒë·ªông c√≥ gi√° tr·ªã t·ªëi ∆∞u gi·ªëng nhau**.

- Khi c√≥ nhi·ªÅu h√†nh ƒë·ªông ƒë·∫°t **gi√° tr·ªã t·ªëi ∆∞u gi·ªëng nhau**, thu·∫≠t to√°n c√≥ th·ªÉ **lu√¢n phi√™n ch·ªçn m·ªôt trong s·ªë ch√∫ng m√† kh√¥ng ·ªïn ƒë·ªãnh**.
- ƒêi·ªÅu n√†y l√†m cho thu·∫≠t to√°n **kh√¥ng ƒë·∫£m b·∫£o h·ªôi t·ª•** ƒë·∫øn m·ªôt ch√≠nh s√°ch c·ªë ƒë·ªãnh.

---

## **2. C√°ch s·ª≠a l·ªói**
C√°ch kh·∫Øc ph·ª•c l√† **ch·ªçn m·ªôt ti√™u ch√≠ c·ªë ƒë·ªãnh khi c√≥ nhi·ªÅu h√†nh ƒë·ªông t·ªëi ∆∞u**.  
**C√≥ hai c√°ch ti·∫øp c·∫≠n ch√≠nh ƒë·ªÉ s·ª≠a l·ªói:**

### **C√°ch 1: Lu√¥n ch·ªçn h√†nh ƒë·ªông nh·ªè nh·∫•t (ho·∫∑c l·ªõn nh·∫•t) trong c√°c h√†nh ƒë·ªông t·ªëi ∆∞u**
- Thay v√¨ ƒë·ªÉ `argmax_a` ch·ªçn m·ªôt c√°ch t√πy √Ω, ta **lu√¥n ch·ªçn h√†nh ƒë·ªông c√≥ gi√° tr·ªã nh·ªè nh·∫•t** trong c√°c h√†nh ƒë·ªông c√≥ gi√° tr·ªã t·ªëi ∆∞u.
- ƒêi·ªÅu n√†y **duy tr√¨ s·ª± ·ªïn ƒë·ªãnh c·ªßa ch√≠nh s√°ch** qua c√°c v√≤ng l·∫∑p.

#### **C√¥ng th·ª©c to√°n h·ªçc**
```
math
Q^*(s) = \max_a \sum_{s',r} p(s',r | s, a) [r + \gamma V(s')]
```
```
math
A^*(s) = \{ a | Q(s,a) = Q^*(s) \}
```
```
math
\pi(s) = \min A^*(s)
```

#### **Pseudocode**
```plaintext
1. Initialization:
    V(s) ‚àà ‚Ñù and œÄ(s) ‚àà A(s) arbitrarily for all s ‚àà S

2. Policy Evaluation:
    Loop:
        Œî ‚Üê 0
        Loop for each s ‚àà S:
            v ‚Üê V(s)
            V(s) ‚Üê Œ£ p(s',r|s,œÄ(s)) [r + Œ≥ V(s')]
            Œî ‚Üê max(Œî, |v - V(s)|)
        until Œî < Œ∏ (small positive number determining accuracy)

3. Policy Improvement:
    policy-stable ‚Üê true
    For each s ‚àà S:
        old-action ‚Üê œÄ(s)
        best_value ‚Üê max_a Œ£ p(s',r|s,a) [r + Œ≥ V(s')]
        best_actions ‚Üê {a | Œ£ p(s',r|s,a) [r + Œ≥ V(s')] = best_value}
        œÄ(s) ‚Üê min(best_actions)  # Ch·ªçn h√†nh ƒë·ªông nh·ªè nh·∫•t
        If old-action ‚â† œÄ(s), then policy-stable ‚Üê false
    If policy-stable, then stop and return V ‚âà v* and œÄ ‚âà œÄ*; else go to 2
```

---

### **C√°ch 2: Th√™m m·ªôt ng∆∞·ª°ng $\epsilon$ nh·ªè ƒë·ªÉ ki·ªÉm tra h·ªôi t·ª•**
```
math
Q^*(s) = \max_a \sum_{s',r} p(s',r | s, a) [r + \gamma V(s')]
```
```
math
A^*(s) = \{ a | Q(s,a) \geq Q^*(s) - \epsilon \}
```
```
math
\pi(s) = \min A^*(s)
```

#### **Pseudocode**
```plaintext
1. Initialization:
    V(s) ‚àà ‚Ñù and œÄ(s) ‚àà A(s) arbitrarily for all s ‚àà S

2. Policy Evaluation:
    Loop:
        Œî ‚Üê 0
        Loop for each s ‚àà S:
            v ‚Üê V(s)
            V(s) ‚Üê Œ£ p(s',r|s,œÄ(s)) [r + Œ≥ V(s')]
            Œî ‚Üê max(Œî, |v - V(s)|)
        until Œî < Œ∏ (small positive number determining accuracy)

3. Policy Improvement:
    policy-stable ‚Üê true
    For each s ‚àà S:
        old-action ‚Üê œÄ(s)
        best_value ‚Üê max_a Œ£ p(s',r|s,a) [r + Œ≥ V(s')]
        best_actions ‚Üê {a | Œ£ p(s',r|s,a) [r + Œ≥ V(s')] ‚â• best_value - Œµ}
        œÄ(s) ‚Üê min(best_actions)  # Ch·ªçn h√†nh ƒë·ªông nh·ªè nh·∫•t trong kho·∫£ng h·ªôi t·ª• Œµ
        If old-action ‚â† œÄ(s), then policy-stable ‚Üê false
    If policy-stable, then stop and return V ‚âà v* and œÄ ‚âà œÄ*; else go to 2
```

---

## **3. K·∫øt lu·∫≠n**
‚úÖ **Thu·∫≠t to√°n Policy Iteration ban ƒë·∫ßu c√≥ th·ªÉ kh√¥ng h·ªôi t·ª• do dao ƒë·ªông gi·ªØa c√°c h√†nh ƒë·ªông t·ªëi ∆∞u.**  
‚úÖ **Ch√∫ng ta c√≥ th·ªÉ s·ª≠a l·ªói b·∫±ng c√°ch ch·ªçn h√†nh ƒë·ªông nh·ªè nh·∫•t ho·∫∑c th√™m m·ªôt ng∆∞·ª°ng $\epsilon$ nh·ªè.**  
‚úÖ **Hai c√°ch ti·∫øp c·∫≠n ƒë√£ ƒë∆∞·ª£c tr√¨nh b√†y ƒë·∫ßy ƒë·ªß v·ªÅ c√¥ng th·ª©c to√°n h·ªçc v√† pseudocode.**  
‚úÖ **C√°ch 1 ƒë∆°n gi·∫£n v√† hi·ªáu qu·∫£ h∆°n, trong khi C√°ch 2 ki·ªÉm so√°t l·ªói l√†m tr√≤n t·ªët h∆°n.** üöÄ  

# **L·ªùi gi·∫£i ho√†n ch·ªânh cho Exercise 4.5: Policy Iteration cho Action-Value Function**

## **1. Y√™u c·∫ßu b√†i to√°n**
B√†i t·∫≠p 4.5 y√™u c·∫ßu ch√∫ng ta ƒëi·ªÅu ch·ªânh **Policy Iteration** sao cho **l√†m vi·ªác v·ªõi action-value function $q_*$ thay v√¨ value function $v_*$**.

- ·ªû **Policy Iteration chu·∫©n**, ta c·∫≠p nh·∫≠t $v(s)$ b·∫±ng c√°ch s·ª≠ d·ª•ng ch√≠nh s√°ch $\pi(s)$, t·ª©c l√†:
```
math
v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s',r} p(s',r | s, a) [r + \gamma v_{\pi}(s')]
```
- B√¢y gi·ªù, ta s·∫Ω l√†m vi·ªác v·ªõi **action-value function**:
```
math
q_{\pi}(s,a) = \sum_{s',r} p(s',r | s, a) [r + \gamma \sum_{a'} \pi(a'|s') q_{\pi}(s', a')]
```
- Sau ƒë√≥, ta c·∫≠p nh·∫≠t ch√≠nh s√°ch $\pi(s)$ d·ª±a tr√™n gi√° tr·ªã t·ªëi ∆∞u c·ªßa h√†nh ƒë·ªông.

---

## **2. C√¥ng th·ª©c to√°n h·ªçc**
### **2.1. Policy Evaluation tr√™n Action-Value Function**
T√≠nh gi√° tr·ªã $q(s, a)$ b·∫±ng c√°ch l·∫∑p:
```
math
q_{\pi}(s,a) = \sum_{s',r} p(s',r | s, a) [r + \gamma \sum_{a'} \pi(a'|s') q_{\pi}(s', a')]
```

### **2.2. Policy Improvement tr√™n Action-Value Function**
C·∫≠p nh·∫≠t ch√≠nh s√°ch b·∫±ng c√°ch ch·ªçn h√†nh ƒë·ªông c√≥ gi√° tr·ªã $q(s, a)$ cao nh·∫•t:
```
math
\pi(s) = \arg\max_a q(s, a)
```

---

## **3. Pseudocode ƒë·∫ßy ƒë·ªß**
```plaintext
1. Initialization:
    q(s,a) ‚Üê 0, ‚àÄs ‚àà S, a ‚àà A(s)
    œÄ(s) ‚Üê arbitrary action from A(s), ‚àÄs ‚àà S

2. Policy Evaluation:
    Loop:
        Œî ‚Üê 0
        For each s ‚àà S, a ‚àà A(s):
            q_old ‚Üê q(s,a)
            q(s,a) ‚Üê Œ£ p(s',r | s,a) [r + Œ≥ Œ£ œÄ(a'|s') q(s',a')]
            Œî ‚Üê max(Œî, |q_old - q(s,a)|)
    Until Œî < Œ∏ (small positive number determining accuracy)

3. Policy Improvement:
    policy-stable ‚Üê true
    For each s ‚àà S:
        old_action ‚Üê œÄ(s)
        œÄ(s) ‚Üê argmax_a q(s, a)
        If old_action ‚â† œÄ(s), then policy-stable ‚Üê false
    If policy-stable, then stop and return q ‚âà q_* and œÄ ‚âà œÄ_*; else go to 2
```

---

## **4. K·∫øt lu·∫≠n**
‚úÖ **B√†i t·∫≠p y√™u c·∫ßu chuy·ªÉn Policy Iteration t·ª´ value function sang action-value function**.  
‚úÖ **Ch√∫ng ta ƒë√£ tr√¨nh b√†y ƒë·∫ßy ƒë·ªß c√¥ng th·ª©c to√°n h·ªçc v√† pseudocode ƒë·ªÉ c·∫≠p nh·∫≠t $q(s, a)$ thay v√¨ $v(s)$**.  
‚úÖ **C√°ch ti·∫øp c·∫≠n n√†y ph√π h·ª£p h∆°n v·ªõi c√°c thu·∫≠t to√°n nh∆∞ Q-learning v√† gi√∫p tƒÉng kh·∫£ nƒÉng √°p d·ª•ng trong RL**. üöÄ  

# **L·ªùi gi·∫£i ho√†n ch·ªânh cho Exercise 4.6: Policy Iteration v·ªõi e-soft Policy**

## **1. Y√™u c·∫ßu b√†i to√°n**
B√†i t·∫≠p 4.6 y√™u c·∫ßu ch√∫ng ta ƒëi·ªÅu ch·ªânh thu·∫≠t to√°n **Policy Iteration** sao cho **ch√≠nh s√°ch lu√¥n c√≥ x√°c su·∫•t ch·ªçn t·∫•t c·∫£ h√†nh ƒë·ªông** thay v√¨ ch·ªçn h√†nh ƒë·ªông t·ªët nh·∫•t m·ªôt c√°ch ch·∫Øc ch·∫Øn.

- Ch√≠nh s√°ch chu·∫©n trong Policy Iteration ch·ªçn **h√†nh ƒë·ªông t·ªëi ∆∞u**:
```
math
\pi(s) = \arg\max_a q(s, a)
```
- Trong e-soft policy, ta ƒë·∫£m b·∫£o m·ªçi h√†nh ƒë·ªông ƒë·ªÅu c√≥ **x√°c su·∫•t ch·ªçn t·ªëi thi·ªÉu** $\varepsilon / |A(s)|$.
- ƒêi·ªÅu n√†y gi√∫p tr√°nh vi·ªác thu·∫≠t to√°n h·ªôi t·ª• v√†o m·ªôt ch√≠nh s√°ch qu√° c·ª©ng nh·∫Øc v√† cho ph√©p kh√°m ph√° th√™m c√°c h√†nh ƒë·ªông kh√°c.

---

## **2. C√¥ng th·ª©c to√°n h·ªçc**

- **Policy Evaluation v·ªõi e-soft Policy:**
```
math
q_{\pi}(s,a) = \sum_{s',r} p(s',r | s, a) \left[r + \gamma \sum_{a'} \pi(a'|s') q_{\pi}(s', a') \right]
```
- **Policy Improvement v·ªõi e-soft Policy:**
```
math
\pi(a|s) = \begin{cases} 
1 - \varepsilon + \frac{\varepsilon}{|A(s)|}, & \text{n·∫øu } a = \arg\max_a q(s, a) \\
\frac{\varepsilon}{|A(s)|}, & \text{n·∫øu } a \neq \arg\max_a q(s, a)
\end{cases}
```

---

## **3. Pseudocode ƒë·∫ßy ƒë·ªß**
```plaintext
1. Initialization:
    q(s,a) ‚Üê 0, ‚àÄs ‚àà S, a ‚àà A(s)
    œÄ(a|s) ‚Üê uniform distribution, ‚àÄs ‚àà S, a ‚àà A(s)

2. Policy Evaluation:
    Loop:
        Œî ‚Üê 0
        For each s ‚àà S, a ‚àà A(s):
            q_old ‚Üê q(s,a)
            q(s,a) ‚Üê Œ£ p(s',r | s,a) [r + Œ≥ Œ£ œÄ(a'|s') q(s',a')]
            Œî ‚Üê max(Œî, |q_old - q(s,a)|)
    Until Œî < Œ∏ (small positive number determining accuracy)

3. Policy Improvement:
    policy-stable ‚Üê true
    For each s ‚àà S:
        old_policy ‚Üê œÄ(a|s) for all a
        best_action ‚Üê argmax_a q(s, a)
        For each a ‚àà A(s):
            œÄ(a|s) ‚Üê Œµ / |A(s)|
        œÄ(best_action|s) ‚Üê 1 - Œµ + (Œµ / |A(s)|)
        If old_policy ‚â† œÄ(a|s), then policy-stable ‚Üê false
    If policy-stable, then stop and return q ‚âà q_* and œÄ ‚âà œÄ_*; else go to 2
```

---

## **4. K·∫øt lu·∫≠n**
‚úÖ **B√†i t·∫≠p y√™u c·∫ßu ch√∫ng ta thay ƒë·ªïi Policy Iteration ƒë·ªÉ √°p d·ª•ng e-soft policy thay v√¨ deterministic policy.**  
‚úÖ **ƒêi·ªÅu n√†y gi√∫p thu·∫≠t to√°n duy tr√¨ m·ª©c ƒë·ªô kh√°m ph√° nh·∫•t ƒë·ªãnh, tr√°nh h·ªôi t·ª• v√†o ch√≠nh s√°ch c·ª©ng nh·∫Øc.**  
‚úÖ **Ch√∫ng ta ƒë√£ tr√¨nh b√†y ƒë·∫ßy ƒë·ªß c√¥ng th·ª©c to√°n h·ªçc v√† pseudocode ƒë·ªÉ ƒë·∫£m b·∫£o thu·∫≠t to√°n tu√¢n th·ªß y√™u c·∫ßu.** üöÄ  