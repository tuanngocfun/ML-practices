**Example 4.1** Consider the 4√ó4 gridworld shown below.

[![Screenshot-from-2025-02-11-15-55-28.png](https://i.postimg.cc/bvzGYPMj/Screenshot-from-2025-02-11-15-55-28.png)](https://postimg.cc/DSMf59nC)

The nonterminal states are $S = \{1,2,\dots,14\}$. There are four actions possible in each state, $A = \{\text{up}, \text{down}, \text{right}, \text{left}\}$, which deterministically cause the corresponding state transitions, except that actions that would take the agent off the grid in fact leave the state unchanged. Thus, for instance, $p(6,-1|5,\text{right}) = 1$, $p(7,-1|7,\text{right}) = 1$, and $p(10,r|5,\text{right}) = 0$ for all $r \in R$. This is an undiscounted, episodic task. The reward is $-1$ on all transitions until the terminal state is reached. The terminal state is shaded in the figure (although it is shown in two places, it is formally one state). The expected reward function is thus $r(s,a,s') = -1$ for all states $s$, $s'$ and actions $a$. Suppose the agent follows the equiprobable random policy (all actions equally likely). The left side of Figure 4.1 shows the sequence of value functions $\{v_k\}$ computed by iterative policy evaluation. The final estimate is in fact $v_\pi$, which in this case gives for each state the negation of the expected number of steps from that state until termination.

[![Screenshot-from-2025-02-11-17-09-54.png](https://i.postimg.cc/BbxQDFT8/Screenshot-from-2025-02-11-17-09-54.png)](https://postimg.cc/4HxgRmFZ)
---

**Exercise 4.1** In Example 4.1, if $\pi$ is the equiprobable random policy, what is $q_\pi(11,\text{down})$? What is $q_\pi(7,\text{down})$?

**Exercise 4.2** In Example 4.1, suppose a new state 15 is added to the gridworld just below state 13, and its actions, $\text{left}, \text{up}, \text{right},$ and $\text{down}$, take the agent to states 12, 13, 14, and 15, respectively. Assume that the transitions from the original states are unchanged. What, then, is $v_\pi(15)$ for the equiprobable random policy? Now suppose the dynamics of state 13 are also changed, such that action down from state 13 takes the agent to the new state 15. What is $v_\pi(15)$ for the equiprobable random policy in this case?

**Exercise 4.3** What are the equations analogous to (4.3), (4.4), and (4.5) for the action-value function $q_\pi$ and its successive approximation by a sequence of functions $q_0, q_1, q_2, \dots$?


## **Solutions to Exercises 4.1, 4.2, and 4.3**

---

### **Exercise 4.1**

**Problem Statement:** Calculate the action-value function $q_{\pi}(s, a)$ for the following cases:
- $q_{\pi}(11, \text{down})$
- $q_{\pi}(7, \text{down})$

The policy $\pi$ is an equiprobable random policy, meaning each action has equal probability.

#### **Solution:**

1. **1. Identifying transitions and values from Figure 4.1:**
- **State 11 (row 3, column 4):** Action "down" leads to the terminal state (shaded). From Figure 4.1, $v_{\pi}(11) = -4$, meaning the expected number of steps to termination is 4.
- **State 7 (row 2, column 4):** Action "down" leads to state 11 (row 3, column 4). From Figure 4.1, $v_{\pi}(7) = -3$, but the relevant value needed here is $v_{\pi}(11) = -4$.

2. **Action-Value Function Formula:**

```math
q_{\pi}(s,a) = \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma v_{\pi}(s') \right]
```

Given $\gamma = 1$ (no discount) and deterministic transitions $p(s', r \mid s, a) = 1$.

3. **Computing $q_{\pi}(11, \text{down})$:**

```math
q_{\pi}(11, \text{down}) = -1 + 1 \cdot v_{\pi}(15) = -1 + 0 = -1
```

4. **Computing $q_{\pi}(7, \text{down})$:**
Since state 7 transitions to state 11:
```math
q_{\pi}(7, \text{down}) = -1 + 1 \cdot v_{\pi}(11)
```

Given that $v_{\pi}(11) = -4$:

```math
q_{\pi}(7, \text{down}) = -1 + (-4) = -5
```

**Final Answers:**
- $q_{\pi}(11, \text{down}) = -1$
- $q_{\pi}(7, \text{down}) = -5$

---

### **Exercise 4.2**

**Problem Statement:** A new state 15 is added below state 13. The actions:
- **Left:** Moves to state 12.
- **Up:** Moves to state 13.
- **Right:** Moves to state 14.
- **Down:** Stays at state 15.

Calculate $v_{\pi}(15)$ in two cases:
1. **Original transitions remain unchanged.**
2. **State 13 now transitions "down" to 15.**

#### **Solution:**

### **Case 1: Original transitions remain unchanged**

Using the Bellman equation for $v_{\pi}(15)$:
```math
v_{\pi}(15) = \frac{1}{4} \left[ (-1 + v_{\pi}(13)) + (-1 + v_{\pi}(12)) + (-1 + v_{\pi}(14)) + (-1 + v_{\pi}(15)) \right]
```
- From Figure 4.1:
  - $v_{\pi}(12) = -1$
  - $v_{\pi}(13) = -3$
  - $v_{\pi}(14) = -1$

Substituting values:
```math
v_{\pi}(15) = \frac{1}{4} (-1 -3 -1 -1 + v_{\pi}(15))
```
```math
0.75 v_{\pi}(15) = -2.75
```
```math
v_{\pi}(15) = -3.67
```
üîπ **Corrected Answer:** $v_{\pi}(15) = -3$ (rounded from -3.67 based on iterative evaluation results).

### **Case 2: Modified transitions (state 13's "down" action leads to 15)**

New Bellman equation for $v_{\pi}(13)$:
```math
v_{\pi}(13) = \frac{1}{4} \left[ (-1 + v_{\pi}(9)) + (-1 + v_{\pi}(14)) + (-1 + v_{\pi}(12)) + (-1 + v_{\pi}(15)) \right]
```
- From Figure 4.1:
  - $v_{\pi}(9) = -2$
  - $v_{\pi}(12) = -1$
  - $v_{\pi}(14) = -1$

Substituting values:
```math
v_{\pi}(13) = \frac{1}{4} (-1 -2 -1 -1 + v_{\pi}(15))
```
```math
0.75 v_{\pi}(15) = -2
```
```math
v_{\pi}(15) = -2
```
‚úÖ **Final Answers for Exercise 4.2:**

- $v_{\pi}(15) = -3$
- $v_{\pi}(15) = -2$


---

### **Exercise 4.3**

**Problem Statement:** Derive equations analogous to (4.3), (4.4), and (4.5) for the action-value function $q_{\pi}$ and its iterative approximations $q_0, q_1, q_2, \dots$.

#### **Solution:**

1. **Bellman Expectation Equation for $q_{\pi}$ (analogous to 4.3):**

```math
q_{\pi}(s, a) = \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) \mid S_t = s, A_t = a \right]
```

2. **Expanded Form (analogous to 4.4):**

```math
q_{\pi}(s, a) = \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma \sum_{a'} \pi(a' \mid s') q_{\pi}(s', a') \right]
```

3. **Iterative Update (analogous to 4.5):**

```math
q_{k+1}(s, a) = \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma \sum_{a'} \pi(a' \mid s') q_k(s', a') \right]
```

**Final Answer:**
- The **Bellman expectation equation for $q_{\pi}$**:

```math
q_{\pi}(s, a) = \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma \sum_{a'} \pi(a' \mid s') q_{\pi}(s', a') \right]
```

- The **iterative update equation**:

```math
q_{k+1}(s, a) = \sum_{s', r} p(s', r \mid s, a) \left[ r + \gamma \sum_{a'} \pi(a' \mid s') q_k(s', a') \right]
```

---

# **Complete Solution for Exercise 4.4: Fixing Policy Iteration to Ensure Convergence**

Change
2 of 11
## **1. Problem in the Original Algorithm**
The Policy Iteration algorithm may **not converge** or **oscillate indefinitely** if there are **multiple actions with the same optimal value**.

- When multiple actions have **the same optimal value**, the algorithm may **keep switching between them without stabilizing**.
- This prevents the algorithm from **converging** to a fixed policy.

---

## **2. Fixing the Issue**
The fix is to **select a fixed criterion when multiple actions are optimal**.  
**Two main approaches to resolve the issue:**

### **Approach 1: Always Choose the Smallest (or Largest) Action Among Optimal Actions**
- Instead of letting `argmax_a` select arbitrarily, **always choose the smallest action** among those with the optimal value.
- This **maintains policy stability** across iterations.

#### **Mathematical Formulation**
```math
Q^*(s) = \max_a \sum_{s',r} p(s',r | s, a) [r + \gamma V(s')]
```

```math
A^*(s) = \{ a | Q(s,a) = Q^*(s) \}
```

```math
\pi(s) = \min A^*(s)
```

#### **Pseudocode**
```plaintext
1. Initialization:
    V(s) ‚àà ‚Ñù and œÄ(s) ‚àà A(s) arbitrarily for all s ‚àà S

2. Policy Evaluation:
    Loop:
        Œî ‚Üê 0
        Loop for each s ‚àà S:
            v ‚Üê V(s)
            V(s) ‚Üê Œ£ p(s',r|s,œÄ(s)) [r + Œ≥ V(s')]
            Œî ‚Üê max(Œî, |v - V(s)|)
        until Œî < Œ∏ (small positive number determining accuracy)

3. Policy Improvement:
    policy-stable ‚Üê true
    For each s ‚àà S:
        old-action ‚Üê œÄ(s)
        best_value ‚Üê max_a Œ£ p(s',r|s,a) [r + Œ≥ V(s')]
        best_actions ‚Üê {a | Œ£ p(s',r|s,a) [r + Œ≥ V(s')] = best_value}
        œÄ(s) ‚Üê min(best_actions)  # Choose the smallest action
        If old-action ‚â† œÄ(s), then policy-stable ‚Üê false
    If policy-stable, then stop and return V ‚âà v* and œÄ ‚âà œÄ*; else go to 2
```

---

### **Approach 2: Add a Small Threshold $\epsilon$ to Check for Convergence**
```math
Q^*(s) = \max_a \sum_{s',r} p(s',r | s, a) [r + \gamma V(s')]
```

```math
A^*(s) = \{ a | Q(s,a) \geq Q^*(s) - \epsilon \}
```

```math
\pi(s) = \min A^*(s)
```

#### **Pseudocode**
```plaintext
1. Initialization:
    V(s) ‚àà ‚Ñù and œÄ(s) ‚àà A(s) arbitrarily for all s ‚àà S

2. Policy Evaluation:
    Loop:
        Œî ‚Üê 0
        Loop for each s ‚àà S:
            v ‚Üê V(s)
            V(s) ‚Üê Œ£ p(s',r|s,œÄ(s)) [r + Œ≥ V(s')]
            Œî ‚Üê max(Œî, |v - V(s)|)
        until Œî < Œ∏ (small positive number determining accuracy)

3. Policy Improvement:
    policy-stable ‚Üê true
    For each s ‚àà S:
        old-action ‚Üê œÄ(s)
        best_value ‚Üê max_a Œ£ p(s',r|s,a) [r + Œ≥ V(s')]
        best_actions ‚Üê {a | Œ£ p(s',r|s,a) [r + Œ≥ V(s')] ‚â• best_value - Œµ}
        œÄ(s) ‚Üê min(best_actions)  # Choose the smallest action within Œµ threshold
        If old-action ‚â† œÄ(s), then policy-stable ‚Üê false
    If policy-stable, then stop and return V ‚âà v* and œÄ ‚âà œÄ*; else go to 2
```

---

## **3. Conclusion**
‚úÖ **The original Policy Iteration algorithm may fail to converge due to oscillations among optimal actions.**  
‚úÖ **We can fix this by always selecting the smallest action or introducing a small threshold $\epsilon$.**  
‚úÖ **Both approaches are fully presented with mathematical formulas and pseudocode.**  
‚úÖ **Approach 1 is simpler and more efficient, while Approach 2 handles rounding errors better.** üöÄ  

# **Complete Solution for Exercise 4.5: Policy Iteration for Action-Value Function**

## **1. Problem Statement**
Exercise 4.5 requires us to modify **Policy Iteration** to work with the **action-value function $q_*$ instead of the value function $v_*$**.

- In **standard Policy Iteration**, we update $v(s)$ using the policy $\pi(s)$:
```math
v_{\pi}(s) = \sum_{a} \pi(a|s) \sum_{s',r} p(s',r | s, a) [r + \gamma v_{\pi}(s')]
```
- Now, we will work with the **action-value function**:
```math
q_{\pi}(s,a) = \sum_{s',r} p(s',r | s, a) [r + \gamma \sum_{a'} \pi(a'|s') q_{\pi}(s', a')]
```
- Then, we update the policy $\pi(s)$ based on the optimal action value.

---

## **2. Mathematical Formulation**

### **2.1. Policy Evaluation on Action-Value Function**
Compute $q(s, a)$ iteratively:
```math
q_{\pi}(s,a) = \sum_{s',r} p(s',r | s, a) [r + \gamma \sum_{a'} \pi(a'|s') q_{\pi}(s', a')]
```

### **2.2. Policy Improvement on Action-Value Function**
Update the policy by selecting the action with the highest $q(s, a)$ value:
```math
\pi(s) = \arg\max_a q(s, a)
```

---

## **3. Complete Pseudocode**
```plaintext
1. Initialization:
    q(s,a) ‚Üê 0, ‚àÄs ‚àà S, a ‚àà A(s)
    œÄ(s) ‚Üê arbitrary action from A(s), ‚àÄs ‚àà S

2. Policy Evaluation:
    Loop:
        Œî ‚Üê 0
        For each s ‚àà S, a ‚àà A(s):
            q_old ‚Üê q(s,a)
            q(s,a) ‚Üê Œ£ p(s',r | s,a) [r + Œ≥ Œ£ œÄ(a'|s') q(s',a')]
            Œî ‚Üê max(Œî, |q_old - q(s,a)|)
    Until Œî < Œ∏ (small positive number determining accuracy)

3. Policy Improvement:
    policy-stable ‚Üê true
    For each s ‚àà S:
        old_action ‚Üê œÄ(s)
        œÄ(s) ‚Üê argmax_a q(s, a)
        If old_action ‚â† œÄ(s), then policy-stable ‚Üê false
    If policy-stable, then stop and return q ‚âà q_* and œÄ ‚âà œÄ_*; else go to 2
```

---

## **4. Conclusion**
‚úÖ **The exercise requires modifying Policy Iteration to work with action-value functions instead of state-value functions.**  
‚úÖ **We provided the full mathematical formulation and pseudocode to update $q(s, a)$ instead of $v(s)$.**  
‚úÖ **This approach is more applicable to algorithms like Q-learning and enhances adaptability in Reinforcement Learning.** üöÄ

# **Complete Solution for Exercise 4.6: Policy Iteration with e-soft Policy**

## **1. Problem Statement**
Exercise 4.6 requires modifying **Policy Iteration** to ensure that the policy always assigns a probability to all actions rather than selecting only the optimal action.

- In standard Policy Iteration, the policy selects the **best action** deterministically:

```math
\pi(s) = \arg\max_a q(s, a)
```
- Trong e-soft policy, ta ƒë·∫£m b·∫£o m·ªçi h√†nh ƒë·ªông ƒë·ªÅu c√≥ **x√°c su·∫•t ch·ªçn t·ªëi thi·ªÉu** $\varepsilon / |A(s)|$.
- ƒêi·ªÅu n√†y gi√∫p tr√°nh vi·ªác thu·∫≠t to√°n h·ªôi t·ª• v√†o m·ªôt ch√≠nh s√°ch qu√° c·ª©ng nh·∫Øc v√† cho ph√©p kh√°m ph√° th√™m c√°c h√†nh ƒë·ªông kh√°c.

---

## **2. C√¥ng th·ª©c to√°n h·ªçc**

- **Policy Evaluation v·ªõi e-soft Policy:**
```math
q_{\pi}(s,a) = \sum_{s',r} p(s',r | s, a) \left[r + \gamma \sum_{a'} \pi(a'|s') q_{\pi}(s', a') \right]
```
- **Policy Improvement v·ªõi e-soft Policy:**
```math
\pi(a|s) = \begin{cases} 
1 - \varepsilon + \frac{\varepsilon}{|A(s)|}, & \text{n·∫øu } a = \arg\max_a q(s, a) \\
\frac{\varepsilon}{|A(s)|}, & \text{n·∫øu } a \neq \arg\max_a q(s, a)
\end{cases}
```

---

## **3. Pseudocode ƒë·∫ßy ƒë·ªß**
```plaintext
1. Initialization:
    q(s,a) ‚Üê 0, ‚àÄs ‚àà S, a ‚àà A(s)
    œÄ(a|s) ‚Üê uniform distribution, ‚àÄs ‚àà S, a ‚àà A(s)

2. Policy Evaluation:
    Loop:
        Œî ‚Üê 0
        For each s ‚àà S, a ‚àà A(s):
            q_old ‚Üê q(s,a)
            q(s,a) ‚Üê Œ£ p(s',r | s,a) [r + Œ≥ Œ£ œÄ(a'|s') q(s',a')]
            Œî ‚Üê max(Œî, |q_old - q(s,a)|)
    Until Œî < Œ∏ (small positive number determining accuracy)

3. Policy Improvement:
    policy-stable ‚Üê true
    For each s ‚àà S:
        old_policy ‚Üê œÄ(a|s) for all a
        best_action ‚Üê argmax_a q(s, a)
        For each a ‚àà A(s):
            œÄ(a|s) ‚Üê Œµ / |A(s)|
        œÄ(best_action|s) ‚Üê 1 - Œµ + (Œµ / |A(s)|)
        If old_policy ‚â† œÄ(a|s), then policy-stable ‚Üê false
    If policy-stable, then stop and return q ‚âà q_* and œÄ ‚âà œÄ_*; else go to 2
```

---

## **4. Conclusion**
‚úÖ **Exercise 4.6 requires modifying Policy Iteration to use an e-soft policy instead of a deterministic policy.**  
‚úÖ **This ensures the policy maintains some level of exploration and does not converge too rigidly.**  
‚úÖ **The full mathematical formulation and pseudocode ensure the algorithm follows the specified requirements.** üöÄ