{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "pjFPwBDf3UGP"
      },
      "source": [
        "# Learning from Heterogeneous Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fTWmy3gCJ_8a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "!pip install -q torch-scatter~=2.1.0 torch-sparse~=0.6.16 torch-cluster~=1.6.0 torch-spline-conv~=1.2.1 torch-geometric==2.2.0 -f https://data.pyg.org/whl/torch-{torch.__version__}.html\n",
        "\n",
        "torch.manual_seed(0)\n",
        "torch.cuda.manual_seed(0)\n",
        "torch.cuda.manual_seed_all(0)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Z0FxDj17J1ir"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn import MessagePassing\n",
        "from torch_geometric.utils import add_self_loops, degree\n",
        "\n",
        "class GCNConv(MessagePassing):\n",
        "    def __init__(self, dim_in, dim_h):\n",
        "        super().__init__(aggr='add')\n",
        "        self.linear = Linear(dim_in, dim_h, bias=False)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
        "\n",
        "        x = self.linear(x)\n",
        "\n",
        "        row, col = edge_index\n",
        "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
        "        deg_inv_sqrt = deg.pow(-0.5)\n",
        "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
        "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
        "\n",
        "        out = self.propagate(edge_index, x=x, norm=norm)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def message(self, x, norm):\n",
        "        return norm.view(-1, 1) * x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VOxgX0sQKCTx"
      },
      "outputs": [],
      "source": [
        "conv = GCNConv(16, 32)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KqjK7PVZKtLg"
      },
      "source": [
        "## Heterogeneous graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EWnTph5t3SW5",
        "outputId": "01cd6b23-3850-4b0a-c79d-72cbab87bf45"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "HeteroData(\n",
              "  \u001b[1muser\u001b[0m={ x=[3, 4] },\n",
              "  \u001b[1mgame\u001b[0m={ x=[2, 2] },\n",
              "  \u001b[1mdev\u001b[0m={ x=[2, 1] },\n",
              "  \u001b[1m(user, follows, user)\u001b[0m={ edge_index=[2, 2] },\n",
              "  \u001b[1m(user, plays, game)\u001b[0m={\n",
              "    edge_index=[2, 4],\n",
              "    edge_attr=[4, 1]\n",
              "  },\n",
              "  \u001b[1m(dev, develops, game)\u001b[0m={ edge_index=[2, 2] }\n",
              ")"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from torch_geometric.data import HeteroData\n",
        "\n",
        "data = HeteroData()\n",
        "\n",
        "data['user'].x = torch.Tensor([[1, 1, 1, 1], [2, 2, 2, 2], [3, 3, 3, 3]]) # [num_users, num_features_users]\n",
        "data['game'].x = torch.Tensor([[1, 1], [2, 2]])\n",
        "data['dev'].x = torch.Tensor([[1], [2]])\n",
        "\n",
        "data['user', 'follows', 'user'].edge_index = torch.Tensor([[0, 1], [1, 2]]) # [2, num_edges_follows]\n",
        "data['user', 'plays', 'game'].edge_index = torch.Tensor([[0, 1, 1, 2], [0, 0, 1, 1]])\n",
        "data['dev', 'develops', 'game'].edge_index = torch.Tensor([[0, 1], [0, 1]])\n",
        "\n",
        "data['user', 'plays', 'game'].edge_attr = torch.Tensor([[2], [0.5], [10], [12]])\n",
        "\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LM57bFo_HTl",
        "outputId": "48465ce5-8521-483e-85f1-bade4f9f7ed6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://www.dropbox.com/s/yh4grpeks87ugr2/DBLP_processed.zip?dl=1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting ./raw/DBLP_processed.zip\n",
            "Processing...\n",
            "Done!\n",
            "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch_sparse/matmul.py:97: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
            "  C = torch.sparse.mm(A, B)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HeteroData(\n",
            "  metapath_dict={ (author, metapath_0, author)=[2] },\n",
            "  \u001b[1mauthor\u001b[0m={\n",
            "    x=[4057, 334],\n",
            "    y=[4057],\n",
            "    train_mask=[4057],\n",
            "    val_mask=[4057],\n",
            "    test_mask=[4057]\n",
            "  },\n",
            "  \u001b[1mpaper\u001b[0m={ x=[14328, 4231] },\n",
            "  \u001b[1mterm\u001b[0m={ x=[7723, 50] },\n",
            "  \u001b[1mconference\u001b[0m={ num_nodes=20 },\n",
            "  \u001b[1m(author, metapath_0, author)\u001b[0m={ edge_index=[2, 11113] }\n",
            ")\n",
            "Epoch:   0 | Train Loss: 1.4351 | Train Acc: 25.25% | Val Acc: 22.00%\n",
            "Epoch:  20 | Train Loss: 1.2815 | Train Acc: 46.50% | Val Acc: 37.50%\n",
            "Epoch:  40 | Train Loss: 1.1641 | Train Acc: 63.75% | Val Acc: 53.25%\n",
            "Epoch:  60 | Train Loss: 1.0628 | Train Acc: 76.50% | Val Acc: 63.25%\n",
            "Epoch:  80 | Train Loss: 0.9771 | Train Acc: 81.00% | Val Acc: 66.25%\n",
            "Epoch: 100 | Train Loss: 0.9040 | Train Acc: 83.50% | Val Acc: 67.75%\n",
            "Test accuracy: 72.43%\n"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import DBLP\n",
        "from torch_geometric.nn import GAT\n",
        "\n",
        "metapaths = [[('author', 'paper'), ('paper', 'author')]]\n",
        "transform = T.AddMetaPaths(metapaths=metapaths, drop_orig_edge_types=True)\n",
        "dataset = DBLP('.', transform=transform)\n",
        "data = dataset[0]\n",
        "print(data)\n",
        "\n",
        "model = GAT(in_channels=-1, hidden_channels=64, out_channels=4, num_layers=1)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data, model = data.to(device), model.to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(mask):\n",
        "    model.eval()\n",
        "    pred = model(data.x_dict['author'], data.edge_index_dict[('author', 'metapath_0', 'author')]).argmax(dim=-1)\n",
        "    acc = (pred[mask] == data['author'].y[mask]).sum() / mask.sum()\n",
        "    return float(acc)\n",
        "\n",
        "for epoch in range(101):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x_dict['author'], data.edge_index_dict[('author', 'metapath_0', 'author')])\n",
        "    mask = data['author'].train_mask\n",
        "    loss = F.cross_entropy(out[mask], data['author'].y[mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        train_acc = test(data['author'].train_mask)\n",
        "        val_acc = test(data['author'].val_mask)\n",
        "        print(f'Epoch: {epoch:>3} | Train Loss: {loss:.4f} | Train Acc: {train_acc*100:.2f}% | Val Acc: {val_acc*100:.2f}%')\n",
        "\n",
        "test_acc = test(data['author'].test_mask)\n",
        "print(f'Test accuracy: {test_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFROVbIHhDoj",
        "outputId": "48e0bd72-d4b6-41f6-9d72-b0f3e99adbc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GraphModule(\n",
            "  (conv): ModuleDict(\n",
            "    (author__to__paper): GATConv((-1, -1), 64, heads=1)\n",
            "    (paper__to__author): GATConv((-1, -1), 64, heads=1)\n",
            "    (paper__to__term): GATConv((-1, -1), 64, heads=1)\n",
            "    (paper__to__conference): GATConv((-1, -1), 64, heads=1)\n",
            "    (term__to__paper): GATConv((-1, -1), 64, heads=1)\n",
            "    (conference__to__paper): GATConv((-1, -1), 64, heads=1)\n",
            "  )\n",
            "  (linear): ModuleDict(\n",
            "    (author): Linear(in_features=64, out_features=4, bias=True)\n",
            "    (paper): Linear(in_features=64, out_features=4, bias=True)\n",
            "    (term): Linear(in_features=64, out_features=4, bias=True)\n",
            "    (conference): Linear(in_features=64, out_features=4, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "\n",
            "\n",
            "def forward(self, x, edge_index):\n",
            "    x_dict = torch_geometric_nn_to_hetero_transformer_get_dict(x);  x = None\n",
            "    x__author = x_dict.get('author', None)\n",
            "    x__paper = x_dict.get('paper', None)\n",
            "    x__term = x_dict.get('term', None)\n",
            "    x__conference = x_dict.get('conference', None);  x_dict = None\n",
            "    edge_index_dict = torch_geometric_nn_to_hetero_transformer_get_dict(edge_index);  edge_index = None\n",
            "    edge_index__author__to__paper = edge_index_dict.get(('author', 'to', 'paper'), None)\n",
            "    edge_index__paper__to__author = edge_index_dict.get(('paper', 'to', 'author'), None)\n",
            "    edge_index__paper__to__term = edge_index_dict.get(('paper', 'to', 'term'), None)\n",
            "    edge_index__paper__to__conference = edge_index_dict.get(('paper', 'to', 'conference'), None)\n",
            "    edge_index__term__to__paper = edge_index_dict.get(('term', 'to', 'paper'), None)\n",
            "    edge_index__conference__to__paper = edge_index_dict.get(('conference', 'to', 'paper'), None);  edge_index_dict = None\n",
            "    conv__paper1 = self.conv.author__to__paper((x__author, x__paper), edge_index__author__to__paper);  edge_index__author__to__paper = None\n",
            "    conv__author = self.conv.paper__to__author((x__paper, x__author), edge_index__paper__to__author);  x__author = edge_index__paper__to__author = None\n",
            "    conv__term = self.conv.paper__to__term((x__paper, x__term), edge_index__paper__to__term);  edge_index__paper__to__term = None\n",
            "    conv__conference = self.conv.paper__to__conference((x__paper, x__conference), edge_index__paper__to__conference);  edge_index__paper__to__conference = None\n",
            "    conv__paper2 = self.conv.term__to__paper((x__term, x__paper), edge_index__term__to__paper);  x__term = edge_index__term__to__paper = None\n",
            "    conv__paper3 = self.conv.conference__to__paper((x__conference, x__paper), edge_index__conference__to__paper);  x__conference = x__paper = edge_index__conference__to__paper = None\n",
            "    conv__paper_1 = torch.add(conv__paper1, conv__paper2);  conv__paper1 = conv__paper2 = None\n",
            "    conv__paper = torch.add(conv__paper3, conv__paper_1);  conv__paper3 = conv__paper_1 = None\n",
            "    relu__author = conv__author.relu();  conv__author = None\n",
            "    relu__paper = conv__paper.relu();  conv__paper = None\n",
            "    relu__term = conv__term.relu();  conv__term = None\n",
            "    relu__conference = conv__conference.relu();  conv__conference = None\n",
            "    linear__author = self.linear.author(relu__author);  relu__author = None\n",
            "    linear__paper = self.linear.paper(relu__paper);  relu__paper = None\n",
            "    linear__term = self.linear.term(relu__term);  relu__term = None\n",
            "    linear__conference = self.linear.conference(relu__conference);  relu__conference = None\n",
            "    return {'author': linear__author, 'paper': linear__paper, 'term': linear__term, 'conference': linear__conference}\n",
            "    \n",
            "# To see more debug info, please use `graph_module.print_readable()`\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:   0 | Train Loss: 1.3974 | Train Acc: 20.75% | Val Acc: 23.00%\n",
            "Epoch:  20 | Train Loss: 1.2047 | Train Acc: 95.25% | Val Acc: 68.00%\n",
            "Epoch:  40 | Train Loss: 0.8654 | Train Acc: 96.75% | Val Acc: 67.50%\n",
            "Epoch:  60 | Train Loss: 0.5061 | Train Acc: 98.75% | Val Acc: 73.50%\n",
            "Epoch:  80 | Train Loss: 0.2580 | Train Acc: 99.50% | Val Acc: 73.50%\n",
            "Epoch: 100 | Train Loss: 0.1384 | Train Acc: 100.00% | Val Acc: 74.00%\n",
            "Test accuracy: 78.63%\n"
          ]
        }
      ],
      "source": [
        "from torch_geometric.nn import GATConv, Linear, to_hetero\n",
        "\n",
        "dataset = DBLP(root='.')\n",
        "data = dataset[0]\n",
        "\n",
        "data['conference'].x = torch.zeros(20, 1)\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, dim_h, dim_out):\n",
        "        super().__init__()\n",
        "        self.conv = GATConv((-1, -1), dim_h, add_self_loops=False)\n",
        "        self.linear = nn.Linear(dim_h, dim_out)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        h = self.conv(x, edge_index).relu()\n",
        "        h = self.linear(h)\n",
        "        return h\n",
        "\n",
        "model = GAT(dim_h=64, dim_out=4)\n",
        "model = to_hetero(model, data.metadata(), aggr='sum')\n",
        "print(model)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data, model = data.to(device), model.to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(mask):\n",
        "    model.eval()\n",
        "    pred = model(data.x_dict, data.edge_index_dict)['author'].argmax(dim=-1)\n",
        "    acc = (pred[mask] == data['author'].y[mask]).sum() / mask.sum()\n",
        "    return float(acc)\n",
        "\n",
        "for epoch in range(101):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x_dict, data.edge_index_dict)['author']\n",
        "    mask = data['author'].train_mask\n",
        "    loss = F.cross_entropy(out[mask], data['author'].y[mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        train_acc = test(data['author'].train_mask)\n",
        "        val_acc = test(data['author'].val_mask)\n",
        "        print(f'Epoch: {epoch:>3} | Train Loss: {loss:.4f} | Train Acc: {train_acc*100:.2f}% | Val Acc: {val_acc*100:.2f}%')\n",
        "\n",
        "test_acc = test(data['author'].test_mask)\n",
        "print(f'Test accuracy: {test_acc*100:.2f}%')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "xADuuEQseUrj"
      },
      "source": [
        "## Hierarchical Self-Attention Network (HAN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "original"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0AcupaiIddjz",
        "outputId": "63bece01-a990-440e-f5d5-002dced07c83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HeteroData(\n",
            "  \u001b[1mauthor\u001b[0m={\n",
            "    x=[4057, 334],\n",
            "    y=[4057],\n",
            "    train_mask=[4057],\n",
            "    val_mask=[4057],\n",
            "    test_mask=[4057]\n",
            "  },\n",
            "  \u001b[1mpaper\u001b[0m={ x=[14328, 4231] },\n",
            "  \u001b[1mterm\u001b[0m={ x=[7723, 50] },\n",
            "  \u001b[1mconference\u001b[0m={ num_nodes=20 },\n",
            "  \u001b[1m(author, to, paper)\u001b[0m={ edge_index=[2, 19645] },\n",
            "  \u001b[1m(paper, to, author)\u001b[0m={ edge_index=[2, 19645] },\n",
            "  \u001b[1m(paper, to, term)\u001b[0m={ edge_index=[2, 85810] },\n",
            "  \u001b[1m(paper, to, conference)\u001b[0m={ edge_index=[2, 14328] },\n",
            "  \u001b[1m(term, to, paper)\u001b[0m={ edge_index=[2, 85810] },\n",
            "  \u001b[1m(conference, to, paper)\u001b[0m={ edge_index=[2, 14328] }\n",
            ")\n",
            "Epoch:   0 | Train Loss: 1.3867 | Train Acc: 32.75% | Val Acc: 26.25%\n",
            "Epoch:  20 | Train Loss: 1.1576 | Train Acc: 94.75% | Val Acc: 69.25%\n",
            "Epoch:  40 | Train Loss: 0.7842 | Train Acc: 96.75% | Val Acc: 74.00%\n",
            "Epoch:  60 | Train Loss: 0.4900 | Train Acc: 98.50% | Val Acc: 78.00%\n",
            "Epoch:  80 | Train Loss: 0.2945 | Train Acc: 99.25% | Val Acc: 80.00%\n",
            "Epoch: 100 | Train Loss: 0.2175 | Train Acc: 100.00% | Val Acc: 79.25%\n",
            "Test accuracy: 81.52%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import DBLP\n",
        "from torch_geometric.nn import HANConv, Linear\n",
        "\n",
        "\n",
        "dataset = DBLP('.')\n",
        "data = dataset[0]\n",
        "print(data)\n",
        "\n",
        "data['conference'].x = torch.zeros(20, 1)\n",
        "\n",
        "class HAN(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, dim_h=128, heads=8):\n",
        "        super().__init__()\n",
        "        self.han = HANConv(dim_in, dim_h, heads=heads, dropout=0.6, metadata=data.metadata())\n",
        "        self.linear = nn.Linear(dim_h, dim_out)\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        out = self.han(x_dict, edge_index_dict)\n",
        "        out = self.linear(out['author'])\n",
        "        return out\n",
        "\n",
        "model = HAN(dim_in=-1, dim_out=4)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data, model = data.to(device), model.to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(mask):\n",
        "    model.eval()\n",
        "    pred = model(data.x_dict, data.edge_index_dict).argmax(dim=-1)\n",
        "    acc = (pred[mask] == data['author'].y[mask]).sum() / mask.sum()\n",
        "    return float(acc)\n",
        "\n",
        "for epoch in range(101):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x_dict, data.edge_index_dict)\n",
        "    mask = data['author'].train_mask\n",
        "    loss = F.cross_entropy(out[mask], data['author'].y[mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 20 == 0:\n",
        "        train_acc = test(data['author'].train_mask)\n",
        "        val_acc = test(data['author'].val_mask)\n",
        "        print(f'Epoch: {epoch:>3} | Train Loss: {loss:.4f} | Train Acc: {train_acc*100:.2f}% | Val Acc: {val_acc*100:.2f}%')\n",
        "\n",
        "test_acc = test(data['author'].test_mask)\n",
        "print(f'Test accuracy: {test_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "cải tiến mô hình HAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "version A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== DBLP Dataset Information ===\n",
            "HeteroData(\n",
            "  \u001b[1mauthor\u001b[0m={\n",
            "    x=[4057, 334],\n",
            "    y=[4057],\n",
            "    train_mask=[4057],\n",
            "    val_mask=[4057],\n",
            "    test_mask=[4057]\n",
            "  },\n",
            "  \u001b[1mpaper\u001b[0m={ x=[14328, 4231] },\n",
            "  \u001b[1mterm\u001b[0m={ x=[7723, 50] },\n",
            "  \u001b[1mconference\u001b[0m={ num_nodes=20 },\n",
            "  \u001b[1m(author, to, paper)\u001b[0m={ edge_index=[2, 19645] },\n",
            "  \u001b[1m(paper, to, author)\u001b[0m={ edge_index=[2, 19645] },\n",
            "  \u001b[1m(paper, to, term)\u001b[0m={ edge_index=[2, 85810] },\n",
            "  \u001b[1m(paper, to, conference)\u001b[0m={ edge_index=[2, 14328] },\n",
            "  \u001b[1m(term, to, paper)\u001b[0m={ edge_index=[2, 85810] },\n",
            "  \u001b[1m(conference, to, paper)\u001b[0m={ edge_index=[2, 14328] }\n",
            ")\n",
            "Node type 'conference' is missing features. Assigning dummy features of shape (20, 1).\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:   0 | Loss: 1.3822 | Train Acc: 30.25% | Val Acc: 26.25%\n",
            "Epoch:  20 | Loss: 1.2255 | Train Acc: 81.25% | Val Acc: 59.50%\n",
            "Epoch:  40 | Loss: 0.9450 | Train Acc: 91.50% | Val Acc: 67.50%\n",
            "Epoch:  60 | Loss: 0.6405 | Train Acc: 96.50% | Val Acc: 72.50%\n",
            "Epoch:  80 | Loss: 0.4617 | Train Acc: 99.00% | Val Acc: 78.75%\n",
            "Epoch: 100 | Loss: 0.3363 | Train Acc: 99.25% | Val Acc: 78.50%\n",
            "\n",
            "Test Accuracy: 82.01%\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import DBLP\n",
        "from torch_geometric.nn import HANConv\n",
        "\n",
        "##############################\n",
        "# 1. Set Seed for Reproducibility\n",
        "##############################\n",
        "def set_seed(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    # Ensure deterministic behavior\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "##############################\n",
        "# 2. Load the DBLP Dataset\n",
        "##############################\n",
        "# We load the full heterogeneous graph. Note that we are not filtering out any node types.\n",
        "dataset = DBLP(root='.')\n",
        "data = dataset[0]\n",
        "print(\"=== DBLP Dataset Information ===\")\n",
        "print(data)\n",
        "\n",
        "# For node types missing features, e.g., 'conference', assign dummy features.\n",
        "if 'conference' in data and 'x' not in data['conference']:\n",
        "    num_conferences = data['conference'].num_nodes\n",
        "    # Create a dummy feature vector (here, zeros with dimension 1).\n",
        "    data['conference'].x = torch.zeros((num_conferences, 1))\n",
        "\n",
        "# (Optional) Check if any other node type is missing features:\n",
        "for node_type in data.node_types:\n",
        "    if 'x' not in data[node_type]:\n",
        "        num_nodes = data[node_type].num_nodes\n",
        "        print(f\"Node type '{node_type}' is missing features. Assigning dummy features of shape ({num_nodes}, 1).\")\n",
        "        data[node_type].x = torch.zeros((num_nodes, 1))\n",
        "\n",
        "##############################\n",
        "# 3. Define the HAN Model\n",
        "##############################\n",
        "class HAN(torch.nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, dim_h=128, heads=8, dropout=0.6):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            dim_in (int): Input feature dimension. Set to -1 to let PyG infer per node type.\n",
        "            dim_out (int): Number of output classes (for DBLP author classification, 4 classes).\n",
        "            dim_h (int): Hidden dimension.\n",
        "            heads (int): Number of attention heads.\n",
        "            dropout (float): Dropout probability.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # HANConv leverages all available meta-relations in the heterogeneous graph.\n",
        "        self.han_conv = HANConv(dim_in, dim_h, heads=heads, dropout=dropout, metadata=data.metadata())\n",
        "        self.linear = nn.Linear(dim_h, dim_out)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        # HANConv returns a dict mapping each node type to its learned embedding.\n",
        "        out_dict = self.han_conv(x_dict, edge_index_dict)\n",
        "        # We only need the 'author' embeddings for our classification task.\n",
        "        author_emb = out_dict['author']\n",
        "        author_emb = F.dropout(author_emb, p=self.dropout, training=self.training)\n",
        "        out = self.linear(author_emb)\n",
        "        return out\n",
        "\n",
        "##############################\n",
        "# 4. Initialize Model, Optimizer, and Device\n",
        "##############################\n",
        "model = HAN(dim_in=-1, dim_out=4, dim_h=128, heads=8, dropout=0.6)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "data = data.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)\n",
        "\n",
        "##############################\n",
        "# 5. Define the Test Function\n",
        "##############################\n",
        "@torch.no_grad()\n",
        "def test(mask):\n",
        "    model.eval()\n",
        "    out = model(data.x_dict, data.edge_index_dict)\n",
        "    pred = out.argmax(dim=-1)\n",
        "    correct = (pred[mask] == data['author'].y[mask]).sum().item()\n",
        "    acc = correct / mask.sum().item()\n",
        "    return acc\n",
        "\n",
        "##############################\n",
        "# 6. Training Loop\n",
        "##############################\n",
        "num_epochs = 101\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    out = model(data.x_dict, data.edge_index_dict)\n",
        "    loss = F.cross_entropy(out[data['author'].train_mask],\n",
        "                           data['author'].y[data['author'].train_mask])\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if epoch % 20 == 0:\n",
        "        train_acc = test(data['author'].train_mask)\n",
        "        val_acc = test(data['author'].val_mask)\n",
        "        print(f\"Epoch: {epoch:>3} | Loss: {loss:.4f} | Train Acc: {train_acc*100:.2f}% | Val Acc: {val_acc*100:.2f}%\")\n",
        "\n",
        "##############################\n",
        "# 7. Final Evaluation on Test Set\n",
        "##############################\n",
        "test_acc = test(data['author'].test_mask)\n",
        "print(f\"\\nTest Accuracy: {test_acc*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "version B"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch:   0 | Train Loss: 1.6305 | Train Acc: 30.10% | Val Acc: 27.13%\n",
            "Epoch:  20 | Train Loss: 1.3850 | Train Acc: 42.22% | Val Acc: 36.74%\n",
            "Epoch:  40 | Train Loss: 0.6900 | Train Acc: 91.42% | Val Acc: 87.18%\n",
            "Epoch:  60 | Train Loss: 0.5256 | Train Acc: 94.29% | Val Acc: 88.90%\n",
            "Epoch:  80 | Train Loss: 0.1428 | Train Acc: 98.77% | Val Acc: 88.53%\n",
            "Epoch: 100 | Train Loss: 0.0869 | Train Acc: 99.71% | Val Acc: 88.04%\n",
            "Epoch: 120 | Train Loss: 0.0609 | Train Acc: 99.88% | Val Acc: 88.41%\n",
            "Epoch: 140 | Train Loss: 0.0517 | Train Acc: 99.88% | Val Acc: 87.92%\n",
            "Epoch: 160 | Train Loss: 0.0452 | Train Acc: 99.92% | Val Acc: 87.05%\n",
            "Epoch: 180 | Train Loss: 0.0314 | Train Acc: 99.92% | Val Acc: 87.30%\n",
            "Test accuracy: 85.94%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.datasets import DBLP\n",
        "from torch_geometric.nn import HANConv, Linear\n",
        "from torch.nn import LayerNorm, Dropout\n",
        "\n",
        "class EnhancedHAN(nn.Module):\n",
        "    def __init__(self, dim_in, dim_out, dim_h=192, heads=12, num_layers=2):\n",
        "        super().__init__()\n",
        "        self.dropout = Dropout(p=0.5)  # Increased dropout\n",
        "        \n",
        "        # Fewer layers but with careful regularization\n",
        "        self.hans = nn.ModuleList([\n",
        "            HANConv(\n",
        "                in_channels=-1 if i == 0 else dim_h,\n",
        "                out_channels=dim_h,\n",
        "                heads=heads,\n",
        "                dropout=0.5,  # Increased dropout in attention\n",
        "                metadata=data.metadata()\n",
        "            ) for i in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        self.layer_norms = nn.ModuleList([\n",
        "            LayerNorm(dim_h) for _ in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "        # Simplified classifier with strong regularization\n",
        "        self.classifier = nn.Sequential(\n",
        "            Linear(dim_h, dim_h),\n",
        "            nn.GELU(),  # Changed to GELU for better regularization\n",
        "            LayerNorm(dim_h),\n",
        "            Dropout(0.5),\n",
        "            Linear(dim_h, dim_h // 2),\n",
        "            nn.GELU(),\n",
        "            LayerNorm(dim_h // 2),\n",
        "            Dropout(0.5),\n",
        "            Linear(dim_h // 2, dim_out)\n",
        "        )\n",
        "        \n",
        "        # L2 regularization on attention weights\n",
        "        self.attention_l2 = 0.01\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict):\n",
        "        hidden = x_dict\n",
        "        attention_weights = []\n",
        "        \n",
        "        for i, (han, norm) in enumerate(zip(self.hans, self.layer_norms)):\n",
        "            new_hidden = han(hidden, edge_index_dict)\n",
        "            \n",
        "            # Store attention weights for regularization\n",
        "            if hasattr(han, 'alpha'):\n",
        "                attention_weights.append(han.alpha)\n",
        "            \n",
        "            # Residual connection with scaling\n",
        "            if i > 0:\n",
        "                for node_type in new_hidden.keys():\n",
        "                    new_hidden[node_type] = 0.8 * new_hidden[node_type] + 0.2 * hidden[node_type]\n",
        "            \n",
        "            # Strong regularization after each layer\n",
        "            for node_type in new_hidden.keys():\n",
        "                new_hidden[node_type] = self.dropout(norm(new_hidden[node_type]))\n",
        "            \n",
        "            hidden = new_hidden\n",
        "        \n",
        "        out = self.classifier(hidden['author'])\n",
        "        \n",
        "        # L2 regularization on attention\n",
        "        self.attention_reg = sum(w.pow(2).mean() for w in attention_weights) * self.attention_l2\n",
        "        \n",
        "        return out\n",
        "\n",
        "# Enhanced data preprocessing\n",
        "transform = T.Compose([\n",
        "    T.NormalizeFeatures(),\n",
        "    T.ToUndirected(),\n",
        "    T.AddSelfLoops(),\n",
        "    T.RandomNodeSplit(split='train_rest', num_val=0.2, num_test=0.2),  # Stratified split\n",
        "])\n",
        "\n",
        "# Load and preprocess dataset\n",
        "dataset = DBLP('.', transform=transform)\n",
        "data = dataset[0]\n",
        "data['conference'].x = torch.zeros(20, 1)\n",
        "\n",
        "# Model initialization\n",
        "model = EnhancedHAN(dim_in=-1, dim_out=4)\n",
        "\n",
        "# Optimizer with reduced learning rate and increased weight decay\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=0.001,  # Reduced learning rate\n",
        "    weight_decay=0.05,  # Increased weight decay\n",
        "    betas=(0.9, 0.999)\n",
        ")\n",
        "\n",
        "# Cosine annealing scheduler with warm restarts\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "    optimizer,\n",
        "    T_0=20,  # Reset every 20 epochs\n",
        "    T_mult=2,  # Double the reset interval each time\n",
        "    eta_min=1e-6\n",
        ")\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "data, model = data.to(device), model.to(device)\n",
        "\n",
        "@torch.no_grad()\n",
        "def test(mask):\n",
        "    model.eval()\n",
        "    out = model(data.x_dict, data.edge_index_dict)\n",
        "    pred = out[mask].argmax(dim=-1)\n",
        "    acc = (pred == data['author'].y[mask]).sum() / mask.sum()\n",
        "    return float(acc)\n",
        "\n",
        "# Training with early stopping\n",
        "best_val_acc = 0\n",
        "patience = 15\n",
        "patience_counter = 0\n",
        "best_state = None\n",
        "\n",
        "for epoch in range(200):  # Increased max epochs\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    out = model(data.x_dict, data.edge_index_dict)\n",
        "    mask = data['author'].train_mask\n",
        "    \n",
        "    # Combined loss with attention regularization\n",
        "    main_loss = F.cross_entropy(out[mask], data['author'].y[mask])\n",
        "    reg_loss = model.attention_reg\n",
        "    loss = main_loss + reg_loss\n",
        "    \n",
        "    loss.backward()\n",
        "    \n",
        "    # Gradient clipping\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "    \n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    \n",
        "    if epoch % 20 == 0:\n",
        "        train_acc = test(data['author'].train_mask)\n",
        "        val_acc = test(data['author'].val_mask)\n",
        "        print(f'Epoch: {epoch:>3} | Train Loss: {loss:.4f} | Train Acc: {train_acc*100:.2f}% | Val Acc: {val_acc*100:.2f}%')\n",
        "        \n",
        "        # Early stopping check\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            best_state = model.state_dict().copy()\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            \n",
        "        if patience_counter >= patience:\n",
        "            print(f'Early stopping at epoch {epoch}')\n",
        "            break\n",
        "\n",
        "# Load best model for final testing\n",
        "if best_state is not None:\n",
        "    model.load_state_dict(best_state)\n",
        "    \n",
        "test_acc = test(data['author'].test_mask)\n",
        "print(f'Test accuracy: {test_acc*100:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "book",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "3556630122da5213751af4465d61fcf5a52cd22515d400aee51118aaa1721248"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
