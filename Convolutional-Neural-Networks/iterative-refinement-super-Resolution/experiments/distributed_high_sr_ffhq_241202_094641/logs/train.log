24-12-02 09:46:41.956 - INFO:   name: distributed_high_sr_ffhq
  phase: val
  gpu_ids: [0, 1]
  path:[
    log: experiments/distributed_high_sr_ffhq_241202_094641/logs
    tb_logger: experiments/distributed_high_sr_ffhq_241202_094641/tb_logger
    results: experiments/distributed_high_sr_ffhq_241202_094641/results
    checkpoint: experiments/distributed_high_sr_ffhq_241202_094641/checkpoint
    resume_state: I830000_E32
    experiments_root: experiments/distributed_high_sr_ffhq_241202_094641
  ]
  datasets:[
    train:[
      name: FFHQ
      mode: HR
      dataroot: dataset/ffhq_64_512
      datatype: img
      l_resolution: 64
      r_resolution: 512
      batch_size: 2
      num_workers: 8
      use_shuffle: True
      data_len: -1
    ]
    val:[
      name: CelebaHQ
      mode: LRHR
      dataroot: dataset/celebahq_64_512
      datatype: img
      l_resolution: 64
      r_resolution: 512
      data_len: 50
    ]
  ]
  model:[
    which_model_G: sr3
    finetune_norm: False
    unet:[
      in_channel: 6
      out_channel: 3
      inner_channel: 64
      norm_groups: 16
      channel_multiplier: [1, 2, 4, 8, 16]
      attn_res: []
      res_blocks: 1
      dropout: 0
    ]
    beta_schedule:[
      train:[
        schedule: linear
        n_timestep: 2000
        linear_start: 1e-06
        linear_end: 0.01
      ]
      val:[
        schedule: linear
        n_timestep: 2000
        linear_start: 1e-06
        linear_end: 0.01
      ]
    ]
    diffusion:[
      image_size: 512
      channels: 3
      conditional: True
    ]
  ]
  train:[
    n_iter: 1000000
    val_freq: 10000.0
    save_checkpoint_freq: 10000.0
    print_freq: 50
    optimizer:[
      type: adam
      lr: 3e-06
    ]
    ema_scheduler:[
      step_start_ema: 5000
      update_ema_every: 1
      ema_decay: 0.9999
    ]
  ]
  wandb:[
    project: distributed_high_sr_ffhq
  ]
  distributed: True
  log_infer: False
  enable_wandb: False

