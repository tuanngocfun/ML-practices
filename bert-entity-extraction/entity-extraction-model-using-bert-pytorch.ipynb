{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity extraction using BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import everything important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-11-18T10:00:25.567485Z",
     "iopub.status.busy": "2024-11-18T10:00:25.567092Z",
     "iopub.status.idle": "2024-11-18T10:00:28.019352Z",
     "shell.execute_reply": "2024-11-18T10:00:28.018322Z",
     "shell.execute_reply.started": "2024-11-18T10:00:25.567445Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import transformers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn import model_selection\n",
    "\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T10:00:30.602673Z",
     "iopub.status.busy": "2024-11-18T10:00:30.602243Z",
     "iopub.status.idle": "2024-11-18T10:00:40.822351Z",
     "shell.execute_reply": "2024-11-18T10:00:40.821560Z",
     "shell.execute_reply.started": "2024-11-18T10:00:30.602632Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Not authenticated.  Copy a key from https://app.wandb.ai/authorize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key:  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "execution": {
     "iopub.execute_input": "2024-11-18T10:00:52.577722Z",
     "iopub.status.busy": "2024-11-18T10:00:52.577335Z",
     "iopub.status.idle": "2024-11-18T10:00:52.637275Z",
     "shell.execute_reply": "2024-11-18T10:00:52.636468Z",
     "shell.execute_reply.started": "2024-11-18T10:00:52.577684Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    MAX_LEN = 128\n",
    "    TRAIN_BATCH_SIZE = 64 # 32 for 5.8GB\n",
    "    VALID_BATCH_SIZE = 16 # 8 for 5.8GB\n",
    "    EPOCHS = 5\n",
    "    BASE_MODEL_PATH = \"../input/bert-base-uncased/\"\n",
    "    MODEL_PATH = \"model.bin\"\n",
    "    TRAINING_FILE = \"../input/entity-annotated-corpus/ner_dataset.csv\"\n",
    "    TOKENIZER = transformers.BertTokenizer.from_pretrained(\n",
    "        BASE_MODEL_PATH,\n",
    "        do_lower_case=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T10:13:00.835632Z",
     "iopub.status.busy": "2024-11-18T10:13:00.835284Z",
     "iopub.status.idle": "2024-11-18T10:13:00.839791Z",
     "shell.execute_reply": "2024-11-18T10:13:00.838920Z",
     "shell.execute_reply.started": "2024-11-18T10:13:00.835602Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['WANDB_MODE'] = 'offline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T10:13:01.380191Z",
     "iopub.status.busy": "2024-11-18T10:13:01.379662Z",
     "iopub.status.idle": "2024-11-18T10:13:06.430903Z",
     "shell.execute_reply": "2024-11-18T10:13:06.428886Z",
     "shell.execute_reply.started": "2024-11-18T10:13:01.380139Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "CommError",
     "evalue": "To use W&B in kaggle you must enable internet in the settings panel on the right.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCommError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-dcd3f02a4aa3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m\"valid_batch_size\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVALID_BATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"max_len\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;34m\"base_model_path\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBASE_MODEL_PATH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m })\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/wandb/__init__.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(job_type, dir, config, project, entity, reinit, tags, group, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, name, notes, id, magic, anonymous, config_exclude_keys, config_include_keys, save_code)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m         \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb_run\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_environment_or_defaults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m         \u001b[0m_global_run_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/wandb/wandb_run.py\u001b[0m in \u001b[0;36mfrom_environment_or_defaults\u001b[0;34m(cls, environment)\u001b[0m\n\u001b[1;32m    280\u001b[0m                   \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwandb_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwandb_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                   \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnotes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnotes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m                   resume=resume, api=api)\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/wandb/wandb_run.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, run_id, mode, dir, group, job_type, config, sweep_id, storage_id, description, resume, program, args, wandb_dir, tags, name, notes, api)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_viewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_viewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0;31m# give access to watch method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/wandb/wandb_run.py\u001b[0m in \u001b[0;36m_load_viewer\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    190\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mviewer_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mis_kaggle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mCommError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"To use W&B in kaggle you must enable internet in the settings panel on the right.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_viewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mviewer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mCommError\u001b[0m: To use W&B in kaggle you must enable internet in the settings panel on the right."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop. See /kaggle/working/wandb/debug.log for full traceback.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Network error (ConnectionError), entering retry loop. See /kaggle/working/wandb/debug.log for full traceback.\n"
     ]
    }
   ],
   "source": [
    "wandb.init(project=\"entity-extraction-by-bert\", config={\n",
    "    \"epochs\": config.EPOCHS,\n",
    "    \"train_batch_size\": config.TRAIN_BATCH_SIZE,\n",
    "    \"valid_batch_size\": config.VALID_BATCH_SIZE,\n",
    "    \"max_len\": config.MAX_LEN,\n",
    "    \"base_model_path\": config.BASE_MODEL_PATH\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T10:14:53.364923Z",
     "iopub.status.busy": "2024-11-18T10:14:53.364591Z",
     "iopub.status.idle": "2024-11-18T10:14:53.383021Z",
     "shell.execute_reply": "2024-11-18T10:14:53.382064Z",
     "shell.execute_reply.started": "2024-11-18T10:14:53.364893Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EntityDataset:\n",
    "    def __init__(self, texts, pos, tags):\n",
    "        self.texts = texts\n",
    "        self.pos = pos\n",
    "        self.tags = tags\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        text = self.texts[item]\n",
    "        pos = self.pos[item]\n",
    "        tags = self.tags[item]\n",
    "\n",
    "        ids = []\n",
    "        target_pos = []\n",
    "        target_tag =[]\n",
    "\n",
    "        for i, s in enumerate(text):\n",
    "            inputs = config.TOKENIZER.encode(\n",
    "                s,\n",
    "                add_special_tokens=False\n",
    "            )\n",
    "            # abhishek: ab ##hi ##sh ##ek\n",
    "            input_len = len(inputs)\n",
    "            ids.extend(inputs)\n",
    "            target_pos.extend([pos[i]] * input_len)\n",
    "            target_tag.extend([tags[i]] * input_len)\n",
    "\n",
    "        ids = ids[:config.MAX_LEN - 2]\n",
    "        target_pos = target_pos[:config.MAX_LEN - 2]\n",
    "        target_tag = target_tag[:config.MAX_LEN - 2]\n",
    "\n",
    "        ids = [101] + ids + [102]\n",
    "        target_pos = [0] + target_pos + [0]\n",
    "        target_tag = [0] + target_tag + [0]\n",
    "\n",
    "        mask = [1] * len(ids)\n",
    "        token_type_ids = [0] * len(ids)\n",
    "\n",
    "        padding_len = config.MAX_LEN - len(ids)\n",
    "\n",
    "        ids = ids + ([0] * padding_len)\n",
    "        mask = mask + ([0] * padding_len)\n",
    "        token_type_ids = token_type_ids + ([0] * padding_len)\n",
    "        target_pos = target_pos + ([0] * padding_len)\n",
    "        target_tag = target_tag + ([0] * padding_len)\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"target_pos\": torch.tensor(target_pos, dtype=torch.long),\n",
    "            \"target_tag\": torch.tensor(target_tag, dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T10:14:54.349914Z",
     "iopub.status.busy": "2024-11-18T10:14:54.349565Z",
     "iopub.status.idle": "2024-11-18T10:14:54.358031Z",
     "shell.execute_reply": "2024-11-18T10:14:54.356959Z",
     "shell.execute_reply.started": "2024-11-18T10:14:54.349886Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, optimizer, device, scheduler):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    for data in tqdm(data_loader, total=len(data_loader)):\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        _, _, loss = model(**data)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        final_loss += loss.item()\n",
    "        \n",
    "        # Clear cached memory\n",
    "        torch.cuda.empty_cache()\n",
    "    return final_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T10:14:55.742361Z",
     "iopub.status.busy": "2024-11-18T10:14:55.742000Z",
     "iopub.status.idle": "2024-11-18T10:14:55.759831Z",
     "shell.execute_reply": "2024-11-18T10:14:55.758823Z",
     "shell.execute_reply.started": "2024-11-18T10:14:55.742326Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def eval_fn(data_loader, model, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    all_preds_tag = []\n",
    "    all_targets_tag = []\n",
    "    all_preds_pos = []\n",
    "    all_targets_pos = []\n",
    "    \n",
    "    for data in tqdm(data_loader, total=len(data_loader)):\n",
    "        for k, v in data.items():\n",
    "            data[k] = v.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            tag, pos, loss = model(**data)\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        \n",
    "        # Get predictions and targets for tags and pos\n",
    "        preds_tag = tag.argmax(2).cpu().numpy()\n",
    "        targets_tag = data[\"target_tag\"].cpu().numpy()\n",
    "        preds_pos = pos.argmax(2).cpu().numpy()\n",
    "        targets_pos = data[\"target_pos\"].cpu().numpy()\n",
    "        \n",
    "        # Flatten and filter out padding tokens\n",
    "        for i in range(targets_tag.shape[0]):  # iterate over batch\n",
    "            all_preds_tag.extend(preds_tag[i][data[\"mask\"][i].cpu().numpy() == 1])\n",
    "            all_targets_tag.extend(targets_tag[i][data[\"mask\"][i].cpu().numpy() == 1])\n",
    "            all_preds_pos.extend(preds_pos[i][data[\"mask\"][i].cpu().numpy() == 1])\n",
    "            all_targets_pos.extend(targets_pos[i][data[\"mask\"][i].cpu().numpy() == 1])\n",
    "    \n",
    "    avg_loss = final_loss / len(data_loader)\n",
    "    \n",
    "    # Define labels to ensure consistency\n",
    "    tag_labels = list(range(len(enc_tag.classes_)))\n",
    "    pos_labels = list(range(len(enc_pos.classes_)))\n",
    "    \n",
    "    # Classification report for tags\n",
    "    tag_report = classification_report(all_targets_tag, all_preds_tag, labels=tag_labels, target_names=enc_tag.classes_, zero_division=0)\n",
    "    pos_report = classification_report(all_targets_pos, all_preds_pos, labels=pos_labels, target_names=enc_pos.classes_, zero_division=0)\n",
    "    \n",
    "    print(\"Tagging Classification Report:\\n\", tag_report)\n",
    "    print(\"POS Classification Report:\\n\", pos_report)\n",
    "    \n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T10:14:56.555899Z",
     "iopub.status.busy": "2024-11-18T10:14:56.555525Z",
     "iopub.status.idle": "2024-11-18T10:14:56.571283Z",
     "shell.execute_reply": "2024-11-18T10:14:56.570347Z",
     "shell.execute_reply.started": "2024-11-18T10:14:56.555862Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def loss_fn(output, target, mask, num_labels):\n",
    "    lfn = nn.CrossEntropyLoss()\n",
    "    active_loss = mask.view(-1) == 1\n",
    "    active_logits = output.view(-1, num_labels)\n",
    "    active_labels = torch.where(\n",
    "        active_loss,\n",
    "        target.view(-1),\n",
    "        torch.tensor(lfn.ignore_index).type_as(target)\n",
    "    )\n",
    "    loss = lfn(active_logits, active_labels)\n",
    "    return loss\n",
    "\n",
    "\n",
    "class EntityModel(nn.Module):\n",
    "    def __init__(self, num_tag, num_pos):\n",
    "        super(EntityModel, self).__init__()\n",
    "        self.num_tag = num_tag\n",
    "        self.num_pos = num_pos\n",
    "        self.bert = transformers.BertModel.from_pretrained(\n",
    "            config.BASE_MODEL_PATH\n",
    "        )\n",
    "        self.bert_drop_1 = nn.Dropout(0.3)\n",
    "        self.bert_drop_2 = nn.Dropout(0.3)\n",
    "        self.out_tag = nn.Linear(768, self.num_tag)\n",
    "        self.out_pos = nn.Linear(768, self.num_pos)\n",
    "    \n",
    "    def forward(\n",
    "        self, \n",
    "        ids, \n",
    "        mask, \n",
    "        token_type_ids, \n",
    "        target_pos, \n",
    "        target_tag\n",
    "    ):\n",
    "        o1, _ = self.bert(\n",
    "            ids, \n",
    "            attention_mask=mask, \n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "\n",
    "        bo_tag = self.bert_drop_1(o1)\n",
    "        bo_pos = self.bert_drop_2(o1)\n",
    "\n",
    "        tag = self.out_tag(bo_tag)\n",
    "        pos = self.out_pos(bo_pos)\n",
    "\n",
    "        loss_tag = loss_fn(tag, target_tag, mask, self.num_tag)\n",
    "        loss_pos = loss_fn(pos, target_pos, mask, self.num_pos)\n",
    "\n",
    "        loss = (loss_tag + loss_pos) / 2\n",
    "\n",
    "        return tag, pos, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T10:14:57.356479Z",
     "iopub.status.busy": "2024-11-18T10:14:57.356078Z",
     "iopub.status.idle": "2024-11-18T10:14:57.365628Z",
     "shell.execute_reply": "2024-11-18T10:14:57.364754Z",
     "shell.execute_reply.started": "2024-11-18T10:14:57.356440Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def process_data(data_path):\n",
    "    df = pd.read_csv(data_path, encoding=\"latin-1\")\n",
    "    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n",
    "\n",
    "    enc_pos = preprocessing.LabelEncoder()\n",
    "    enc_tag = preprocessing.LabelEncoder()\n",
    "\n",
    "    df.loc[:, \"POS\"] = enc_pos.fit_transform(df[\"POS\"])\n",
    "    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n",
    "\n",
    "    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n",
    "    pos = df.groupby(\"Sentence #\")[\"POS\"].apply(list).values\n",
    "    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n",
    "    return sentences, pos, tag, enc_pos, enc_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T10:28:36.908323Z",
     "iopub.status.busy": "2024-11-18T10:28:36.907960Z",
     "iopub.status.idle": "2024-11-18T11:24:06.231888Z",
     "shell.execute_reply": "2024-11-18T11:24:06.230730Z",
     "shell.execute_reply.started": "2024-11-18T10:28:36.908291Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 675/675 [10:40<00:00,  1.05it/s]\n",
      "100%|██████████| 300/300 [00:20<00:00, 14.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-art       1.00      0.99      1.00      9672\n",
      "       B-eve       0.00      0.00      0.00        40\n",
      "       B-geo       0.85      0.89      0.87      5908\n",
      "       B-gpe       0.92      0.91      0.92      1735\n",
      "       B-nat       0.00      0.00      0.00        24\n",
      "       B-org       0.71      0.66      0.69      3429\n",
      "       B-per       0.83      0.82      0.83      2625\n",
      "       B-tim       0.88      0.84      0.86      2207\n",
      "       I-art       0.00      0.00      0.00        43\n",
      "       I-eve       0.00      0.00      0.00        42\n",
      "       I-geo       0.77      0.70      0.74       861\n",
      "       I-gpe       0.00      0.00      0.00        20\n",
      "       I-nat       0.00      0.00      0.00         7\n",
      "       I-org       0.65      0.62      0.63      2102\n",
      "       I-per       0.82      0.94      0.87      3148\n",
      "       I-tim       0.73      0.64      0.68       625\n",
      "           O       0.99      0.99      0.99     94743\n",
      "\n",
      "    accuracy                           0.96    127231\n",
      "   macro avg       0.54      0.53      0.53    127231\n",
      "weighted avg       0.95      0.96      0.95    127231\n",
      "\n",
      "POS Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           $       1.00      1.00      1.00      9702\n",
      "           ,       1.00      1.00      1.00      3170\n",
      "           .       1.00      1.00      1.00      4780\n",
      "           :       0.79      0.66      0.72       106\n",
      "           ;       1.00      1.00      1.00        17\n",
      "          CC       1.00      1.00      1.00      2305\n",
      "          CD       0.99      0.99      0.99      3308\n",
      "          DT       1.00      1.00      1.00      9802\n",
      "          EX       0.98      0.95      0.97        64\n",
      "          FW       0.00      0.00      0.00         0\n",
      "          IN       0.99      0.99      0.99     12124\n",
      "          JJ       0.94      0.96      0.95      9870\n",
      "         JJR       0.86      0.99      0.92       299\n",
      "         JJS       0.94      0.95      0.94       326\n",
      "         LRB       1.00      1.00      1.00        69\n",
      "          MD       1.00      1.00      1.00       678\n",
      "          NN       0.97      0.96      0.96     15511\n",
      "         NNP       0.97      0.97      0.97     19838\n",
      "        NNPS       0.86      0.53      0.65       272\n",
      "         NNS       0.98      0.99      0.98      8253\n",
      "         PDT       0.00      0.00      0.00        18\n",
      "         POS       1.00      1.00      1.00      2244\n",
      "         PRP       1.00      1.00      1.00      1333\n",
      "        PRP$       0.99      1.00      1.00       873\n",
      "          RB       0.97      0.95      0.96      2156\n",
      "         RBR       0.94      0.57      0.71       107\n",
      "         RBS       0.88      0.27      0.41        26\n",
      "          RP       0.82      0.95      0.88       243\n",
      "         RRB       1.00      1.00      1.00        69\n",
      "          TO       1.00      1.00      1.00      2320\n",
      "          UH       0.00      0.00      0.00         4\n",
      "          VB       0.97      0.98      0.98      2581\n",
      "         VBD       0.97      0.98      0.98      4013\n",
      "         VBG       0.97      0.97      0.97      2119\n",
      "         VBN       0.97      0.94      0.95      3276\n",
      "         VBP       0.97      0.98      0.98      1609\n",
      "         VBZ       0.99      1.00      0.99      2552\n",
      "         WDT       0.96      0.98      0.97       335\n",
      "          WP       0.96      1.00      0.98       245\n",
      "         WP$       0.00      0.00      0.00        12\n",
      "         WRB       1.00      1.00      1.00       245\n",
      "          ``       1.00      1.00      1.00       357\n",
      "\n",
      "   micro avg       0.98      0.98      0.98    127231\n",
      "   macro avg       0.87      0.84      0.85    127231\n",
      "weighted avg       0.98      0.98      0.98    127231\n",
      "\n",
      "Train Loss = 0.3235557379214852 Valid Loss = 0.11885501343756914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 675/675 [10:39<00:00,  1.05it/s]\n",
      "100%|██████████| 300/300 [00:20<00:00, 14.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-art       1.00      0.99      1.00      9672\n",
      "       B-eve       0.50      0.07      0.13        40\n",
      "       B-geo       0.86      0.89      0.87      5908\n",
      "       B-gpe       0.93      0.93      0.93      1735\n",
      "       B-nat       0.00      0.00      0.00        24\n",
      "       B-org       0.74      0.68      0.71      3429\n",
      "       B-per       0.84      0.83      0.84      2625\n",
      "       B-tim       0.89      0.85      0.87      2207\n",
      "       I-art       0.00      0.00      0.00        43\n",
      "       I-eve       0.00      0.00      0.00        42\n",
      "       I-geo       0.77      0.73      0.75       861\n",
      "       I-gpe       0.00      0.00      0.00        20\n",
      "       I-nat       0.00      0.00      0.00         7\n",
      "       I-org       0.66      0.67      0.67      2102\n",
      "       I-per       0.82      0.94      0.88      3148\n",
      "       I-tim       0.73      0.76      0.75       625\n",
      "           O       0.99      0.99      0.99     94743\n",
      "\n",
      "    accuracy                           0.96    127231\n",
      "   macro avg       0.57      0.55      0.55    127231\n",
      "weighted avg       0.96      0.96      0.96    127231\n",
      "\n",
      "POS Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           $       1.00      1.00      1.00      9702\n",
      "           ,       1.00      1.00      1.00      3170\n",
      "           .       1.00      1.00      1.00      4780\n",
      "           :       0.79      0.85      0.82       106\n",
      "           ;       1.00      1.00      1.00        17\n",
      "          CC       1.00      1.00      1.00      2305\n",
      "          CD       0.99      0.99      0.99      3308\n",
      "          DT       1.00      1.00      1.00      9802\n",
      "          EX       0.98      0.95      0.97        64\n",
      "          FW       0.00      0.00      0.00         0\n",
      "          IN       1.00      0.99      0.99     12124\n",
      "          JJ       0.94      0.96      0.95      9870\n",
      "         JJR       0.89      0.98      0.93       299\n",
      "         JJS       0.99      0.98      0.98       326\n",
      "         LRB       1.00      1.00      1.00        69\n",
      "          MD       1.00      1.00      1.00       678\n",
      "          NN       0.97      0.96      0.97     15511\n",
      "         NNP       0.97      0.97      0.97     19838\n",
      "        NNPS       0.86      0.65      0.74       272\n",
      "         NNS       0.98      0.99      0.99      8253\n",
      "         PDT       0.00      0.00      0.00        18\n",
      "         POS       1.00      1.00      1.00      2244\n",
      "         PRP       1.00      1.00      1.00      1333\n",
      "        PRP$       1.00      1.00      1.00       873\n",
      "          RB       0.97      0.96      0.96      2156\n",
      "         RBR       0.90      0.65      0.76       107\n",
      "         RBS       0.88      0.88      0.88        26\n",
      "          RP       0.82      0.96      0.88       243\n",
      "         RRB       1.00      1.00      1.00        69\n",
      "          TO       1.00      1.00      1.00      2320\n",
      "          UH       0.00      0.00      0.00         4\n",
      "          VB       0.98      0.98      0.98      2581\n",
      "         VBD       0.97      0.98      0.98      4013\n",
      "         VBG       0.97      0.98      0.98      2119\n",
      "         VBN       0.97      0.94      0.95      3276\n",
      "         VBP       0.97      0.98      0.98      1609\n",
      "         VBZ       0.99      1.00      0.99      2552\n",
      "         WDT       0.96      0.98      0.97       335\n",
      "          WP       1.00      1.00      1.00       245\n",
      "         WP$       1.00      1.00      1.00        12\n",
      "         WRB       1.00      1.00      1.00       245\n",
      "          ``       1.00      1.00      1.00       357\n",
      "\n",
      "   micro avg       0.98      0.98      0.98    127231\n",
      "   macro avg       0.90      0.89      0.90    127231\n",
      "weighted avg       0.98      0.98      0.98    127231\n",
      "\n",
      "Train Loss = 0.10922022257690077 Valid Loss = 0.10349017816285293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 675/675 [10:40<00:00,  1.05it/s]\n",
      "100%|██████████| 300/300 [00:20<00:00, 14.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-art       1.00      0.99      0.99      9672\n",
      "       B-eve       0.43      0.15      0.22        40\n",
      "       B-geo       0.87      0.89      0.88      5908\n",
      "       B-gpe       0.94      0.93      0.93      1735\n",
      "       B-nat       0.12      0.04      0.06        24\n",
      "       B-org       0.73      0.71      0.72      3429\n",
      "       B-per       0.86      0.83      0.84      2625\n",
      "       B-tim       0.89      0.85      0.87      2207\n",
      "       I-art       0.00      0.00      0.00        43\n",
      "       I-eve       0.00      0.00      0.00        42\n",
      "       I-geo       0.78      0.75      0.76       861\n",
      "       I-gpe       0.00      0.00      0.00        20\n",
      "       I-nat       0.00      0.00      0.00         7\n",
      "       I-org       0.66      0.72      0.69      2102\n",
      "       I-per       0.84      0.93      0.88      3148\n",
      "       I-tim       0.70      0.80      0.75       625\n",
      "           O       0.99      0.99      0.99     94743\n",
      "\n",
      "    accuracy                           0.96    127231\n",
      "   macro avg       0.58      0.56      0.56    127231\n",
      "weighted avg       0.96      0.96      0.96    127231\n",
      "\n",
      "POS Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           $       1.00      1.00      1.00      9702\n",
      "           ,       1.00      1.00      1.00      3170\n",
      "           .       1.00      1.00      1.00      4780\n",
      "           :       0.82      0.88      0.85       106\n",
      "           ;       1.00      1.00      1.00        17\n",
      "          CC       1.00      1.00      1.00      2305\n",
      "          CD       0.99      0.99      0.99      3308\n",
      "          DT       1.00      1.00      1.00      9802\n",
      "          EX       0.98      0.95      0.97        64\n",
      "          FW       0.00      0.00      0.00         0\n",
      "          IN       1.00      0.99      0.99     12124\n",
      "          JJ       0.95      0.96      0.95      9870\n",
      "         JJR       0.91      0.98      0.94       299\n",
      "         JJS       0.98      0.98      0.98       326\n",
      "         LRB       1.00      1.00      1.00        69\n",
      "          MD       1.00      1.00      1.00       678\n",
      "          NN       0.97      0.96      0.97     15511\n",
      "         NNP       0.97      0.98      0.97     19838\n",
      "        NNPS       0.83      0.68      0.75       272\n",
      "         NNS       0.98      0.99      0.99      8253\n",
      "         PDT       0.88      0.39      0.54        18\n",
      "         POS       1.00      1.00      1.00      2244\n",
      "         PRP       1.00      1.00      1.00      1333\n",
      "        PRP$       1.00      1.00      1.00       873\n",
      "          RB       0.97      0.96      0.97      2156\n",
      "         RBR       0.91      0.74      0.81       107\n",
      "         RBS       0.89      0.92      0.91        26\n",
      "          RP       0.82      0.96      0.88       243\n",
      "         RRB       1.00      1.00      1.00        69\n",
      "          TO       1.00      1.00      1.00      2320\n",
      "          UH       0.00      0.00      0.00         4\n",
      "          VB       0.98      0.98      0.98      2581\n",
      "         VBD       0.97      0.99      0.98      4013\n",
      "         VBG       0.98      0.98      0.98      2119\n",
      "         VBN       0.97      0.95      0.96      3276\n",
      "         VBP       0.97      0.98      0.98      1609\n",
      "         VBZ       0.99      0.99      0.99      2552\n",
      "         WDT       0.97      0.98      0.97       335\n",
      "          WP       1.00      1.00      1.00       245\n",
      "         WP$       1.00      1.00      1.00        12\n",
      "         WRB       1.00      1.00      1.00       245\n",
      "          ``       1.00      1.00      1.00       357\n",
      "\n",
      "   micro avg       0.98      0.98      0.98    127231\n",
      "   macro avg       0.92      0.91      0.91    127231\n",
      "weighted avg       0.98      0.98      0.98    127231\n",
      "\n",
      "Train Loss = 0.0889324207659121 Valid Loss = 0.0981038355641067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 675/675 [10:40<00:00,  1.05it/s]\n",
      "100%|██████████| 300/300 [00:20<00:00, 14.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-art       1.00      0.99      0.99      9672\n",
      "       B-eve       0.43      0.15      0.22        40\n",
      "       B-geo       0.86      0.90      0.88      5908\n",
      "       B-gpe       0.93      0.93      0.93      1735\n",
      "       B-nat       0.17      0.04      0.07        24\n",
      "       B-org       0.76      0.71      0.74      3429\n",
      "       B-per       0.87      0.83      0.85      2625\n",
      "       B-tim       0.88      0.87      0.88      2207\n",
      "       I-art       0.00      0.00      0.00        43\n",
      "       I-eve       0.00      0.00      0.00        42\n",
      "       I-geo       0.76      0.77      0.76       861\n",
      "       I-gpe       0.00      0.00      0.00        20\n",
      "       I-nat       0.00      0.00      0.00         7\n",
      "       I-org       0.69      0.72      0.70      2102\n",
      "       I-per       0.85      0.94      0.89      3148\n",
      "       I-tim       0.71      0.81      0.76       625\n",
      "           O       0.99      0.99      0.99     94743\n",
      "\n",
      "    accuracy                           0.96    127231\n",
      "   macro avg       0.58      0.57      0.57    127231\n",
      "weighted avg       0.96      0.96      0.96    127231\n",
      "\n",
      "POS Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           $       1.00      1.00      1.00      9702\n",
      "           ,       1.00      1.00      1.00      3170\n",
      "           .       1.00      1.00      1.00      4780\n",
      "           :       0.84      0.87      0.85       106\n",
      "           ;       1.00      1.00      1.00        17\n",
      "          CC       1.00      1.00      1.00      2305\n",
      "          CD       0.99      0.99      0.99      3308\n",
      "          DT       1.00      1.00      1.00      9802\n",
      "          EX       0.98      0.95      0.97        64\n",
      "          FW       0.00      0.00      0.00         0\n",
      "          IN       1.00      0.99      0.99     12124\n",
      "          JJ       0.95      0.96      0.96      9870\n",
      "         JJR       0.92      0.98      0.95       299\n",
      "         JJS       0.96      0.98      0.97       326\n",
      "         LRB       1.00      1.00      1.00        69\n",
      "          MD       1.00      1.00      1.00       678\n",
      "          NN       0.98      0.96      0.97     15511\n",
      "         NNP       0.97      0.98      0.97     19838\n",
      "        NNPS       0.85      0.69      0.76       272\n",
      "         NNS       0.99      0.99      0.99      8253\n",
      "         PDT       0.77      0.94      0.85        18\n",
      "         POS       1.00      1.00      1.00      2244\n",
      "         PRP       1.00      1.00      1.00      1333\n",
      "        PRP$       1.00      1.00      1.00       873\n",
      "          RB       0.97      0.96      0.97      2156\n",
      "         RBR       0.90      0.77      0.83       107\n",
      "         RBS       0.89      0.92      0.91        26\n",
      "          RP       0.83      0.95      0.89       243\n",
      "         RRB       1.00      1.00      1.00        69\n",
      "          TO       1.00      1.00      1.00      2320\n",
      "          UH       0.00      0.00      0.00         4\n",
      "          VB       0.98      0.98      0.98      2581\n",
      "         VBD       0.97      0.99      0.98      4013\n",
      "         VBG       0.98      0.98      0.98      2119\n",
      "         VBN       0.97      0.95      0.96      3276\n",
      "         VBP       0.97      0.98      0.98      1609\n",
      "         VBZ       0.99      0.99      0.99      2552\n",
      "         WDT       0.97      0.98      0.97       335\n",
      "          WP       1.00      1.00      1.00       245\n",
      "         WP$       1.00      1.00      1.00        12\n",
      "         WRB       1.00      1.00      1.00       245\n",
      "          ``       1.00      1.00      1.00       357\n",
      "\n",
      "   micro avg       0.98      0.98      0.98    127231\n",
      "   macro avg       0.92      0.92      0.92    127231\n",
      "weighted avg       0.98      0.98      0.98    127231\n",
      "\n",
      "Train Loss = 0.07737794793314404 Valid Loss = 0.09571450757483642\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 675/675 [10:40<00:00,  1.05it/s]\n",
      "100%|██████████| 300/300 [00:20<00:00, 14.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tagging Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       B-art       1.00      0.99      0.99      9672\n",
      "       B-eve       0.43      0.15      0.22        40\n",
      "       B-geo       0.86      0.90      0.88      5908\n",
      "       B-gpe       0.94      0.93      0.94      1735\n",
      "       B-nat       0.20      0.04      0.07        24\n",
      "       B-org       0.79      0.70      0.74      3429\n",
      "       B-per       0.85      0.86      0.85      2625\n",
      "       B-tim       0.89      0.87      0.88      2207\n",
      "       I-art       0.00      0.00      0.00        43\n",
      "       I-eve       0.00      0.00      0.00        42\n",
      "       I-geo       0.77      0.78      0.77       861\n",
      "       I-gpe       0.00      0.00      0.00        20\n",
      "       I-nat       0.00      0.00      0.00         7\n",
      "       I-org       0.72      0.68      0.70      2102\n",
      "       I-per       0.85      0.93      0.89      3148\n",
      "       I-tim       0.78      0.80      0.79       625\n",
      "           O       0.99      0.99      0.99     94743\n",
      "\n",
      "    accuracy                           0.96    127231\n",
      "   macro avg       0.59      0.57      0.57    127231\n",
      "weighted avg       0.96      0.96      0.96    127231\n",
      "\n",
      "POS Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           $       1.00      1.00      1.00      9702\n",
      "           ,       1.00      1.00      1.00      3170\n",
      "           .       1.00      1.00      1.00      4780\n",
      "           :       0.86      0.84      0.85       106\n",
      "           ;       1.00      1.00      1.00        17\n",
      "          CC       1.00      1.00      1.00      2305\n",
      "          CD       0.99      0.99      0.99      3308\n",
      "          DT       1.00      1.00      1.00      9802\n",
      "          EX       0.98      0.95      0.97        64\n",
      "          FW       0.00      0.00      0.00         0\n",
      "          IN       1.00      0.99      0.99     12124\n",
      "          JJ       0.96      0.96      0.96      9870\n",
      "         JJR       0.92      0.98      0.95       299\n",
      "         JJS       0.97      0.98      0.97       326\n",
      "         LRB       1.00      1.00      1.00        69\n",
      "          MD       1.00      1.00      1.00       678\n",
      "          NN       0.97      0.96      0.97     15511\n",
      "         NNP       0.97      0.98      0.97     19838\n",
      "        NNPS       0.81      0.72      0.76       272\n",
      "         NNS       0.99      0.99      0.99      8253\n",
      "         PDT       0.75      1.00      0.86        18\n",
      "         POS       1.00      1.00      1.00      2244\n",
      "         PRP       1.00      1.00      1.00      1333\n",
      "        PRP$       1.00      1.00      1.00       873\n",
      "          RB       0.97      0.96      0.97      2156\n",
      "         RBR       0.92      0.75      0.82       107\n",
      "         RBS       0.88      0.88      0.88        26\n",
      "          RP       0.86      0.95      0.90       243\n",
      "         RRB       1.00      1.00      1.00        69\n",
      "          TO       1.00      1.00      1.00      2320\n",
      "          UH       0.00      0.00      0.00         4\n",
      "          VB       0.98      0.98      0.98      2581\n",
      "         VBD       0.97      0.99      0.98      4013\n",
      "         VBG       0.98      0.98      0.98      2119\n",
      "         VBN       0.96      0.96      0.96      3276\n",
      "         VBP       0.98      0.98      0.98      1609\n",
      "         VBZ       0.99      0.99      0.99      2552\n",
      "         WDT       0.97      0.98      0.97       335\n",
      "          WP       1.00      1.00      1.00       245\n",
      "         WP$       1.00      1.00      1.00        12\n",
      "         WRB       1.00      1.00      1.00       245\n",
      "          ``       1.00      1.00      1.00       357\n",
      "\n",
      "   micro avg       0.98      0.98      0.98    127231\n",
      "   macro avg       0.92      0.92      0.92    127231\n",
      "weighted avg       0.98      0.98      0.98    127231\n",
      "\n",
      "Train Loss = 0.07015057867875805 Valid Loss = 0.09441660614063342\n"
     ]
    }
   ],
   "source": [
    "sentences, pos, tag, enc_pos, enc_tag = process_data(config.TRAINING_FILE)\n",
    "\n",
    "meta_data = {\n",
    "    \"enc_pos\": enc_pos,\n",
    "    \"enc_tag\": enc_tag\n",
    "}\n",
    "\n",
    "joblib.dump(meta_data, \"meta.bin\")\n",
    "\n",
    "num_pos = len(list(enc_pos.classes_))\n",
    "num_tag = len(list(enc_tag.classes_))\n",
    "\n",
    "(\n",
    "    train_sentences,\n",
    "    test_sentences,\n",
    "    train_pos,\n",
    "    test_pos,\n",
    "    train_tag,\n",
    "    test_tag\n",
    ") = model_selection.train_test_split(\n",
    "    sentences, \n",
    "    pos, \n",
    "    tag, \n",
    "    random_state=42, \n",
    "    test_size=0.1\n",
    ")\n",
    "\n",
    "train_dataset = EntityDataset(\n",
    "    texts=train_sentences, pos=train_pos, tags=train_tag\n",
    ")\n",
    "\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, batch_size=config.TRAIN_BATCH_SIZE, num_workers=4\n",
    ")\n",
    "\n",
    "valid_dataset = EntityDataset(\n",
    "    texts=test_sentences, pos=test_pos, tags=test_tag\n",
    ")\n",
    "\n",
    "valid_data_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset, batch_size=config.VALID_BATCH_SIZE, num_workers=1\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = EntityModel(num_tag=num_tag, num_pos=num_pos)\n",
    "model.to(device)\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "optimizer_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if not any(\n",
    "                nd in n for nd in no_decay\n",
    "            )\n",
    "        ],\n",
    "        \"weight_decay\": 0.001,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in param_optimizer if any(\n",
    "                nd in n for nd in no_decay\n",
    "            )\n",
    "        ],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "\n",
    "num_train_steps = int(\n",
    "    len(train_sentences) / config.TRAIN_BATCH_SIZE * config.EPOCHS\n",
    ")\n",
    "optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=num_train_steps\n",
    ")\n",
    "\n",
    "best_loss = np.inf\n",
    "for epoch in range(config.EPOCHS):\n",
    "    train_loss = train_fn(\n",
    "        train_data_loader, \n",
    "        model, \n",
    "        optimizer, \n",
    "        device, \n",
    "        scheduler\n",
    "    )\n",
    "    test_loss = eval_fn(\n",
    "        valid_data_loader,\n",
    "        model,\n",
    "        device\n",
    "    )\n",
    "    print(f\"Train Loss = {train_loss} Valid Loss = {test_loss}\")\n",
    "    if test_loss < best_loss:\n",
    "        torch.save(model.state_dict(), config.MODEL_PATH)\n",
    "        best_loss = test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T11:24:55.156756Z",
     "iopub.status.busy": "2024-11-18T11:24:55.156355Z",
     "iopub.status.idle": "2024-11-18T11:24:57.904950Z",
     "shell.execute_reply": "2024-11-18T11:24:57.903936Z",
     "shell.execute_reply.started": "2024-11-18T11:24:55.156720Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abhishek', 'is', 'going', 'to', 'india']\n",
      "[101, 11113, 24158, 5369, 2243, 2003, 2183, 2000, 2634, 102]\n",
      "['B-art' 'B-per' 'B-per' 'B-per' 'B-per' 'O' 'O' 'O' 'B-geo' 'B-art']\n",
      "['$' 'NNP' 'NNP' 'NNP' 'NNP' 'VBZ' 'VBG' 'TO' 'NNP' '$']\n"
     ]
    }
   ],
   "source": [
    "meta_data = joblib.load(\"meta.bin\")\n",
    "enc_pos = meta_data[\"enc_pos\"]\n",
    "enc_tag = meta_data[\"enc_tag\"]\n",
    "\n",
    "num_pos = len(list(enc_pos.classes_))\n",
    "num_tag = len(list(enc_tag.classes_))\n",
    "\n",
    "sentence = \"\"\"\n",
    "abhishek is going to india\n",
    "\"\"\"\n",
    "tokenized_sentence = config.TOKENIZER.encode(sentence)\n",
    "\n",
    "sentence = sentence.split()\n",
    "print(sentence)\n",
    "print(tokenized_sentence)\n",
    "\n",
    "test_dataset = EntityDataset(\n",
    "    texts=[sentence], \n",
    "    pos=[[0] * len(sentence)], \n",
    "    tags=[[0] * len(sentence)]\n",
    ")\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = EntityModel(num_tag=num_tag, num_pos=num_pos)\n",
    "model.load_state_dict(torch.load(config.MODEL_PATH))\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    data = test_dataset[0]\n",
    "    for k, v in data.items():\n",
    "        data[k] = v.to(device).unsqueeze(0)\n",
    "    tag, pos, _ = model(**data)\n",
    "\n",
    "    print(\n",
    "        enc_tag.inverse_transform(\n",
    "            tag.argmax(2).cpu().numpy().reshape(-1)\n",
    "        )[:len(tokenized_sentence)]\n",
    "    )\n",
    "    print(\n",
    "        enc_pos.inverse_transform(\n",
    "            pos.argmax(2).cpu().numpy().reshape(-1)\n",
    "        )[:len(tokenized_sentence)]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T11:26:14.439151Z",
     "iopub.status.busy": "2024-11-18T11:26:14.438797Z",
     "iopub.status.idle": "2024-11-18T11:26:37.867957Z",
     "shell.execute_reply": "2024-11-18T11:26:37.867124Z",
     "shell.execute_reply.started": "2024-11-18T11:26:14.439121Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/kaggle/working/kaggle_working.zip' target='_blank'>/kaggle/working/kaggle_working.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/kaggle_working.zip"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Define the directory path and the output zip file name\n",
    "directory_path = '/kaggle/working/'\n",
    "output_zip = '/kaggle/working/kaggle_working.zip'\n",
    "\n",
    "# Compress the directory\n",
    "shutil.make_archive('/kaggle/working/kaggle_working', 'zip', directory_path)\n",
    "\n",
    "# Now, use the Kaggle environment's file download function to download it\n",
    "from IPython.display import FileLink\n",
    "FileLink(output_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T11:27:40.928728Z",
     "iopub.status.busy": "2024-11-18T11:27:40.928354Z",
     "iopub.status.idle": "2024-11-18T11:28:04.383028Z",
     "shell.execute_reply": "2024-11-18T11:28:04.381909Z",
     "shell.execute_reply.started": "2024-11-18T11:27:40.928695Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='/kaggle/working/kaggle_working.zip' target='_blank'>/kaggle/working/kaggle_working.zip</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/kaggle_working.zip"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Define the directory path and the output zip file name in the 'output' folder\n",
    "directory_path = '/kaggle/working/'\n",
    "output_zip = '/kaggle/working/kaggle_working.zip'\n",
    "\n",
    "# Compress the directory\n",
    "shutil.make_archive('/kaggle/working/kaggle_working', 'zip', directory_path)\n",
    "\n",
    "# Move the zip file to /kaggle/working which should be accessible for download\n",
    "import os\n",
    "os.rename('/kaggle/working/kaggle_working.zip', '/kaggle/working/kaggle_working.zip')\n",
    "\n",
    "# Display link for download\n",
    "from IPython.display import FileLink\n",
    "FileLink('/kaggle/working/kaggle_working.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-18T11:28:40.326159Z",
     "iopub.status.busy": "2024-11-18T11:28:40.325825Z",
     "iopub.status.idle": "2024-11-18T11:29:03.910755Z",
     "shell.execute_reply": "2024-11-18T11:29:03.909941Z",
     "shell.execute_reply.started": "2024-11-18T11:28:40.326130Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/kaggle/working/kaggle_working.zip'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Compress the directory and save it to the 'working' directory\n",
    "shutil.make_archive('/kaggle/working/kaggle_working', 'zip', '/kaggle/working/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1014,
     "sourceId": 4361,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 431504,
     "sourceId": 819665,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 29976,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
